\documentclass[12pt]{book}
\usepackage{amsmath, amssymb,epsfig}
\usepackage[spanish,activeacute]{babel}   % para que salga todo en español en vez de inglés
\usepackage{amsfonts,multicol,enumerate,hyperref}
\usepackage[utf8]{inputenc}
\usepackage{xcolor}
\usepackage{tikz}

%pre\'ambulo
\addtolength{\textwidth}{3.5cm}
\addtolength{\hoffset}{-1.7cm}
\addtolength{\voffset}{-1cm}
\addtolength{\textheight}{4cm}


\parindent=0cm
\parskip=1em

\newtheorem{teo}{Teorema}
\newtheorem{defi}{Definici\'on}
\newtheorem{lema}{Lema}
\newtheorem{prop}{Propiedad}
\newtheorem{cor}{Corolario}
\newtheorem{obs}{Observaci\'on}
\newtheorem{ejem}{Ejemplo}
 
\font\rmm=cmr10 at 10truept       
\font\rmmm=cmr8 at 8truept
\font\bff=cmbx10 at 10truept
\font\lhf=cmdunh10 at 10truept
\font\lgf=manfnt at 10truept
\font\bl=cmss10 at 10truept
\font\sll=cmsl10 at 10truept
\font\itt=cmti10 at 10truept
\def\nt{\noindent}
\def\pp{\vspace{0.3cm}}
\def\pq{\vspace{0.5cm}}
\def\ph{\vspace{1.5cm}}

\def\R{\mathbb{R}}
\def\N{\mathbb{N}}
\def\Z{\mathbb{Z}}
\def\C{\mathbb{C}}
\def\Q{\mathbb{Q}}
\def\K{\mathbb{K}}
\def\M{\mathcal{M}}
\def\P{\mathcal{P}}
\def\F{\mathcal{F}}
\def\B{\mathcal{B}}
\def\D{\mathcal{D}}
\def\Ccal{\mathcal{C}}
\def\card{\textup{card}}
\def\dim{\textup{dim}}
\def\Ker{\textup{Ker}}
\def\Im{\textup{Im}}



\title{Álgebra Lineal}
\author{Anahí Gajardo}
\date{\today}

\begin{document}
\maketitle
\frontmatter
%\chapter{Dedication}
%\chapter{Copyright}
\chapter{Agradecimientos}

Especiales agradecimientos son debidos a quienes confeccionaron el material audiovisual enlazado a este apunte: {\em Isabel Donoso}, {\em Felipe Jara}, {\em Daniela Lefimil}, {\em Nicolás Nuñez}, {\em Francisco Tassara}, {\em Filip Thiele}. Al igual que al proyecto IDECLab de la Universidad de Concepción.

\tableofcontents
%\listoffigures
%\listoftables
\mainmatter



\chapter{Introducción}


Este curso aborda lo esencial del {\bf Álgebra Lineal}, la cual es una de las herramientas más ubicuas dentro de la ingeniería.
El álgebra lineal es la teoría que establece qué son los \emph{vectores}, cómo operan entre sí y qué propiedades cumplen. Define el concepto de \emph{dimensión} de manera formal y estricta, permitiendo llevarlo a ámbitos más allá de lo espacial.

En el curso pasado estudiaron el conjunto $\R^2$, como pares ordenados o vectores. 
En este curso vamos a generalizar las vectores a $\R^n$, pero también veremos otros objetos matemáticos, como las funciones, matrices, polinomios y sucesiones, cada uno en su ámbito será un caso particular de lo que llamaremos \emph{espacio vectorial}.

También trata sobre las funciones que se pueden aplicar a los vectores, más precisamente de las \emph{funciones lineales}, que son muy simples, permiten un análisis muy profundo y aplicaciones muy amplias.
Están fuertemente ligados al concepto de \emph{matriz}, de manera que repasaremos este objeto también.
Aprenderemos conceptos como los \emph{valores propios}, que juegan un rol central en el análisis de sistemas dinámicos, estructuras, y análisis de sistemas en general, pues rescatan parte esencial de las funciones.

Esperamos que les guste, avanzaremos con lentitud en cada concepto, deberán hacer consultas continuamente para ir entendiendo paso a paso, y respondiendo los tests formativos que les entregaremos cotidianamente.

\section{Cuerpo}

\begin{defi}{\em (\textbf{Cuerpo})} Considere un conjunto $\K$ donde se definen dos operaciones:\\

\hspace{4 cm}$\begin{array}{l}
+:\K\times\K\rightarrow \K  \\
\hspace{0.9 cm}(x,y)\rightarrow x+y
\end{array}$\hspace{1 cm}$\begin{array}{ccc}
\cdot:\K\times\K\rightarrow \K\\
\hspace{1.1 cm}(x,y)\rightarrow x\cdot y
\end{array}$

La tripleta $(\K,+,\cdot)$ se llamar\'a Cuerpo Algebr\'aico o Cuerpo si:\\
\begin{enumerate}
\item $(\forall x,y,z\in \K)\ x+(y+z)=(x+y)+z$,\hfill (asociatividad de +)
\item $(\forall x,y\in \K)\ x+y=y+x$,\hfill(conmutatividad de +)
\item $(\exists \mathcal{O}\in \K)(\forall x\in \K)\ \mathcal{O}+x=x$, \hfill(existencia de neutro para $+$)
\item $(\forall x\in \K)(\exists y\in \K)\ x+y=\mathcal{O}$.\hfill(existencia de inverso para $+$)
\item $(\forall x,y,z\in \K)\ x\cdot(y\cdot z)=(x\cdot y)\cdot z$,\hfill (asociatividad de $\cdot$)
\item $(\forall x,y\in \K)\ x\cdot y=y\cdot x$,\hfill(conmutatividad de $\cdot$)
\item $(\exists 1\in \K)(\forall x\in \K)\ 1\cdot x=x$, \hfill(existencia de neutro para $\cdot$)
\item $(\forall x\in \K\setminus\{\mathcal{O}\})(\exists y\in \K)\ x\cdot y=1$.\hfill(existencia de inverso para $\cdot$)
\item $(\forall x,y,z\in\K)\ x\cdot(y+z)=x\cdot y+x\cdot z$.\hfill(distributividad)
\end{enumerate}
\end{defi}

Ejemplos de cuerpo son:
\begin{itemize}
\item $(\R,+,\cdot)$ con las operaciones usuales $+$ y $\cdot$ en $\R$.\
\item $(\C,+,\cdot)$ donde $(a+bi)+(c+di)=(a+c)+(b+d)i$ y  $(a+bi)\cdot(c+di)=(ac-bd)+(ad+bc)i$
\end{itemize}

Pero revisemos, ¿qué otros conjuntos conocen ustedes que cumplan estas propiedades? 
\begin{itemize}
\item ¿los números naturales las cumplen?
\item ¿los números racionales las cumplen?
\item ¿el conjunto de los polinomios las cumplen? 
\item ¿los complejos las cumplen?
\end{itemize}

Mientras responden estas preguntas, les contamos que el interés de definir el concepto de \emph{cuerpo} está en poder trabajar al mismo tiempo con todos los conjuntos que cumplen estas propiedades. La teoría que desarrollaremos será válida para todos los cuerpos que conocemos y que podamos llegar a encontrar alguna vez en la vida. Los teoremas que vamos a demostrar, serán válidos de momento que el conjunto con el que trabajemos cumpla las 9 propiedades enumeradas, y podremos hacer uso de los teoremas (sin tener que demostrarlos cada vez) con solo verificar las 9 propiedades, un gran ahorro, ¿no?

{\em Analicemos {\bf el conjunto de los polinomios con la suma ($+$) y el producto ($\cdot$)} de polinomios, veamos, sean $p,q,r\in \P(\K)$ la suma $+$ es:
\begin{itemize}
\item Conmutativa ($p+q=q+p$). En efecto, dado $x\in\K$ cualquiera se cumple $(p+q)(x)=p(x)+q(x)$, entonces usando la conmutatividad en $\K$, esto es igual a $q(x)+p(x)=(q+p)(x)$, y hemos probado que $\forall x\in\K,\ (p+q)(x)=(q+p)(x)$, es decir: $p+q=q+p$.
\item Asociativa ($(p+q)+r=p+(q+r)$). ... la misma idea.
\item Existe el elemento neutro. En efecto, el polinomio nulo, constante igual a 0 es neutro para la suma.
\item Existe el inverso aditivo. En efecto, dado $p$ cualquiera, el polinomio $-p$, definido por $\forall x\in \K,\ (-p)(x)=-p(x)$ cumple $\forall x\in\K,\ (p+(-p))(x)=p(x)+(-p)(x)=p(x)-p(x)=0$, es decir: $p+(-p)$ es el polinomio nulo.
\end{itemize}
Por otro lado, el producto $\cdot$ es:
\begin{itemize}
\item Asociativo ($(p\cdot q)\cdot r=p\cdot(q\cdot r)$). En efecto, dado $x\in\K$ cualquiera se cumple $((p\cdot q)\cdot r)(x)=(p\cdot q)(x)\cdot r(x)=(p(x)\cdot q(x))\cdot r(x)$, entonces usando la asociatividad de $\cdot$ en $\K$, esto es igual a $p(x)\cdot q(x)\cdot r(x)=p(x)\cdot(q(x)\cdot r(x))=p(x)\cdot(q\cdot r)(x)=(p\cdot(q\cdot r))(x)$, y hemos probado que $(p\cdot q)\cdot r=p\cdot(q\cdot r)$.
\item Conmutativo ($p\cdot q=q\cdot p$). La misma idea...
\item Existe elemento neutro. El polinomio constante igual a 1 es neutro para la multiplicación de polinomios.
\item Solo los polinomios constantes no nulos tienen inverso multiplicativo: si $p$ no es constante, entonces $\dfrac{1}{p}$ no es un polinomio.
  Aunque esta afirmación requiere una justificación menos dogmática: supongamos que $q=\dfrac{1}{p}$ es polinomio, o dicho de otra manera, $pq=1$ entonces $gr(pq)=gr(p)+gr(q)=gr(1)=0$, por lo tanto $gr(p)=gr(q)=0$, es decir, ambos polinomios son constantes.
\end{itemize}
Por lo tanto, el conjunto de los polinomios NO es cuerpo con las operaciones usuales.
\hfill $\Box$}


\textcolor{blue}{Para simplificar la notaci\'on, se considerar\'a $\alpha \beta$ en lugar de $\alpha\cdot \beta$ cuando $\alpha$ y $\beta$ pertenecen a un cuerpo}.

La siguiente proposici\'on es bastante básica, establece propiedades que ustedes también conocían para los números reales, pero que ahora veremos que se trata de propiedades que vienen dadas de forma ``gratuita'' partir de la definici\'on de cuerpo.

\begin{prop}
En cualquier cuerpo se cumplen las siguientes propiedades:

\begin{enumerate}[a)]
\item\label{u+x} El neutro aditivo y el neutro multiplicativo son únicos. 
\item Para cada $\alpha\in\K$, su inverso aditivo es único, y por o tanto lo denotaremos como $-\alpha$.
\item\label{--x} El inverso aditivo de $-\alpha$ es $\alpha$, es decir: $-(-\alpha)=\alpha$.
\item $-(\alpha\beta)=(-\alpha)\beta$,
\item\label{0x} $0\alpha=0$,
\item $x+z=y+z\Rightarrow x=y$.
\item $\alpha\beta=0\Rightarrow \alpha=0\vee\beta=0$, 
\item Si $\gamma\not =0$ y  $\alpha\gamma=\beta\gamma$, entonces $\alpha=\beta$.
\end{enumerate}
\end{prop}

A modo de pr\'actica, desarrollaremos la demostraci\'on de algunas partes de la propiedad 1.\\

{\bf Demostración.} {\em
Vamos a demostrar algunas de estas propiedades usando sólo la definición de cuerpo.
\begin{enumerate}
\item[\ref{u+x})] Supongamos que existen dos números que cumplen con la definición de neutro aditivo: $A$ y $B$, esto es, cumplen:
$$ \forall \alpha\in\K,\ A+\alpha=\alpha\wedge B+\alpha=\alpha.$$
Demostraremos que $A=B$ usando las propiedades de cuerpo:
como la propiedad es válida para todo $\alpha$, la aplicamos con $\alpha=B$, entonces,
$$ A+B=B.$$
Pero si la aplicamos con $\alpha=A$ obtenemos,
$$ B+A=A.$$
Juntando ambas y usando conmutatividad, concluimos que $A=B$.\hfill $\Box$
\item[\ref{--x})] La afirmación dice que ``el inverso aditivo de $-\alpha$ es $\alpha$'', en efecto, por la definición de inverso aditivo y la conmutatividad tenemos:\\
$0=\alpha+(-\alpha)=-\alpha+\alpha$, entonces $\alpha$ es el inverso de $-\alpha$. \hfill $\Box$
\item[\ref{0x})] Esta requiere trucos interesantes que aprender, partimos de una igualdad dada porque 0 es neutro, y aplicamos operaciones a ambos lados de la ecuación, y las propiedades de cuerpo:
$$\begin{array} {rcllr}
0+0&=&0 & /\cdot \alpha\\
(0+0)\alpha&=&0\alpha & (\textrm{distributividad})&\\
0\alpha+0\alpha&=&0\alpha & (\textrm{neutro aditivo})&\\
0\alpha+0\alpha&=&0\alpha+0 & /(-0\alpha)+&\\
-0\alpha+0\alpha+0\alpha&=&-0\alpha+0\alpha+0 & (\textrm{conmutatividad})&\\
0\alpha+(-0\alpha)+0\alpha&=&0\alpha+(-0\alpha)+0  \qquad &(\textrm{inverso aditivo})&\\
0+0\alpha&=&0+0  \qquad &(\textrm{neutro aditivo})&\\
0\alpha&=&0&&\Box
\end{array}$$
\end{enumerate}
}

\section{Tuplas}

Recordamos $\R^2$ el plano, compuesto por pares ordenados de números reales, definimos $\R^2=\R\times\R$, como el producto cartesiano del conjunto $\R$ con sí mismo.
Sus elementos son \emph{pares ordenados}:

$$\R^2=\{ (a,b)\ :\ a,b\in\R\}$$

Donde $(a,b)$ denota un par de números en que el orden importa, entonces $(a,b)\not=(b,a)$.
La notación que distingue un par ordenado de un conjunto es el uso del paréntesis redondo ``$($'', que por convención matemática internacional se usará para situaciones en que el orden de aparición de los objetos importa; a diferencia del conjunto, que se denota siempre con llaves ``$\{$'', donde el orden no tiene relevancia: $\{a,b\}=\{b,a\}$.

Claramente esto es válido cualquiera sea el conjunto considerado: $\C^2$ será el conjunto de pares ordenados de números complejos, por ejemplo: $(1+i,2)\in\C^2$, o para un conjunto que nos demos: Si $B=\{0,1\}$, entonces
$$B^2=\{(0,0),(0,1),(1,0),(1,1)\}$$

El ``2'' de $\R^2$ viene de ``par'' ordenado, ¿cómo es entonces $\R^3$?

En efecto podemos definir $\R^3$ como el conjunto de las \emph{tripletas ordenadas}: $\R^3=\R\times\R\times \R$.

$$\R^3=\{ (a,b,c)\ :\ a,b,c\in\R\}$$

Por ejemplo $(2,0,-1)\in\Z^3$. Podemos decir que en general, dado un conjunto cualquiera $A$, $A^3$ es el conjunto de todas las secuencias ordenadas de 3 elementos de $A$.

Las tripletas ordenadas se llaman también ``\emph{3-tuplas}''.
En general podemos definir una $n$-tupla como una secuencia ordenada de $n$ elementos.
Definimos así:

$$\R^n=\R\times\cdots \textrm{\small ($n$ veces)}\cdots \times \R=\{ (a_1,\dots,a_n)\ :\ \textrm{para todo}\ i\in\{1,\dots,n\},\ a_i\in \R\}$$

Algunos textos prefieren la notación vertical:

$$\R^n=\left\{\left(\begin{array}{c}a_1\\a_2\\\vdots\\a_{n-1}\\a_n\end{array}\right)\ :\ \textrm{para todo }i\in\{1,\dots,n\}, a_i\in\R\right\}$$

Ambas son válidas, solo es importante notar que cuando escribimos una tupla en forma horizontal sus componentes van siempre separadas por ``,'', mientras que cuando la escribimos en forma vertical no.

Las tuplas tienen múltiples aplicaciones, por ejemplo cuando llevamos registro de la temperatura máxima en cada uno de los días de un año, lo que estamos haciendo de definir una 365-tupla, la cual nos servirá luego para hacer modelos matemáticos, comparaciones, promedios, etc...

Siempre será interesante poder imaginar las tuplas, para eso usamos la representación como flecha o como ``vector'' (aunque esta solo es posible para $\R^2$ o $\R^3$).
Veamos un ejemplo, el caso $\R^2$ es más sencillo: representamos la primera coordenada en el eje horizontal, y la segunda en el eje vertical, el elemento (2,-1) quedará representado en el punto cuya proyección vertical caiga en 2 sobre el eje horizontal y su proyección horizontal caiga en -1 en el eje vertical.

\begin{center}
\begin{tikzpicture}
\draw[arrows=->] (-3.4,0) -- (3.4,0) node[below] {};
\draw[arrows=->] (0,-2.4) -- (0,2.4) node[left] {};
\foreach \pos in {-3,-2,-1,1,2,3}
    \draw[shift={(\pos,0)}] (0,.1) -- (0,-.1) node[below] {$\pos$};
\foreach \pos in {-2,-1,1,2}
    \draw[shift={(0,\pos)}] (.1,0) -- (-.1,0) node[left] {$\pos$};
\draw[dotted] (2,0)--(2,-1);
\draw[dotted] (0,-1)--(2,-1);
\draw[fill=black] (2,-1) circle (.1cm) node[right] {$(2,-1)$};
%\draw[very thick,color=red,arrows=->] (0,0)--(2,-1);
\end{tikzpicture}
\begin{tikzpicture}
\draw[white] (0,0)--(1,0);
\end{tikzpicture}
\begin{tikzpicture}
\draw[arrows=->] (-3.4,0) -- (3.4,0) node[below] {};
\draw[arrows=->] (0,-2.4) -- (0,2.4) node[left] {};
\foreach \pos in {-3,-2,-1,1,2,3}
    \draw[shift={(\pos,0)}] (0,.1) -- (0,-.1) node[below] {$\pos$};
\foreach \pos in {-2,-1,1,2}
    \draw[shift={(0,\pos)}] (.1,0) -- (-.1,0) node[left] {$\pos$};
\draw[dotted] (2,0)--(2,-1);
\draw[dotted] (0,-1)--(2,-1);
%\draw[fill=black] (2,-1) circle (.1cm) node[right] {$(2,-1)$};
\draw[very thick,color=red,arrows=->] (0,0)--(2,-1);
\end{tikzpicture}
\end{center}

La representación de flecha o vectorial consiste en trazar una flecha partiendo desde el punto (0,0) hasta el punto que representa a la tupla.

Para representar tuplas de $\R^3$, necesitamos una astucia: la idea es representar un tercer eje, también horizontal pero que ``salga del papel'', de manera perpendicular a los otros dos ejes.
Por supuesto esto necesita hacer uso de la imaginación, los arquitectos\footnote{En el video sobre los emojis Ter habla también de estas representaciones.} disponen de distintas maneras de hacer estos dibujos, pero acá solo usaremos lo que GeoGebra llama ``\emph{proyección oblicua}'', en la cual el tercer eje se dibuja en forma inclinada y a una escala más pequeña, emulando la perspectiva.

En la siguiente figura representamos la tupla $(3,2,-1)\in\R^3$.

\begin{center}
\begin{tikzpicture}
\draw[arrows=->] (-4.4,0) -- (4.4,0);
\draw[arrows=->] (0,-4.4) -- (0,4.4);
\draw[arrows=->] (3.11,3.11) -- (-3.11,-3.11);
\foreach \pos in {-4,-3,-2,-1,1,2,3,4}
    \draw[shift={(\pos,0)}] (0,.1) -- (0,-.1) node[below] {$\pos$};
\foreach \pos in {-4,-3,-2,-1,1,2,3,4}
    \draw[shift={(0,\pos)}] (.1,0) -- (-.1,0) node[left] {$\pos$};
\foreach \pos in {-4,-3,-2,-1,1,2,3,4}
    \draw[shift={(-0.7071*\pos,-0.7071*\pos)}] (.1,0) -- (-.1,0) node[left] {$\pos$};
\draw[dotted] (0.7071,2+0.7071)--(3+0.7071,2+0.7071);
\draw[dotted] (3+0.7071,0.7071)--(3+0.7071,2+0.7071);
\draw[dotted] (3,2)--(3+0.7071,2+0.7071);
\draw[dotted] (0.7071,2+0.7071)--(0,2);
\draw[dotted] (0.7071,2+0.7071)--(0.7071,0.7071);
\draw[dotted] (3+0.7071,0.7071)--(0.7071,0.7071);
\draw[dotted] (3+0.7071,0.7071)--(3,0);
\draw[dotted] (3,2)--(3,0);
\draw[dotted] (3,2)--(0,2);
\draw[fill=black] (3+0.7071,2+0.7071) circle (.1cm) node[right] {$(3,2,-1)$};
\draw[very thick,color=red,arrows=->] (0,0)--(3+0.7071,2+0.7071);
\end{tikzpicture}
\end{center}

Lo entretenido comienza cuando comenzamos a hacer operaciones entre tuplas.

Se define la \emph{suma} de tuplas como sigue.
Dados dos vectores cualesquiera 
$\left(\begin{array}{c}x_1\\x_2\\ \vdots\\ x_n\end{array}\right)$, $\left(\begin{array}{c}y_1\\y_2\\ \vdots\\ y_n\end{array}\right)$ en $\K^n$, se define: 
    $$ \left(\begin{array}{c}x_1\\ x_2\\ \vdots\\ x_n\end{array}\right)\oplus \left(\begin{array}{c}y_1\\y_2\\ \vdots\\ y_n\end{array}\right)=\left(\begin{array}{c}x_1+y_1\\x_2+y_2\\ \vdots\\ x_n+y_n\end{array}\right),$$
          que consiste en sumar coordenada a coordenada los números que componen la tupla.
          Vamos a usar el símbolo ``$\oplus$'' para denotar esta suma y así no confundirla con la suma del cuerpo ``$+$''.

Esta suma se torma muy interesante cuando la graficamos en la representación por flechas\footnote{Algo de esto ya lo habíamos visto cuando representamos a los números complejos como 2-tuplas.}, pues la suma se obtiene de transladar paralelamente una de las flechas a la cabeza de la otra, y trazar el vector que va del origen hasta la cabeza de la segunda flecha.
En la figura sumamos $(3,2,-1)\oplus(-1,-3,1)=(2,-1,0)$

\begin{center}--
\begin{tikzpicture}
\draw[arrows=->] (-4.4,0) -- (4.4,0);
\draw[arrows=->] (0,-4.4) -- (0,4.4);
\draw[arrows=->] (3.11,3.11) -- (-3.11,-3.11);
\foreach \pos in {-4,-3,-2,-1,1,2,3,4}
    \draw[shift={(\pos,0)}] (0,.1) -- (0,-.1) node[below] {$\pos$};
\foreach \pos in {-4,-3,-2,-1,1,2,3,4}
    \draw[shift={(0,\pos)}] (.1,0) -- (-.1,0) node[left] {$\pos$};
\foreach \pos in {-4,-3,-2,-1,1,2,3,4}
    \draw[shift={(-0.7071*\pos,-0.7071*\pos)}] (.1,0) -- (-.1,0) node[left] {$\pos$};

\draw[dotted] (0.7071,2+0.7071)--(3+0.7071,2+0.7071);
\draw[dotted] (3+0.7071,0.7071)--(3+0.7071,2+0.7071);
\draw[dotted] (3,2)--(3+0.7071,2+0.7071);
\draw[dotted] (0.7071,2+0.7071)--(0,2);
\draw[dotted] (0.7071,2+0.7071)--(0.7071,0.7071);
\draw[dotted] (3+0.7071,0.7071)--(0.7071,0.7071);
\draw[dotted] (3+0.7071,0.7071)--(3,0);
\draw[dotted] (3,2)--(3,0);
\draw[dotted] (3,2)--(0,2);
\draw[very thick,color=red,arrows=->] (0,0)--(3+0.7071,2+0.7071);
\draw[very thick,color=blue,arrows=->,dashed] (3+0.7071,2+0.7071)--(2,-1);

\foreach \x in {-1} {
  \foreach \y in {-3} {
    \foreach \z in {1} {
      \draw[dotted] (-0.7071*\z,\y-0.7071*\z)--(\x-0.7071*\z,\y-0.7071*\z);
      \draw[dotted] (\x-0.7071*\z,-0.7071*\z)--(\x-0.7071*\z,\y-0.7071*\z);
      \draw[dotted] (\x,\y)--(\x-0.7071*\z,\y-0.7071*\z);
      \draw[dotted] (-0.7071*\z,\y-0.7071*\z)--(0,\y);
      \draw[dotted] (-0.7071*\z,\y-0.7071*\z)--(-0.7071*\z,-0.7071*\z);
      \draw[dotted] (\x-0.7071*\z,-0.7071*\z)--(-0.7071*\z,-0.7071*\z);
      \draw[dotted] (\x-0.7071*\z,-0.7071*\z)--(\x,0);
      \draw[dotted] (\x,\y)--(\x,0);
      \draw[dotted] (\x,\y)--(0,\y);
      \draw[very thick,color=blue,arrows=->] (0,0)--(\x-0.7071*\z,\y-0.7071*\z);
      \draw[very thick,color=red,arrows=->,dashed] (\x-0.7071*\z,\y-0.7071*\z)--(\x+3,\y+2);
    }
  }
}
\foreach \x in {2} {
  \foreach \y in {-1} {
    \foreach \z in {0} {
      \draw[dotted] (-0.7071*\z,\y-0.7071*\z)--(\x-0.7071*\z,\y-0.7071*\z);
      \draw[dotted] (\x-0.7071*\z,-0.7071*\z)--(\x-0.7071*\z,\y-0.7071*\z);
      \draw[dotted] (\x,\y)--(\x-0.7071*\z,\y-0.7071*\z);
      \draw[dotted] (-0.7071*\z,\y-0.7071*\z)--(0,\y);
      \draw[dotted] (-0.7071*\z,\y-0.7071*\z)--(-0.7071*\z,-0.7071*\z);
      \draw[dotted] (\x-0.7071*\z,-0.7071*\z)--(-0.7071*\z,-0.7071*\z);
      \draw[dotted] (\x-0.7071*\z,-0.7071*\z)--(\x,0);
      \draw[dotted] (\x,\y)--(\x,0);
      \draw[dotted] (\x,\y)--(0,\y);
      \draw[very thick,color=green,arrows=->] (0,0)--(\x-0.7071*\z,\y-0.7071*\z);
    }
  }
}


\end{tikzpicture}
\end{center}

Otra operación que nos va a interesar es la \emph{ponderación por escalar}.
Esta consiste en tomar un vector y multiplicar cada una de sus componentes por un mismo número constante (el \emph{escalar}).
Usamos el símbolo ``$\odot$'' para denotar esta operación nueva.

          $$\alpha\odot \left(\begin{array}{c}x_1\\x_2\\ \vdots\\x_n\end{array}\right)=\left(\begin{array}{c}\alpha x_1\\\alpha x_2\\ \vdots\\\alpha x_n\end{array}\right).$$

Gráficamente esta acción lo que hace es ``amplificar'' el vector (la flecha) por el escalar, si el escalar es positivo y mayor que uno, el vector crece, si el escalar es positivo y menor que 1 el vector se achica, si el escalar es negativo, el vector se invierte, y va en la dirección opuesta.

Veamos el efecto de ponderar la 3-tupla $v=(3,2,-1)$ por $\alpha=-0,5$ (en verde) y por $\alpha=1.3$ (en naranja).

\begin{center}--
\begin{tikzpicture}
\draw[arrows=->] (-4.4,0) -- (4.4,0);
\draw[arrows=->] (0,-4.4) -- (0,4.4);
\draw[arrows=->] (3.11,3.11) -- (-3.11,-3.11);
\foreach \pos in {-4,-3,-2,-1,1,2,3,4}
    \draw[shift={(\pos,0)}] (0,.1) -- (0,-.1) node[below] {$\pos$};
\foreach \pos in {-4,-3,-2,-1,1,2,3,4}
    \draw[shift={(0,\pos)}] (.1,0) -- (-.1,0) node[left] {$\pos$};
\foreach \pos in {-4,-3,-2,-1,1,2,3,4}
    \draw[shift={(-0.7071*\pos,-0.7071*\pos)}] (.1,0) -- (-.1,0) node[left] {$\pos$};

\foreach \x in {3.9} {
  \foreach \y in {2.6} {
    \foreach \z in {-1.3} {
      \draw[dotted] (-0.7071*\z,\y-0.7071*\z)--(\x-0.7071*\z,\y-0.7071*\z);
      \draw[dotted] (\x-0.7071*\z,-0.7071*\z)--(\x-0.7071*\z,\y-0.7071*\z);
      \draw[dotted] (\x,\y)--(\x-0.7071*\z,\y-0.7071*\z);
      \draw[dotted] (-0.7071*\z,\y-0.7071*\z)--(0,\y);
      \draw[dotted] (-0.7071*\z,\y-0.7071*\z)--(-0.7071*\z,-0.7071*\z);
      \draw[dotted] (\x-0.7071*\z,-0.7071*\z)--(-0.7071*\z,-0.7071*\z);
      \draw[dotted] (\x-0.7071*\z,-0.7071*\z)--(\x,0);
      \draw[dotted] (\x,\y)--(\x,0);
      \draw[dotted] (\x,\y)--(0,\y);
      \draw[very thick,color=orange,arrows=->] (0,0)--(\x-0.7071*\z,\y-0.7071*\z);
      %\draw[fill=black] (\x-0.7071*\z,\y-0.7071*\z) circle (.1cm);
    }
  }
}

\foreach \x in {-1.5} {
  \foreach \y in {-1} {
    \foreach \z in {0.5} {
      \draw[dotted] (-0.7071*\z,\y-0.7071*\z)--(\x-0.7071*\z,\y-0.7071*\z);
      \draw[dotted] (\x-0.7071*\z,-0.7071*\z)--(\x-0.7071*\z,\y-0.7071*\z);
      \draw[dotted] (\x,\y)--(\x-0.7071*\z,\y-0.7071*\z);
      \draw[dotted] (-0.7071*\z,\y-0.7071*\z)--(0,\y);
      \draw[dotted] (-0.7071*\z,\y-0.7071*\z)--(-0.7071*\z,-0.7071*\z);
      \draw[dotted] (\x-0.7071*\z,-0.7071*\z)--(-0.7071*\z,-0.7071*\z);
      \draw[dotted] (\x-0.7071*\z,-0.7071*\z)--(\x,0);
      \draw[dotted] (\x,\y)--(\x,0);
      \draw[dotted] (\x,\y)--(0,\y);
      \draw[very thick,color=green,arrows=->] (0,0)--(\x-0.7071*\z,\y-0.7071*\z);
    }
  }
}


\foreach \x in {3} {
  \foreach \y in {2} {
    \foreach \z in {-1} {
      \draw[dotted] (-0.7071*\z,\y-0.7071*\z)--(\x-0.7071*\z,\y-0.7071*\z);
      \draw[dotted] (\x-0.7071*\z,-0.7071*\z)--(\x-0.7071*\z,\y-0.7071*\z);
      \draw[dotted] (\x,\y)--(\x-0.7071*\z,\y-0.7071*\z);
      \draw[dotted] (-0.7071*\z,\y-0.7071*\z)--(0,\y);
      \draw[dotted] (-0.7071*\z,\y-0.7071*\z)--(-0.7071*\z,-0.7071*\z);
      \draw[dotted] (\x-0.7071*\z,-0.7071*\z)--(-0.7071*\z,-0.7071*\z);
      \draw[dotted] (\x-0.7071*\z,-0.7071*\z)--(\x,0);
      \draw[dotted] (\x,\y)--(\x,0);
      \draw[dotted] (\x,\y)--(0,\y);
      \draw[very thick,color=red,arrows=->] (0,0)--(\x-0.7071*\z,\y-0.7071*\z);
 %     \draw[fill=black] (\x-0.7071*\z,\y-0.7071*\z) circle (.1cm);
    }
  }
}

\end{tikzpicture}
\end{center}

Ya que hemos definido operaciones, puede ser interesante averiguar qué propiedades tienen estas operaciones.

Sean $x=(x_1,\dots,x_n)$, $y=(y_1,\dots, y_n)$, $z=(z_1,\dots,z_n)\in\K^n$, y sean $\alpha,\beta\in\K$, donde $\K$ es un cuerpo cualquiera.

\begin{itemize}
\item Conmutativa ($x\oplus y=y\oplus x$). En efecto,
  $$ \left(\begin{array}{c}x_1\\ x_2\\ \vdots\\ x_n\end{array}\right)
  \oplus \left(\begin{array}{c}y_1\\y_2\\ \vdots\\ y_n\end{array}\right)
    =\left(\begin{array}{c}x_1+y_1\\x_2+y_2\\ \vdots\\ x_n+y_n\end{array}\right)
      =\left(\begin{array}{c}y_1+x_1\\y_2+x_2\\ \vdots\\ y_n+x_n\end{array}\right)
        =  \left(\begin{array}{c}y_1\\y_2\\ \vdots\\ y_n\end{array}\right)
          \oplus\left(\begin{array}{c}x_1\\ x_2\\ \vdots\\ x_n\end{array}\right)
    $$
En cada componente es la suma del cuerpo la que se usa, por lo tanto podemos apoyarnos en la conmutatividad de la suma en el cuerpo.
\item Asociativa ($(x\oplus y)\oplus z=x\oplus (y\oplus z)$). ... la misma idea.
\item Existe el elemento neutro. En efecto, si sumamos 0 a cada componente no modificamos ningún vector, por lo tanto podemos definir el vector $\Theta$ como el nulo:
$$ \Theta= \left(\begin{array}{c}0\\ 0\\ \vdots\\ 0\end{array}\right)$$
\item Existe el inverso aditivo. En efecto, el inverso aditivo de $x$ es $-x=(-x_1,-x_2,\dots,-x_n)$, pues cumple que al sumarse a $x$ da $\Theta$.
  $$ \left(\begin{array}{c}x_1\\ x_2\\ \vdots\\ x_n\end{array}\right)\oplus
  \left(\begin{array}{c}-x_1\\-x_2\\ \vdots\\ -x_n\end{array}\right)
    =\left(\begin{array}{c}0\\ 0\\ \vdots\\ 0\end{array}\right)$$
  
\end{itemize}

Todas las buenas propiedades.

Para la ponderación no podemos hablar de conmutatividad pues los escalares y las tuplas son objetos diferentes, no se pueden intercambiar.

Pero tal vez hay otras propiedades interesantes:

\begin{itemize}
\item $\alpha\odot (\beta\odot  x)=(\alpha\beta)\odot  x$. En efecto,

  $$ \alpha\odot  \left(\beta\odot \left(\begin{array}{c}x_1\\ x_2\\ \vdots\\ x_n\end{array}\right)\right)
    =\alpha\odot \left(\begin{array}{c}\beta x_1\\\beta x_2\\ \vdots\\ \beta x_n\end{array}\right)
      =\left(\begin{array}{c}\alpha\beta x_1\\\alpha\beta x_2\\ \vdots\\\alpha \beta x_n\end{array}\right)
        =(\alpha\beta)\odot \left(\begin{array}{c}x_1\\ x_2\\ \vdots\\ x_n\end{array}\right)$$

\item $\alpha\odot (x\oplus y)=\alpha\odot  x \oplus \alpha\odot  y$. Similar...
\item $(\alpha+\beta)\odot  x=\alpha\odot  x\oplus \beta\odot  x$. Parecido...
\item $1\odot  x=x$. Donde 1 es el neutro multiplicativo del cuerpo $\K$.
\item NO EXISTE inverso multiplicativo, ya que el resultado de la ponderación es siempre un vector, y por lo tanto nunca puede ser igual al neutro del cuerpo, y por la misma razón, el neutro para la ponderación no puede ser un vector.
\end{itemize}

Vamos a generalizar la noción de vector a algo más amplio, rescatando las operaciones de suma y ponderación y velando por que se preserven las propiedades más importantes que nos gustan.
De esto nacerá el concepto de {\bf Espacio Vectorial}, tema central del curso.

\section{Matrices}

\begin{defi}
Una \emph{matriz} es un arreglo rectangular de números. Una matriz de orden $m\times n$ a coeficientes en un conjunto $\K$ es un arreglo de elementos de $\K$ de $m$ filas y $n$ columnas.

$$A = \underbrace{\left.\left(\begin{array}{cccc}
A_{11}& A_{12}& \cdots & A_{1n}\\
A_{21}& a_{22}& \cdots & A_{2n}\\
\vdots& \vdots& \ddots & \vdots\\
A_{m1}& A_{m2}& \cdots & A_{mn}
\end{array}\right)\right\}}_{n \textrm{ columnas}}  {\footnotesize \textrm{ $m$ filas}}$$

El coeficiente $A_{ij}$ está en la fila $i$ y en la columna $j$. 

El conjunto de todas las matrices de orden $m\times n$ a coeficientes en $\K$ se denota $\mathcal{M}_{m\times n}(\K)$.
\end{defi}


\begin{defi}[Suma]
Sólo se pueden sumar matrices de igual orden, y la suma es coeficiente a coeficiente. Si $A,B\in\mathcal{M}_{m\times n}(\K)$, entonces 
$$ (A+B)_{ij}=A_{ij}+B_{ij}, \textrm{ para cada } i\in\{1,...,m\}, j\in\{1,...,n\}.$$
\end{defi}

Si $\K$ es cuerpo, resulta (y es fácil verlo) que la suma es {\bf conmutativa}, {\bf asociativa}, tiene {\bf neutro aditivo} (el cual es la matriz cuyos coeficientes son todos iguales a 0), y cada matriz tiene un {\bf inverso aditivo}, que se obtiene reemplazando cada coeficiente por su inverso aditivo. En otras palabras, si $A,B,C\in\mathcal{M}_{m\times n}(\K)$, y $\Theta_{m\times n}$ denota la matriz de orden $m\times n$ cuyos coeficientes son todos nulos, entonces:

\begin{multicols}{3}
\begin{itemize}
\item $A+B=B+A$
\item $(A+B)+C=A+(B+C)$
\item $A+\Theta_{m\times n}=A$
\end{itemize}
\end{multicols}

 \begin{defi}[Ponderación]
Sólo se puede ponderar una matriz por un escalar del cuerpo, coeficiente a coeficiente. Si $A\in\mathcal{M}_{m\times n}(\K)$, y $\lambda\in\K$, entonces 
$$ (\lambda A)_{ij}=\lambda A_{ij}, \textrm{ para cada } i\in\{1,...,m\}, j\in\{1,...,n\}.$$
\end{defi}

Si $A,B\in\mathcal{M}_{m\times n}(\K)$, y $\alpha,\beta\in\K$, entonces:

\begin{multicols}{3}
\begin{itemize}
\item $\alpha(\beta(A))=(\alpha \beta)A$
\item $\alpha(A+B)=\alpha A+\alpha B$
\item $(\alpha+\beta)A=\alpha A+\beta B$
\item $1A=A$
\item $A+(-1)A=\Theta_{m\times n}$
\end{itemize}
\end{multicols}

En otras palabras, $(\mathcal{M}_{m\times n}(\K),+,\cdot)$ es un espacio vectorial sobre $\K$.

Como espacio vectorial, $\mathcal{M}_{m\times n}(\K)$ sobre $\K$ también tiene su base canónica, y esta consiste en tomar las matrices que tienen un 1 en una posición y 0 en las demás, e irlas poniendo en orden de izquierda a derecha y de arriba a abajo.
Por ejemplo, la base canónica del espacio de las matrices de orden $3\times 2$ es la siguiente, en el ordenamiento que sigue:

$$\mathcal{C}_{3\times 2}(\K)=\left\{\left(\begin{array}{cc} 1&0\\ 0&0\\0&0\end{array}\right),\ 
\left(\begin{array}{cc} 0&1\\ 0&0\\0&0\end{array}\right),\
\left(\begin{array}{cc} 0&0\\ 1&0\\0&0\end{array}\right),\ 
\left(\begin{array}{cc} 0&0\\ 0&1\\0&0\end{array}\right),\ 
\left(\begin{array}{cc} 0&0\\ 0&0\\1&0\end{array}\right),\
\left(\begin{array}{cc} 0&0\\ 0&0\\0&1\end{array}\right)
\right\}.$$

Esto nos demuestra que $\dim(\mathcal{M}_{m\times n}(\K))=mn$.
Por ejemplo  $\dim(\mathcal{M}_{3\times 2}(\K))=6$.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\pq

\begin{center}
{\large \bf{ Multiplicación de matrices}}
\end{center}

\pq

Al inicio de esta semana definimos la multiplicación de matriz por vector de manera tal que se tuviera que ``evaluar la transformación en el vector fuera igual a multiplicar la matriz por el vector'', e decir que se tuviera la ecuación:
$$ T(v)=Av.$$

Las matrices se definen para representar bien a las transformaciones lineales.
Recordamos que si $T:\K^m\rightarrow \K^n$, entonces su matriz asociada $A\in\M_{n\times m}(\K)$.

Ahora vamos a definir la multiplicación entre matrices. Para hacerlo con sentido, vamos a asociar esta operación a la composición de transformaciones lineales.

Sean $T:\K^m\rightarrow \K^n$ y $F:\K^l\rightarrow \K^m$, dado que el espacio de llegada de $F$ es igual al espacio de partida de $T$ podemos definir la composición $T\circ F:\K^l\rightarrow \K^n$, por $(T\circ F)(v)=T(F(v))$.

Supongamos ahora que $A$ es la matriz asociada a $T$, es decir cumple: $T(v)=Av$.
Se debe tener que $A\in\M_{n\times m}(\K)$.

Supongamos que $B$ es la matriz asociada a $F$, es decir que cumple: $F(u)=Bu$.
Se debe tener que $B\in\M_{m\times l}(\K)$.

Tenemos entonces que $(T\circ F)(v)=T(F(v))=T(Bu)=A(Bu)=(AB)u$.
En otras palabras, la matriz asociada a la composición de $T$ con $F$ es igual a la multiplicación de las matrices $A$ y $B$.
Como $T\circ F:\K^l\rightarrow \K^n$, se debe tener $AB\in\M_{n\times l}(\K)$.

Notamos que:

$$ \underbrace{A}_{n\times m} \underbrace{B}_{m\times l} = \underbrace{AB}_{n\times l} $$

¿Pero cómo es $AB$? Recordemos que las columnas de la matriz asociada a una transformación corresponden a las imágenes de los vectores canónicos.

La $i$-ésima columna de $AB$ es entonces $T(F(e_i))$.
Pero $F(e_i)$ es la $i$-ésima columna de $B$, por lo tanto $T(F(e_i))=T(Col_i(B))=ACol_i(B)$, es la multiplicación de $A$ por la $i$-ésima columna de $B$, y esa operación la sabemos hacer pues es multiplicación de matriz por vector.

\begin{ejem}
Si tomamos $l=2$, $m=2$ y $n=3$, $A\in\M_{3\times 2}(\K)$, $B\in\M_{2\times 2}(\K)$, $AB\in\M_{3\times 2}(\K)$.
$$A=\left(\begin{array}{cc} A_{11}&A_{12}\\ A_{21}&A_{22}\\ A_{31}&A_{32}\end{array}\right),\quad 
B=\left(\begin{array}{cc} B_{11}&B_{12}\\B_{21}&B_{22}\end{array}\right)$$

$$Col_1(B)=\left(\begin{array}{c} B_{11}\\B_{21}\end{array}\right),\quad Col_2(B)=\left(\begin{array}{c} B_{12}\\B_{22}\end{array}\right)$$

$$Col_1(AB)=A\left(\begin{array}{c} B_{11}\\B_{21}\end{array}\right)=
\left(\begin{array}{cc} A_{11}&A_{12}\\ A_{21}&A_{22}\\ A_{31}&A_{32}\end{array}\right)\left(\begin{array}{c} B_{11}\\B_{21}\end{array}\right)=
\left(\begin{array}{c} A_{11}B_{11}+A_{12}B_{21}\\ A_{21}B_{11}+A_{22}B_{21}\\ A_{31}B_{11}+A_{32}B_{21}\end{array}\right)$$

$$Col_2(AB)=A\left(\begin{array}{c} B_{12}\\B_{22}\end{array}\right)=
\left(\begin{array}{cc} A_{11}&A_{12}\\ A_{21}&A_{22}\\ A_{31}&A_{32}\end{array}\right)\left(\begin{array}{c} B_{12}\\B_{22}\end{array}\right)=
\left(\begin{array}{c} A_{11}B_{12}+A_{12}B_{22}\\ A_{21}B_{12}+A_{22}B_{22}\\ A_{31}B_{12}+A_{32}B_{22}\end{array}\right)$$

Juntando todo obtenemos $AB$:

$$AB=\left(\begin{array}{cc} A_{11}B_{11}+A_{12}B_{21}&A_{11}B_{12}+A_{12}B_{22}\\ A_{21}B_{11}+A_{22}B_{21}&A_{21}B_{12}+A_{22}B_{22}\\ A_{31}B_{11}+A_{32}B_{21}&A_{31}B_{12}+A_{32}B_{22}\end{array}\right).$$

En otras palabras, tenemos que el coeficiente $ij$ de $AB$ tiene la siguiente fórmula

$$(AB)_{ij}=A_{i1}B_{1j}+A_{i2}B_{2j}$$

\end{ejem}


\pq


\begin{defi}[Multiplicaición]
Sólo se puede calcular $AB$, si $A$ tiene tantas columnas como filas tiene $B$, es decir, si $A\in\mathcal{M}_{m\times n}(S)$ y $B\in\mathcal{M}_{k\times l}(S)$, sólo se pueden multiplicar si $n=k$. En ese caso $AB\in\mathcal{M}_{m\times l}$ y:
$$ (AB)_{ij}=\sum_{p=1}^nA_{ip}B_{pj}, \textrm{ para cada } i\in\{1,...,m\}, j\in\{1,...,l\}.$$
En otras palabras, para calcular el coeficiente $ij$ de $AB$, debemos multiplicar cada coeficiente de la $i$-ésima fila de $A$, con cada coeficiente respectivo de la $j$-ésima columna de $B$, y luego sumar los $n$ productos obtenidos. 
\end{defi}

Dado que la composición de funciones es asociativa, la multiplicación también resulta asociativa.
Hay con neutro multiplicativo, el cual corresponde a la matriz asociada a la función identidad.
Además, la multiplicación distribuye respecto a la adición (no es difícil verlo).
Sin embargo NO es conmutativa en general, y NO toda matriz tiene inverso multiplicativo; peor aún, existen matrices no nulas que al multiplicarlas da como resultado la matriz nula $\Theta_{m\times n}$.

Veamos primero que la multiplicación NO es conmutativa.
Podríamos tomar dos matrices al azar $A\in\mathcal{M}_{2\times3}(\R)$, and $B\in\mathcal{M}_{3\times4}(\R)$, entonces $AB\in\mathcal{M}_{2\times 4}(\R)$.
Sin embargo $BA$ no se puede definir, obviamente no puede haber conmutatividad entre estas matrices.

Pero tomemos mejor dos matrices cuadradas, $C,D\in\mathcal{M}_{2\times2}(\R)$, en este caso $CD$ y $DC$ están definidas y ambas son de orden $2\times2$.
La pregunta es entonces: ¿serán iguales?.
Veámoslo con números.

$$C=\left(\begin{array}{cc} 1 & 2\\ -1 & 1\end{array}\right)\qquad
  D=\left(\begin{array}{cc} 2 & -2\\ 1 & 2\end{array}\right)$$

 
    
Las multiplicamos y obtenemos.

\begin{eqnarray*}
  CD&=&\left(\begin{array}{cc} 1 & 2\\ -1 & 1\end{array}\right)
    \left(\begin{array}{cc} 2 & -2\\ 1 & 2\end{array}\right)=
    \left(\begin{array}{cc} 4 & 2\\ -1 & 4\end{array}\right)\\
      DC&=&\left(\begin{array}{cc} 2 & -2\\ 1 & 2\end{array}\right)
        \left(\begin{array}{cc} 1 & 2\\ -1 & 1\end{array}\right)=
    \left(\begin{array}{cc} 4 & 2\\ -1 & 4\end{array}\right)
\end{eqnarray*}

¡¡Jaja!! ¡¡conmutan!!...

¿Pero será cierto con cualquier par de matrices, o será que tuvimos ``suerte''?

Tomemos otra matriz: $$E=\left(\begin{array}{cc} -1 & 2\\ 3 & 1\end{array}\right).$$

\begin{eqnarray*}
  CE&=&\left(\begin{array}{cc} 1 & 2\\ -1 & 1\end{array}\right)
    \left(\begin{array}{cc} -1 & 2\\ 3 & 1\end{array}\right)=
    \left(\begin{array}{cc} 5 & 4\\ 4 & -1\end{array}\right)\\
  EC&=&\left(\begin{array}{cc} -1 & 2\\ 3 & 1\end{array}\right)
    \left(\begin{array}{cc} 1 & 2\\ -1 & 1\end{array}\right)=
    \left(\begin{array}{cc} -3 & 0\\ 2 & 7\end{array}\right)
\end{eqnarray*}

... era casualidad. Veámoslo algebraicamente ahora.

\begin{eqnarray*}
  (AB)_{ij}= \sum_{k=1}^m A_{ik}B_{kj}&\qquad&
  (BA)_{ij}= \sum_{k=1}^n B_{ik}A_{kj}=\sum_{k=1}^n A_{kj}B_{ik}
\end{eqnarray*}

Son expresiones distintas, era de esperarse que no dieran el mismo resultado.
La multiplicaición de matrices no es conmutativa.

La otra cosa que sorprende es que dos matrices distintas de la nula pueden producir una matriz nula al multiplicarse, veamos un ejemplo donde esto ocurre.

$$\left(\begin{array}{cc} 2 & -3\\ -4 & 6\end{array}\right)
  \left(\begin{array}{cc} 3 & 9\\ 2 & 6\end{array}\right)=
  \left(\begin{array}{cc} 0 & 0\\ 0 & 0\end{array}\right)$$


Exploremos ahora el neutro multiplicativo, primero observamos que debe ser una matriz cuadrada, para que no modifique las dimensiones de la que multiplica.
Resulta que la matriz que funciona como neutro por la derecha para una matriz de orden $m\times n$ es una matriz de orden $n\times n$ que llamamos $I_{(n)}$ y definimos como sigue.

$$I_{(n)} = \left(\begin{array}{cccc}
1& 0& \cdots &0\\
0& 1& \cdots &0\\
\vdots& \vdots& \ddots & \vdots\\
0& 0& \cdots & 1
\end{array}\right), \textrm{ es decir } I_{(n)ij}=\left\{\begin{array}{rl} 1&\textrm{si }i=j,\\ 0&\textrm{si }i\not=j.\end{array}\right.$$

Verificamos que la primera columna de $I_{(n)}$ no hace más que replicar la primera columna de la matriz que multiplica, cuando lo hace por la derecha (ojo que por la izquierda ni siquiera se va a poder multiplicar si $m\not=n$), la segunda replica la segunda columna, y etc.

Por la izquierda se deberá usar $I_{(m)}$ (Ejercicio: verificarlo).

 

Estudiemos ahora la asociatividad, como pretendemos que es una propiedad verdadera, debemos demostrarla algebraícamente, ya no son válidos los contra-ejemplos.
Tomemos $A$ de orden $m\times n$, $B$ de orden $n\times l$, y $C$ de orden $l\times k$.

\begin{eqnarray*}
  (A(BC))_{ij} &=& \sum_{r=1}^n A_{ir}(BC)_{rj}\\
  &=& \sum_{r=1}^n A_{ir}(\sum_{t=1}^l B_{rt}C_{tj})\\
  &=& \sum_{r=1}^n \sum_{t=1}^l (A_{ir}B_{rt}C_{tj})
\end{eqnarray*}
\begin{eqnarray*}
  ((AB)C)_{ij} &=& \sum_{t=1}^l (AB)_{it}C_{tj}\\
  &=& \sum_{t=1}^l (\sum_{r=1}^n A_{ir}B_{rt}) C_{tj}\\
  &=& \sum_{t=1}^l \sum_{r=1}^n (A_{ir}B_{rt} C_{tj}\\
  &=& \sum_{r=1}^n \sum_{t=1}^l (A_{ir}B_{rt} C_{tj})
\end{eqnarray*}

 

La distributividad usa las mismas técnicas.
Supongamos que $D$ es de orden $n\times l$.

\begin{eqnarray*}
  (A(B+D))_{ij} &=& \sum_{r=1}^n A_{ir}(B+D)_{rj}\\
  &=& \sum_{r=1}^n A_{ir}(B_{rj}+D_{rj})\\
  &=& \sum_{r=1}^n (A_{ir}B_{rj}+ A_{ir}D_{rj})\\
  &=& \sum_{r=1}^n A_{ir}B_{rj}+ \sum_{r=1}^n A_{ir}D_{rj})\\
  &=& (AB)_{ij} + (AD)_{ij}
\end{eqnarray*}

 

En resumen, si $A$ es de orden $m\times n$, $B, D$ es de orden $n\times l$, $C$ es de orden $l\times k$, entonces

\begin{multicols}{2}
\begin{itemize}
\item $(AB)C=A(BC)$
\item $A(B+D)=AB+AD$
\item $AI_{(n)}=A$
\item $I_{(m)}A=A$
\end{itemize}
\end{multicols}

Como tenemos neutro multiplicativo podemos definir la inversa multiplicativa de una matriz (aunque no todas las matrices son invertibles, como veremos).

Antes de definirlo, volvamos a las transformaciones lineales, ya que la inversa multiplicativa de una matriz $A$ asociada a una transformación $T:\K^m\rightarrow\K^n$, debería estar asociada a la transformación inversa de $T$.

Por lo tanto solo las matrices asociadas a transformaciones invertibles pueden ser invertibles.
Si recordamos de Álgebra I que una función es invertible si y solo si es inyectiva y sobreyectiva, y si recordamos de la semana pasada que una transformación lineal $T:\K^m\rightarrow\K^n$ es sobreyectiva e inyectiva si y solo si $n(T)=0$ y $r(T)=\dim(\K^n)=n$, y que $n(T)+r(T)=\dim(\K^m)=m$, concluimos que solo se puede ser invertible si $r(T)=m=n$ y $n(T)=0$.

Así que si vamos a hablar de la inversa multiplicativa de una matriz $A$, necesitamos que $A$ sea una matriz cuadrada.

\begin{defi}
  Una matriz $B$ de orden $m\times m$ es la inversa de una matriz $A$ de orden $m\times m$ si y solo si se cumple:
$$AB=I_{(m)}\textrm{ y } BA=I_{(m)}.$$
\end{defi}


En general obtener la inversa de una matriz es trabajoso, incluso decidir si un matriz es invertible o no, requiere trabajo, aprenderemos a hacerlo durante el semestre.


\subsection{Matrices Cuadradas}

En el mundo de las matrices cuadradas, es decir, las matrices de orden $n\times n$ (igual ancho que alto), hay tipos particulares de matrices que facilitan algunos cálculos, estos tipos se refieren a la ``forma'' que tiene la matriz.

En una matriz llamamos \emph{diagonal principal} a la diagonal que parte de la esquina superior izquierda y termina en la esquina inferior derecha.
Corresponde a los coeficientes de coordenadas iguales.

\begin{minipage}{.45\textwidth}
\begin{center}
  \begin{tikzpicture}
    \draw (0,0)--(2,0)--(2,2)--(0,2)--cycle;
    \draw[color=magenta] (0,2)--(2,0);
  \end{tikzpicture}
\end{center}
\end{minipage}
\begin{minipage}{.45\textwidth}
$$A = \left(\begin{array}{cccc}
\color{magenta}{A_{11}}& A_{12}& \cdots & A_{1n}\\
A_{21}& \color{magenta}{A_{22}}& \cdots & A_{2n}\\
\vdots& \vdots& \color{magenta}{\ddots} & \vdots\\
A_{n1}& A_{n2}& \cdots & \color{magenta}{A_{nn}}
\end{array}\right)$$
\end{minipage}

 \pq

{\bf Matriz Diagonal}

Decimos que una matriz es  \emph{diagonal} si es cuadrada y fuera de la diagonal principal todos sus coeficientes son 0.
Es decir, la matriz $A$ es diagonal si y solo si es cuadrada y cumple
  $$\forall i\not=j,\ A_{ij}=0.$$

Decida cuáles de las siguientes matrices son diagonales:

\begin{center}
$\left(\begin{array}{rr}0& -1\\ 2& 0\end{array}\right)$\qquad
$\left(\begin{array}{rrrr}0& 0&0&0\\ 0& 3& 0&0\\ 0&0&-10.5&0\\0&0&0&1\end{array}\right)$\qquad
$\left(\begin{array}{rrr}1& 0&0\\ 0& 3&0\end{array}\right)$\qquad
$\left(\begin{array}{rrr}0& 0&0\\ 0& 0&0\\0&0&0\end{array}\right)$
\end{center}

La matriz identidad también es una matri diagonal

 \pq

{\bf Matriz Triangular Superior}

Decimos que una matriz es  \emph{triangular superior} si bajo la diagonal principal todos sus coeficientes son 0.
Es decir, la matriz $A$ es triangular superior si y solo si cumple
  $$\forall i>j,\ A_{ij}=0.$$

Decida cuáles de las siguientes matrices son triangulares superior:

\begin{center}
$\left(\begin{array}{rr}0& -1\\ 0& 3\end{array}\right)$\qquad
$\left(\begin{array}{rrrr}0& -100&0&2\\ 0& 3& 1&0\\ 0&0&-10.5&0\\0&0&1&1\end{array}\right)$\qquad
$\left(\begin{array}{rrr}1& 0&3\\ 0& 0&1\end{array}\right)$\qquad
$\left(\begin{array}{rrr}1& 0&-2\\ 0& 10&-5\\0&0&0\end{array}\right)$
\end{center}
 
\pq 

{\bf Matriz Triangular Inferior}

De manera análoga se dice que una matriz es \emph{triangular inferior} si bajo\emph{sobre} la diagonal principal todos sus coeficientes son 0.
Es decir, la matriz $A$ es triangular inferior si y solo si cumple
  $$\forall i<j,\ A_{ij}=0.$$

Observamos que solo las matrices diagonales pueden ser triangular inferior y superior al mismo tiempo.

 \pq 

{\bf Matriz Simétrica}

Decimos que una matriz es  \emph{simétrica} si es simétrica respecto a la diagonal principal.
Es decir, la matriz $A$ es simétrica si y solo si cumple
  $$\forall i,j,\ A_{ij}=A_{ji}.$$
Decida cuáles de las siguientes matrices son simétricas:
\begin{center}
$\left(\begin{array}{rr}0& -1\\ -1& 3\end{array}\right)$\qquad
$\left(\begin{array}{rrrr}0& 3&-4&2\\ 3& 1& 1&0\\ -4&1&0.2&1\\2&0&1&1\end{array}\right)$\qquad
$\left(\begin{array}{rrr}1& 0&3\\ 3& 0&1\end{array}\right)$\qquad
$\left(\begin{array}{rrr}1& 0&-2\\ -2& 10&-5\\-5&0&0\end{array}\right)$
\end{center} 
Observamos además que toda matriz diagonal es simétrica.

 \pq

{\bf Matriz Antisimétrica}

Decimos que una matriz es  \emph{antisimétrica} si cada coeficiente es el opuesto aditivo de su coeficiente en posición simétrica respecto a la diagonal principal.
Es decir, la matriz $A$ es antisimétrica si y solo si cumple
$$\forall i,j,\ A_{ij}=-A_{ji}.$$

Notamos que entonces $A_{ii}=-A_{ii}$, lo cual implica que $A_{ii}=0$ para todo $i$.

Decida cuáles de las siguientes matrices son antisimétricas:
\begin{center}
$\left(\begin{array}{rr}1& 0\\ 0& 1\end{array}\right)$\qquad
$\left(\begin{array}{rrrr}0& 3&-4&2\\ -3& 0& 1&0\\ 4&-1&0&1\\-2&0&-1&0\end{array}\right)$\qquad
$\left(\begin{array}{rrr}0& 0&-2\\ 0& 0&-5\\2&5&0\end{array}\right)$\qquad
$\left(\begin{array}{rrr}0& 3&0\\ -2& 0&2\\0&-3&0\\\end{array}\right)$\qquad
\end{center}
 \color{black}Observamos que sólo la matriz nula puede ser simétrica y antisimétrica.

 

Para ayudarnos a entender mejor estos últimos tipos de matrices definimos un operador en el conjunto de las matrices, que lo que hace es ``girar'' la matriz respecto a la diagonal principal, visualizaremos así más fácilmente las simetrías.

Dada una matriz cualquiera $A\in\mathcal{M}_{m\times n}(\R)$, se define su {\bf traspuesta} por $A^\top\in\mathcal{M}_{n\times m}(\R)$ como sigue:
$$ \forall i,\ \forall j,\ (A^\top)_{ij}=A_{ji}$$

$$A=\left(\begin{array}{rrr}\color{magenta}{1}& 2&3\\ 4& \color{magenta}{5}&6\\7&8&\color{magenta}{9}\\ \pi&0&\sqrt{3}\end{array}\right)\qquad\rightarrow\qquad
A^\top=\left(\begin{array}{rrrr}\color{magenta}{1}& 4&7&\pi\\ 2& \color{magenta}{5}& 8&0\\ 3&6&\color{magenta}{9}&\sqrt{3}\end{array}\right)$$

 \pq

\begin{prop}
\begin{itemize}
\item $A$ es simétrica si y solo si $A^\top=A$
\item $A$ es antisimétrica si y solo si $A^\top=-A$
\item La traspuesta de una matriz triangular inferior es triangular superior.
\item Toda matriz diagonal es simétrica.
\item La traspuesta de una matriz columna, es una matriz fila.
\end{itemize}
\end{prop}

\pq

\begin{prop}
  \begin{itemize}
  \item La suma de matrices simétricas es simétrica.
  \item La suma de matrices antisimétricas es antisimétrica
  \item La suma (y la multiplicación) de matrices triangulares inferiores es triangular inferior.
  \item La inversa de una matriz diagonal e invertible $A$ es la matriz diagonal en que cada coeficiente de la diagonal es el inverso multiplicativo del respectivo coeficiente de $A$.
  \item Si $B$ se multiplica por una matriz diagonal $A$ por la derecha, entonces cada una de sus columnas resulta multiplicada por el coeficiente de la misma columna de $A$.
  \item Si $B$ se multiplica por una matriz diagonal $A$ por la izquierda, entonces cada una de sus filas resulta multiplicada por el coeficiente de la misma fila de $A$.
  \end{itemize}
\end{prop}


\subsubsection{Traza}

Para las matrices cuadradas se define una función muy fácil de calcular y que tiene muy bonitas propiedades, que descubriremos poco a poco en lo que queda del semestre: la \emph{traza}.
\begin{defi}
  Dada una matriz $A$ cuadrada de orden $m\times m$ se define la traza de $A$, que se denota $tr(A)\in\K$ como la suma de los elementos de la diagonal de $A$:
  $$tr(A)=\sum_{i=1}^m A_{ii}$$
\end{defi}
\begin{ejem}
Calculemos la traza de las siguientes matrices:
\begin{center}
$tr\left(\begin{array}{rr}0& -1\\ -1& 3\end{array}\right)=3$\qquad
$tr\left(\begin{array}{rrrr}0& 3&-4&2\\ 3& 1& 1&0\\ -4&1&0.2&1\\2&0&1&1\end{array}\right)=2.2$\qquad
$tr\left(\begin{array}{rrr}1& 0&-2\\ -2& 10&-5\\-5&0&0\end{array}\right)=11$
\end{center}
\end{ejem}

\subsubsection{Determinante}

La siguiente es una herramienta extremadamente útil en el análisis de matrices : el \emph{determinante}.
Se trata de un escalar que es posible asociar a una matriz cuadrada, y que ofrece información relevante respecto a ésta.
Su cálculo es medianamente complejo, pero es un concepto muy profundo que vale la pena aprender.
En términos abstractos el determinante se define a través de las siguientes propiedades.
 
\begin{defi}\label{def:det}
  Una función $\det:\mathcal{M}_{n\times n}(\K)\rightarrow \K$ se llama determinante si para cualquier serie de filas $v_1,\dots, v_n,u\in\K^n$ y cualquier $\alpha,\beta\in\K$ cumple:
  \begin{enumerate}
  \item
    $\det
    \left(
    \begin{array}{ccc}
      -&v_1&-\\
      &\cdots&\\
      -&\alpha v_i+\beta u&-\\
      &\cdots&\\
      -&v_n&-
    \end{array}
    \right)=\alpha\det
    \left(
    \begin{array}{ccc}
      -&v_1&-\\
      &\cdots&\\
      -&v_i&-\\
      &\cdots&\\
      -&v_n&-
    \end{array}
    \right)+
    \beta \det\left(
    \begin{array}{ccc}
      -&v_1&-\\
      &\cdots&\\
      -&u&-\\
      &\cdots&\\
      -&v_n&-
    \end{array}\right)$
  \item 
    $\det
    \left(
    \begin{array}{ccc}
      -&v_1&-\\
      &\cdots&\\
      -&v_i&-\\
      &\cdots&\\
      -&v_j&-\\
      &\cdots&\\
      -&v_n&-
    \end{array}
    \right)=
    -\det
    \left(
    \begin{array}{ccc}
      -&v_1&-\\
      &\cdots&\\
      -&v_j&-\\
      &\cdots&\\
      -&v_i&-\\
      &\cdots&\\
      -&v_n&-
    \end{array}
    \right)$
  \item $\det(I_n)=1$
  \end{enumerate}    
\end{defi}
 
La matemática demuestra, mediante un muy largo proceso, que existe una sola función que cumple estas 3 propiedades, y ésta se puede calcular recursívamente como sigue:

\begin{defi}[Cálculo del determinante]
  El determinante de una matriz cuadrada se obtiene así:
\begin{itemize}
\item Si $A=(a_{11})\in\mathcal{M}_{1\times 1}(\K)$, entonces: $\det(A)=a_{11}$.
\item Si $A\in\mathcal{M}_{n\times n}(\K)$, con $n\ge 2$, entonces:
  $$\det(A)=\sum_{i=1}^n(-1)^{i+1}a_{i1}\det(A^{i1})$$
  Donde $A^{kl}$ representa la (sub)matriz que se obtiene de $A$ al eliminar la fila $k$ y la columna $l$.
\end{itemize}
\end{defi}

 
%0

{\bf Ejemplo:} {\em Si a la matriz ${ a_{11}\ a_{12}\choose a_{21}\ a_{22} }$ le borramos la fila 1 y la columna 2, nos queda la matriz de $1\times 1$: $(a_{21})$. Así

  $\det{ a_{11}\ a_{12}\choose a_{21}\ a_{22} }=(-1)^{1+1}a_{11}\det(a_{22})+(-1)^{2+1}a_{21}\det(a_{12})=a_{11}a_{22}-a_{12}a_{21}$

Por ejemplo, $\det{2\ 4\choose -3\ 5}=2\cdot 5-4\cdot(-3)=10+12=22$.}

{\bf Ejemplo:} {\em Si $A\in\mathcal{M}_{3\times 3}(\K)$, entonces:
  $$A=\left(\begin{array}{ccc}a_{11}&a_{12}&a_{13}\\a_{21}&a_{22}&a_{23}\\a_{31}&a_{32}&a_{33}\end{array}\right) \longrightarrow
  A^{11}=\left(\begin{array}{ccc}&&\\\phantom{a_{21}}&a_{22}&a_{23}\\&a_{32}&a_{33}\end{array}\right)=\left(\begin{array}{cc}a_{22}&a_{23}\\a_{32}&a_{33}\end{array}\right)$$
  $$\longrightarrow A^{21}=\left(\begin{array}{ccc}\phantom{a_{11}}& a_{12}& a_{13}
    \\&&\\&a_{32}&a_{33}\end{array}\right)=\left(\begin{array}{cc}a_{12}&a_{13}\\a_{32}&a_{33}\end{array}\right)$$
  $$\longrightarrow A^{31}=\left(\begin{array}{ccc}&a_{12}&a_{13}\\
    \phantom{a_{21}}&a_{22}&{a_{23}}\\
    \phantom{a_{31}}&&\end{array}\right)=
  \left(\begin{array}{cc}a_{12}&a_{13}\\a_{22}&a_{23}\end{array}\right)$$
}

{\bf Ejemplo.} {\em Calculemos el determinante de la siguiente matriz:
  \begin{eqnarray*}
    \det(A)&=&
    \det\left(\begin{array}{rrr}1&1&-2\\3&0&-6\\-5&1&8\end{array}\right)\\
      &=&
      (-1)^{1+1}1\det{0\ -6 \choose 1\ \ 8}+(-1)^{2+1}3\det{\ 1\ -2\choose 1\ \ 8}+(-1)^{3+1}(-5)\det{\ 1\ -2\choose 0\ -6}\\
      &=&
      (0+6)-3(8+2)-5(-6+0)\\
      &=&6-30+30\\
      &=&6
  \end{eqnarray*}
}

 

El determinante tiene muchos usos, pero en esta parte del curso el que nos servirá más tiene que ver con la invertibilidad de una matriz.
\begin{teo} Si $A\in\mathcal{M}_{n\times n}(\K)$, entonces:
   \begin{center} $A$ es invertible si y solo si $\det(A)\not=0$.\end{center}
\end{teo}
No estamos en condiciones de demostrar esta propiedad, pero podemos adelantar que el determinante se puede usar para calcular la inversa misma de una matriz mediante el Método de Cramer, aunque la manera más eficiente por ahora de calcular la inversa seguirá siendo aplicar el método de Gauss-Jordan.

No es difícil ver que el determinante cumple propiedades de la Definición~\ref{def:det}, de hecho esas propiedades se tienen que ver con operaciones elementales de filas, y el determinante se porta muy bien con las operaciones de fila, puesto que estas son multiplicaciones matriciales y el determinante respeta la multiplicación de matrices
 

\begin{prop}\label{prop:det}
  Dadas $A,B\in\mathcal{M}_{n\times n}(\K)$, $\lambda\in\K$, se cumple:
  \begin{enumerate}
  \item Si $A$ es triangular, entonces $\det(A)=a_{11}a_{22}\dots a_{nn}$. 
  \item $\det(\lambda A)=\lambda^n\det(A)$
  \item Si $A$ tiene una fila nula, entonces $\det(A)=0$.
  \item Si $A$ tiene dos filas iguales, entonces $\det(A)=0$.
  \item Si a una fila de $A$ le sumamos otra fila ponderada, el determinante no cambia.
  \item Si al aplicar operaciones de fila a $A$, una fila se anula, entoces $\det(A)=0$.
  \item $\det(A^\top)=\det(A)$
  \item $\det(AB)=\det(A)\det(B)$
  \item Si $A$ es invertible, entonces $\det(A^{-1})=(\det(A))^{-1}$.
  \end{enumerate}
\end{prop}


\chapter{Sistemas de Ecuaciones Lineales}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{Espacios Vectoriales}

\pq

Hemos visto que las tuplas se pueden \emph{sumar} poniendo ``uno delante del otro", lo cual equivale a sumar coordenada a coordenada sus valores.
También se pueden \emph{ponderar} por un \emph{escalar}, amplificando, reduciendo o incluso invirtiendo el sentido del vector.

Vamos a generalizar la noción de vector a algo más amplio, rescatando las operaciones de suma y ponderación y velando para que se preserven las propiedades más importantes que nos gustan.

\begin{defi}{\bf (Espacio Vectorial)} Sea $\K$ un cuerpo y $V$ un conjunto sobre el cual se definen dos operaciones:

\hspace{2.5 cm}$\begin{array}{lccl}
\oplus:V\times V\rightarrow V & & & \odot:\K\times V\rightarrow V \\
$Adici\'on Binaria Interna$ & & & $Ponderación por escalar Binaria Externa$
\end{array}$

\vspace{0.5 cm}
 
La estructura $(V,\oplus,\odot)$ se llama \emph{espacio vectorial (e.v.) sobre $\K$}, o simplemente \emph{$\K$-espacio vectorial}, si se cumplen las siguientes propiedades (axiomas de espacio vectorial):

\begin{multicols}{2}
\begin{enumerate}
\item $\forall u,v,w\in V,\ u\oplus(v\oplus w)=(u\oplus v)\oplus w$
\item\label{conmut} $\forall u,v\in V,\ u\oplus v=v\oplus u$
\item $\exists \Theta\in V,\ \forall u\in V,\ \Theta\oplus u=u$
\item $\forall u\in V,\ \exists v\in V,\ u\oplus v=\Theta$,
\item $\forall \alpha, \beta\in \K,\ \forall u,v\in V,$
  \begin{enumerate}
  \item $ \alpha\odot(\beta\odot u)=(\alpha\beta)\odot u, $
  \item $\alpha\odot(u\oplus v)=\alpha\odot u \oplus \alpha\odot v,$
  \item $(\alpha+\beta)\odot u=\alpha\odot u\oplus \beta\odot u$,
  \end{enumerate}
\item $\forall u\in V,\ 1\odot u=u$.
\end{enumerate}
\end{multicols}

Donde 1 denota el neutro multiplicativo del cuerpo $(\K,+,\cdot)$. 
\end{defi}

Los elementos del espacio vectorial $V$ se llaman \emph{vectores}, a los elementos del cuerpo $\K$ los llamamos \emph{escalares}. \\

\textcolor{blue}{Observar que en la definici\'on, $\alpha\beta$ corresponde a la multiplicaci\'on entre escalares (se est\'an multiplicando dos escalares y por tanto el resultado es un escalar) y que $\alpha \odot u$ corresponde a la ponderación por escalar binaria externa entre un escalar y un vector (su resultado es un vector).
Por otro lado, $\alpha+\beta$ es la operaci\'on suma de escalares y $u\oplus v$ es la operaci\'on suma de vectores.
Usaremos esta notación especial mientras nos acostumbramos a distinguir entre estas dos operaciones.
Al neutro del espacio vectorial lo denotaremos por $\Theta$ para distinguirlo de $0\in\K$.
}



Sea $\K$ un cuerpo, ejemplos de espacios vectoriales son:

\begin{itemize}
\item El conjunto de las tuplas de un cuerpo, $\K^n$, con las operaciones usuales de suma entre tuplas y ponderaci\'on escalar:

\begin{itemize}
\item $x=(x_1,\dots,x_n)$ y $y=(y_1,\dots,y_n)$ $\Rightarrow$ $x\oplus y=(x_1+y_1,\dots,x_n+y_n)$
\item $\alpha\in\K$ y $x=(x_1,\dots,x_n)$ $\Rightarrow$ $\alpha\odot x=(\alpha x_1,\dots,\alpha x_n)$
\end{itemize}
La clase pasada vimos que se cumplían algunos de los 6 axiomas de espacio vectorial, los demás se verifican de manera similar (¡es un buen ejercicio!).

%\begin{itemize}
%\item $A=(a_{ij})_{m\times n}$ y $B=(b_{ij})_{m\times n}$ $\Rightarrow$ $A\oplus B=(a_{ij}+b_{ij})_{m\times n}$
%\item $\alpha\in\K$ y $A=(b_{ij})_{m\times n}$ $\Rightarrow$ $\alpha\odot A=(\alpha a_{ij})_{m\times n}$
%\end{itemize}

\item El conjunto de los polinomios de grado menor o igual que $n$, denotado por $\P_n(\K)$, es espacio vectorial.

\begin{itemize}
\item $p(x)=\displaystyle\sum_{i=0}^n a_ix^i$ y $q(x)=\displaystyle\sum_{i=0}^nb_{i}x^i$ $\Rightarrow$ $(p\oplus q)(x)=\displaystyle\sum_{i=0}^n(a_{i}+b_i)x^i$
\item $\alpha\in\K$ y $p(x)=\displaystyle\sum_{i=0}^na_{i}x^i$  $\Rightarrow$ $(\alpha\odot p)(x)=\displaystyle\sum_{i=0}^n(\alpha a_{i})x^i$ 
\end{itemize}


\item El conjunto de las funciones de un conjunto $X$ sobre $\K$, denotado por $$\F(X,\K)=\{ f:X\rightarrow \K\ :\ f\textrm{ es función}\},$$ es espacio vectorial sobre $\K$ con las operaciones usuales de suma de funciones y multiplicaci\'on por escalar.

\begin{itemize}
\item $(f \oplus g)(x)=f(x) + g(x)$, con $x\in X$.
\item $\alpha\in\K$, $(\alpha \odot f)(x)=\alpha f(x)$, con $x\in X$.
\end{itemize}
Notamos que estas son las mismas operaciones que definimos para polinomios, ya que los polinomios son funciones.
Muchos de los axiomas de espacio vectorial los demostramos ya en el capítulo de ``Funciones Reales'' en el curso de Álgebra I.
\end{itemize}

Tal como en el caso de los cuerpos, podemos demostrar propiedades que se desprenden de los axiomas de Espacio Vectorial.
Notar que las propiedades de cuerpo no son las mismas que las propiedades de espacio vectorial, se requiere mucho más sigilo para trabajar con vectores, pues hay muchas cosas que ya no se pueden hacer, por ejemplo NO se puede ``dividir'' por un vector.

\begin{prop}
En cualquier espacio vectorial $(V,+,\odot)$ sobre un cuerpo $(\K,+,\cdot)$ se cumple que el neutro aditivo es \'unico, al igual que el inverso aditivo.
Si denotamos por 0 al neutro aditivo del cuerpo $\K$ y por $\Theta$ al neutro aditivo de $V$; y denotamos adem\'as por $-u$ al inverso aditivo de $u$, entonces para cualquier $u\in V$ y $\alpha \in \K$ se cumple lo siguiente.

\begin{multicols}{2}
\begin{enumerate}[a)]
\item $0\odot u=\Theta$
\item\label{a0} $\alpha\odot\Theta=\Theta$
\item\label{integridad} Sí $\alpha\odot u=\Theta, \textrm{ entonces } \alpha=0 \textrm{ o } u=\Theta.$
\item $-(-u)=u$
\item $(-1)\odot u=-u$
\item\label{-au} $(-\alpha)\odot u=-(\alpha\odot u)$
\item Si $\alpha\not =0$ y $\alpha\odot u=\alpha\odot v$, entonces $u=v$.
\item\label{cancela_u} Si $u\not =\Theta$ y $\alpha\odot u=\beta\odot u$, entonces $\alpha=\beta$.
\end{enumerate}
\end{multicols}
\end{prop}
{\bf Demostración.} { Sean $\alpha,\beta\in\K$ y $u\in V$ cualesquiera.
\begin{itemize}
\item[\ref{integridad})] Suponemos que es verdad el lado izquierdo de la implicancia, suponemos que es verdad que $\alpha\odot u=\Theta$.
Tenemos que demostrar que ya sea  $\alpha=0$ o bien $u=\Theta$.
Si $\alpha$ fuera 0, estaríamos listas, entonces supondremos que $\alpha\not =0$ y demostraremos que $u=\Theta$.
En ese caso existe el inverso mulutiplicativo de $\alpha$, podemos multiplicar la primera hipótesis por éste.
$$\begin{array}{rcll}
\alpha\odot u&=&\Theta &/\alpha^{-1}\odot(\phantom{h})\\
\Leftrightarrow\quad\alpha^{-1}\alpha\odot u&=&\alpha^{-1}\odot \Theta &\textit{(por ítem \ref{a0}))}\\
1\odot u&=&\Theta&\textit{(por axioma 8 de e.v.)}\\
u&=&\Theta
\end{array}$$
\item[\ref{cancela_u})] Como la ponderación no tiene inverso, hacemos un truco.
$$
\begin{array}{rcll}
\alpha\odot u&=&\beta\odot u & /+(-\beta\odot u)\\
\alpha\odot u+(-\beta\odot u)&=&\beta\odot u+(-\beta\odot u)&\textit{(inverso aditivo e ítem \ref{-au}))}\\
\alpha\odot u+(-\beta)\odot u&=&\Theta &\textit{(distributividad)}\\
(\alpha+(-\beta))\odot u&=&\Theta &\textit{(ítem \ref{integridad}) y $u\not=\Theta$)}\\
\alpha+(-\beta)&=&0&/+\beta\\
\alpha+(-\beta)+\beta&=&0+\beta&\textit{(neutro e inverso aditivo)}\\
\alpha&=&\beta&
\end{array}
$$
\end{itemize}
}
Desarrollaremos una vasta teoría a través del curso, la cual será válida para cualquier conjunto que cumpla los axiomas de espacio vectorial.
Para poder aplicar la teoría solo necesitaremos verificar que nuestro conjunto cumpla los axiomas de cuerpo, gracias a lo cual toda la teoría se cumplirá.
Esta es la gran ventaja de trabajar en forma abstracta sobre un espacio vectorial $V$ inespecífico, desarrollaremos la teoría una sola vez y será válida para todos los espacios vectoriales que se crucen en nuestro camino.


\section{Subespacios vectoriales}

El objeto principal de estudio serán los {\em subespacios vectoriales}, es decir los subconjuntos de un espacio vectorial que sean cerrados para las operaciones del espacio.
Estos son muy relevantes en las distintas ciencias, tal como se muestra en el siguiente video: \url{https://vimeo.com/596627274/09ec63235e}.

\begin{defi}[\textbf{Subespacio Vectorial}] Dado un espacio vectorial $(V,\oplus,\odot)$ sobre un cuerpo $\K$. Se dice que $S$ es un \emph{subespacio vectorial (s.e.v) de $V$} si $S$ es subconjunto de $V$ y, con las mismas operaciones binarias definidas en $V$, es tambi\'en un espacio vectorial. 
\end{defi}

En otras palabras, $(S,\oplus,\odot)$ satisface:\\

\hspace{2.5 cm}$\begin{array}{l}
 \oplus:S\times S\rightarrow S 
\end{array}$\hspace{5 cm}$\begin{array}{l}
\odot:\K\times S\rightarrow S
\end{array}$

\begin{multicols}{2}
\begin{enumerate}
\item $(\forall u,v,w\in S)\ u\oplus (v\oplus w)=(u\oplus v)\oplus w$
\item\label{conmut} $(\forall u,v\in S)\ u\oplus v=v\oplus u$
\item $(\exists \Theta\in S)(\forall u\in S)\ \Theta\oplus u=u$
\item $(\forall u\in S)(\exists v\in S)\ u\oplus v=\Theta$,
\item $(\forall \alpha, \beta\in \K)(\forall u,v\in S)$
  \begin{enumerate}
  \item[a.] $ \alpha\odot(\beta\odot u)=(\alpha\beta)\odot u, $
  \item[b.] $\alpha\odot(u\oplus v)=\alpha\odot u\oplus \alpha\odot v,$
  \item[c.] $(\alpha+\beta)\odot u=\alpha\odot u\oplus \beta\odot u$,
  \end{enumerate}
\item $(\forall u\in S)\ 1\odot u=u$.
\end{enumerate}
\end{multicols}


\vspace{0.5 cm}


\begin{ejem} {\em Para probar que $S=\left\{\left(\begin{array}{c}x\\y\\z\end{array}\right)\in\R^3: xy=0\right\}\subset \R^3$ es un subespacio vectorial de $\R^3$, las operaciones $\oplus:S\times S\rightarrow S$ y $\odot:\R\times S\rightarrow S$ deben cumplir las propiedades.\\

En este caso, $S$ no es subespacio vectorial puesto que el resultado de la suma $\oplus$ de dos elementos de $S$ no est\'a en $S$ (no se cumple: $\oplus:S\times S\rightarrow S$), por ejemplo:

\hspace{5 cm}$\left(\begin{array}{c}1\\0\\1\end{array}\right)\in S$  y $\left(\begin{array}{c}0\\1\\2\end{array}\right)\in S$, su suma es $\left(\begin{array}{c}1\\1\\3\end{array}\right)\notin S$\\
}
\end{ejem}

\begin{ejem}{\em $S=\left\{\left(\begin{array}{c}x\\y\end{array}\right)\in\R^2: x+y=0\right\}\subset \R^2$ es un subespacio vectorial de $\R^2$ sobre $\R$.
De partida vemos que las operacions de suma y ponderación donde participan los vectores de $S$ arrojan resultados que también están en $S$, veamos.

Sea $(x,y), (a,b)\in S$, esto significa que $x+y=0$ y $a+b=0$. 
La suma: $(x+a,y+b)$ cumple $x+a+y+b=x+y+a+b=0+0=0$, por l tanto $(x,y)\oplus(a,b)\in S$.

Asimismo, si $\alpha\in\R$ y $(x,y)\in S$, entonces $\alpha\odot(x,y)=(\alpha x, \alpha y)$ verifica que $\alpha x+ \alpha y=\alpha(x+y)=\alpha0=0$, por lo tanto $\alpha\odot(x,y)\in S$.

Por otra parte, los axiomas 1,2,5 y 6 de espacio vectorial se cumplen para todos los elementos de $\R^2$, en particular para los elementos de $S$ también se cumplen, no necesitamos verificarlos nuevamente.
Solo falta verificar los axiomas 3 y 4.
Pero el neutro de $\R^2$ es $(0,0)\in S$ ya que $0+0=0$.
Por otra parte si $x+y=0$, entonces $-x-y=0$, así $(-x,-y)\in S$, se cumple el axioma 4.
}
\end{ejem}

\textcolor{blue}{De aqu\'i en adelante, para simplificar notaci\'on se ocupar\'a $+$ en lugar de $\oplus$ y se entender\'a que $\alpha+\beta$ es la suma en cuerpo cuando $\alpha$ y $\beta$ son escalares y que $u+v$ es la suma de vectores cuando $u$ y $v$ son vectores. Tambi\'en, se considerar\'a que $\alpha \beta$ es la multiplicaci\'on en cuerpo cuando $\alpha$ y $\beta$ son escalares y que $\alpha u$ es la ponderación cuando $u$ es un vector.}\\

Una forma m\'as f\'acil de comprobar si un conjunto es un subespacio vectorial es aplicando el siguiente lema.\\

\begin{lema}
  Dado $V$ un $\K$-espacio vectorial y $S\subset V$, se cumple que:
  $S$ es un s.e.v. de $V$ sobre $\K$ s\'i y s\'olo si se cumplen:
\begin{enumerate}
\item $\Theta\in S$ $($el nulo de $V$ est\'a en $S$ $)$.
\item $\forall\;v,u\in S,$ $u+v\in S$ $($la suma es cerrada$)$.
\item $\forall\;\alpha\in\K,$\;$\forall\;u\in S,$ $\alpha u\in S$ $($la multiplicaci\'on por escalar es cerrada$)$.
\end{enumerate}
\end{lema}
    {\bf Demostración.} {
      \begin{itemize}
      \item[($\Rightarrow$)] Si $S$ es un s.e.v., entonces es un $\K$-espacio vectorial, y por lo tanto es claro que cumple las tres propiedades pues son parte de estas, en efecto: 1. es el axioma 3 de espacio vectorial; 2. viene de la definición de la suma en $S$; y 3. viene de la definción de la ponderación en $S$ sobre $\K$.
      \item[($\Leftarrow$)] Si $S$ cumple las 3 propiedades, entonces ya sabemos al menos que la suma y la ponderación están definidas dentro de $S$ sobre $\K$.
        Por otra parte, como $S\subseteq V$ y $V$ es un $\K$-espacio vectorial, es claro que se cumplen los axiomas 1,2,5 y 6 de espacio vectorial.
        La hipótesis incluye al axioma 3 de espacio vectorial, solo falta verificar el axioma 4: la existencia del inverso aditivo dentro de $S$.
        
        Pero esta se deduce de lo siguiente: llamemos $-1$ al inverso aditivo de $1\in\K$, si recordamos las propiedades de espacio vectorial que se desprenden de los axiomas obtenemos que $$-u=-(1u)=(-1)u,$$ es decir, el inverso aditivo de un vector se obtiene ponderando al vector por $-1$.
        Como la hipótesis nos dice que $S$ es cerrado para la ponderación por cualquier escalar, entonces si $u\in S$, se tiene $-u\in S$.\hfill $\Box$
        \end{itemize}
      }


\vspace{0.5 cm}

\begin{ejem}{\em
$S=\left\{\left(\begin{array}{c}x\\y\\z\end{array}\right)\in\R^3: x+y=1\right\}\subset \R^3$ no es s.e.v  porque el nulo de $\R^3$ no est\'a en $S$.}
\end{ejem}
\begin{ejem}{\em
El conjunto de las funciones de $\R$ en $\R$ que toman solo valores mayores o iguales a 0 no es un s.e.v del conjunto de todas las funciones, pues si bien contiene a la función nula y es cerrado para la suma de funciones, NO es cerrado para la ponderación de funciones, ya que si pondero una función positiva por $-1$, obtengo una función que toma valores negativos.}
\end{ejem}
\begin{ejem}{\em
  \begin{itemize}
\item  $\P_5(\R)$ es s.e.v de $\P(\R)$. {\em En efecto: $\P_5(\R)$ es un espacio vectorial y $\P_5(\R)\subseteq \P(\R)$ }
\item  El conjunto de las funciones continuas de $\R$ en $\R$ es s.e.v. del conjunto de las funciones cualesquiera de $\R$ en $\R$. {\em En efecto: la suma de funciones continuas es continua; la ponderación por escalar de una función continua es continua; además, la función constante igual a 0 es una función continua.}
\end{itemize}}
\end{ejem}

\begin{ejem}{\em
  $$U=\{ (x,x,0,y)\in \C^4\ :\ x,y\in \C\}$$
Cumple las tres proiedades:
\begin{enumerate}
\item $(0,0,0,0)\in U$, para mostrarolo basta tomar $x=y=0\in \C$, obteniendo $(x,x,0,y)=(0,0,0,0)$.
\item Si tengo dos vectores arbitrarios en $U$, digamos $(x,x,0,y)$ y $(a,a,0,b)$, y los sumo, obtengo: $(x+a,x+a,0,y+b)$, el cual también es un vector de $U$ ya que tiene la forma de los elementos de $U$.
\item  Si pondero un elemento $(x,x,0,y)$ de $U$, con $x,y\in\C$ cualesquiera, por un escalar cualquiera $\alpha$ en $\C$, obtengo $(\alpha x,\alpha x ,0,\alpha y)$, que nuevamente se puede probar que está en $U$ tomando $x'=\alpha x\in\C$ y $y'=\alpha y\in\C$, los cuales están en $\C$ ya que $\alpha, x,y\in\C$ y la multiplicación de elementos de $\C$ está en $\C$.
\end{enumerate}}
\end{ejem}
\begin{ejem}{\em
  $$W=\{ (x,\overline{x})\in\C^2\ :\ x\in \C\}$$
  Veamos si cumple o no las 3 propiedades:
  \begin{enumerate}
\item $(0,0)\in W$, para mostrarolo basta tomar $x=0\in \C$, obteniendo $(x,\overline{x})=(0,\overline{0})=(0,0)$.
\item Si tengo dos vectores arbitrarios en $W$, digamos $(x,\overline{x})$ y $(y,\overline{y})$, y los sumo, obtengo:\break $(x+y,\overline{x}+\overline{y})=(x+y,\overline{x+y})$, el cual también es un vector de $W$ ya que tiene la forma de los elementos de $W$.
\item  Si pondero un elemento $(x,\overline{x})$ de $W$, con $x\in\C$ cualquiera, por un escalar cualquiera $\alpha$ en $\C$, obtengo $(\alpha x,\alpha \overline{x})=(\alpha x,\overline{\overline{\alpha}x})$... no pareciera ser posible llevarlo a la forma $(y,\overline{y})$ para algún $y\in\C$...

  Tomemos un contraejemplo entonces: si tomo $x=1+i$, obtengo $(1+i,1-i)\in W$.

  Si escojo ahora $\alpha=i$ obtengo
  \begin{eqnarray*}
    \alpha(1+i,1-i)&=&i(1+i,1-i)\\
    &=&(i(1+i),i(1-i))\\
    &=&(i-1,i+1)\not\in W,
  \end{eqnarray*}
  ya que estoy obligada a tomar $y=i-1$, pero entonces $\overline{y}=-i-1\not=i+1$.
\end{enumerate}
}
\end{ejem}


Trabajaremos mucho con los subespacios vectoriales, pues estos aparecen naturalmente en muchos modelos y aplicaciones de la ingeniería, y serán {\bf el objeto central} de este curso.

En $\R^2$ y de $\R^3$, los únicos subespacios vectoriales que encontraremos son: 

\begin{multicols}{2}
\begin{itemize}
\item El espacio que solo contiene al nulo $\{\Theta\}$ (espacio trivial).
\item Las rectas que pasan por el origen.
\item Los planos que pasan por el origen.
\item $V$ es siempre subespacio de sí mismo.
\item $\R^2$ NO es subespacio de $\R^3$ pues NO está contenido en este.
\end{itemize}
\end{multicols}

\newpage

\begin{center}
{\Large Operaciones entre Subespacios}
\end{center}

\vspace{.3cm}

{\bf Recuerdo}\\{\em
Dados dos subconjuntos $W$, $U$ de $V$, su \emph{intersección} se define como sigue.

\[ W\cap U=\{s\in V\ |\ s\in W\ \textrm{ y }\ s\in U\}\]
}

Nos va a interesar particularmente intersectar subespacios y ver qué obtenemos.
Por ejemplo la intersección entre dos planos diferentes de $\R^3$ que pasan por el origen nos da una recta que también pasa por el origen.
La intersección entre dos rectas diferentes por el origen, nos da un punto: el origen.

Esta operación de conjuntos la conocemos, introduzcamos otra operación entonces.


\begin{defi}
Dados dos subconjuntos $W$, $U$ de $V$, se define su \emph{suma} como sigue.

\[ W+U=\{w+u\ |\ w\in W\ \textrm{ y }\ u\in U\}\]
\end{defi}

\begin{ejem}{\em 
Si tenemos por ejemplo el conjunto $A=\{1,3,4\}$ y el conjunto $B=\{1,2\}$, entonces 
\begin{eqnarray*}
A\cap B&=&\{1\}\\
A+B&=& \{1+1,1+2,3+1,3+2,4+1,4+2\}\\
&=&\{2,3,4,5,6\}
\end{eqnarray*}
}
\end{ejem}

\begin{prop}
Dados dos s. e. v. $W$, $U$ de $V$, resulta que $W\cap U$ y $W+U$ son tambi\'en s. e. v. de $V$.
\end{prop}
{\bf Demostración.} Sean $W$, $U$ s.e.v. de $V$ sobre $\K$.
Esto significa que ambos cumplen las tres propiedades que caracterizan a los s.e.v. (y todas las demás, por cierto):
\begin{enumerate}
\item $\Theta \in U$ y $\Theta\in W$.
\item $U$ y $W$ son cerrados para la suma.
\item $U$ y $W$ son cerrados para la ponderación.
\end{enumerate}
Esta es nuestra hipótesis.
Veamos ahora si el conjunto intersección y el conjunto suma cumplen también las 3 propiedades.
\begin{itemize}
\item[$W\cap U$] Analicemos cada una de las 3 propiedades. 
Recordamos la definición de la intersección:
\[ W\cap U=\{s\in V\ |\ s\in W\ \textrm{ y }\ s\in U\}\]
\begin{enumerate}
\item Debemos demostrar que $\Theta\in W\cap U$, lo cual equivale a mostrar que $\Theta \in W\ \textrm{ y }\ \Theta\in U$; pero esto es cierto de la hipótesis 1.
\item Sean dos vectores $ x,y \in W\cap U$, esto equivale a decir que  $x,y\in W\ \textrm{ y }\ x,y\in U$.
Debemos demostrar que $x+y\in W\cap U$, es decir,  $x+y\in W\ \textrm{ y }\ x+y\in U$.
Pero por la hipótesis 2, y dado que $x,y\in W$, tenemos que $x+y\in W$, por otra parte, y dado que $x,y\in U$, tenemos que $x+y\in U$, así $x+y\in W\cap U$.
\item Sea un vector $ x \in W\cap U$, esto equivale a decir que  $x\in W\ \textrm{ y }\ x\in U$, y sea un escalar $\alpha\in\K$.
Debemos demostrar que $\alpha x\in W\cap U$, es decir,  $\alpha x\in W\ \textrm{ y }\ \alpha x\in U$.
Pero por la hipótesis 3, y dado que $x\in W$, tenemos que $\alpha x\in W$, por otra parte, y dado que $x\in U$, tenemos que $\alpha x\in U$, así $\alpha x\in W\cap U$.
\end{enumerate}
\item[$W+U$] Analicemos cada una de las 3 propiedades. 
Recordamos la definición de la suma:
\[ W+U=\{w+u\ |\ w\in W\ \textrm{ y }\ u\in U\}\]
\begin{enumerate}
\item Debemos demostrar que $\Theta\in W+ U$, lo cual equivale a mostrar que existe $w\in W\ \textrm{ y }\ u\in U$ tales que $\Theta=w+u$; pero si usamos la hipótesis 1, vemos que podemos tomar $w=\Theta$ y $u=\Theta$, se cumple $w+u=\Theta+\Theta=\Theta$, así $\Theta\in W+U$.
\item Sean dos vectores $ x,y \in W+ U$, esto equivale a decir que existen $w_1,w_2\in W\ \textrm{ y }\ u_1,u_2\in U$ tales que $x=w_1+u_1$ e $y=w_2+u_2$.
Debemos demostrar que $x+y\in W+ U$, es decir, que existen $w_3\in W\ \textrm{ y }\ u_3\in U$ tales que $x+y=w_3+u_3$.
Pero $x+y=w_1+u_1+w_2+u_2=w_1+w_2+u_1+u_2$, entonces tomando $w_3=w_1+w_2$, el cual sabemos por la hipótesis 2 que está en $W$, y tomando $u_3=u_1+u_2$ que también sabemos que está en $U$, se tiene $x+y=w_3+u_3\in W+ U$.
\item Sea un vector $ x \in W+ U$, esto equivale a decir que existen  $w\in W\ \textrm{ y }\ u\in U$ tales que $x=w+u$, y sea un escalar $\alpha\in\K$.
Debemos demostrar que $\alpha x\in W+ U$, es decir,  que existen $w'\in W\ \textrm{ y }\ u'\in U$ tales que $\alpha x=w'+u'$.
Pero $\alpha x=\alpha (w+u)=\alpha w+\alpha u$.
Por la hipótesis 3, y dado que $w\in W$, tenemos que $\alpha w\in W$, por otra parte, y dado que $u\in U$, tenemos que $\alpha u\in U$, así $\alpha w+\alpha u\in W+ U$, en otras palabras, basta tomar $w'=\alpha w\in W$ y $u'=\alpha u\in U$ para demostrar que $\alpha x=w'+u'$ y por lo tanto $\alpha x\in W+U$. \hfill $\Box$
\end{enumerate}
\end{itemize}


Se observa que la uni\'on de s. e. v. no es en general un s. e. v., es m\'as, la \'unica forma que la uni\'on de dos s. e. v. d\'e como resultado un s.e.v. es que uno sea subconjunto del otro.
Por eso es que la unión de s.e.v. no tiene uso en álgebra lineal.

Por otra parte, el subespacio más pequeño que contiene a $W\cup U$ es justamente $W+U$, tal como establece el siguiente resultado.

\begin{prop}
Si $W\cup U\subseteq S$ y $S$ es subespacio vectorial de $V$, entonces $W+U\subseteq S$.
\end{prop}
    {\bf Demostración.}
    {
      Nuestras hipótesis son: $W\cup U\subseteq S$ y $S$ es s.e.v. de $V$.

      Debemos demostrar que $W+U\subseteq S$.
      Tomemos entonces un elemento arbitrario de $W+U$, para esto tomamos un elemento arbitrario $w\in W$ y otro $u\in U$, así $w+u$ es un elemento arbitrario de $W+U$; ahora debemos demostrar que $w+u\in S$.

      Pero sabemos que $w,u\in W\cup U\subseteq S$, y como $S$ es s.e.v, es cerrado para la suma, por lo tanto $w+u$ in $S$.
    }\hfill $\Box$



\section{Combinaciones Lineales}

\begin{defi}[Combinaci\'on Lineal] Dado un espacio vectorial $V$ sobre un cuerpo $\K$, y dado $\{v_1,v_2,\dots,v_k\}$ un subconjunto de $V$. \\
El vector $w\in V$ es {\em combinaci\'on lineal} de $\{v_1,v_2,\dots,v_k\}$ si existen $\alpha_1,\alpha_2,\dots,\alpha_k\in\K$ tales que:
$$w=\alpha_1v_1+\alpha_2v_2+\dots+\alpha_kv_k$$
\end{defi}

\vspace{0.2 cm}

\begin{ejem} {\em Si consideramos el espacio vectorial $\R^2$ sobre $\R$ y el conjunto $G=\{(2,1),(-1,1)\}$, una combinaci\'on lineal de este conjunto es el vector $(1,5)$:
$$(1,5)=2(2,1)+3(-1,1)$$
aqu\'i $v_1=(2,1)$, $v_2=(-1,1)$, $\alpha_1=2$ y $\alpha_2=3$.

¿Qué otros vectores son c.l. de $G$? ¿Es $(0,0)$ c.l. de $G$? ¿es $(2,1)$ c.l. de $G$?
}  
\end{ejem}

\begin{ejem} {\em Sea el conjunto $H=\{(0,1,3),(1,0,1)\}\subseteq \R^3$, considerando a $\R^3$ como espacio vectorial sobre $\R$, una combinación lineal  de $H$ es $(-2,1,1)$:
$$(-2,1,1)=1(0,1,3)-2(1,0,1)$$
Sin embargo $(1,1,1)$ no es combinación lineal de $H$, para que lo fuera, necesitaríamos que existieran reales $\alpha, \beta$ tales que:
\begin{eqnarray*}
(1,1,1)&=&\alpha(0,1,3)+\beta(1,0,1))\\
&=&(0,\alpha,3\alpha)+(\beta,0,\beta)\\
&=&(b,\alpha,3\alpha+\beta)
\end{eqnarray*}
Dado que la igualdad de tuplas es una igualdad coordenada a coordenada, esta ecuación vectorial se traduce en 3 ecuaciones:
\begin{eqnarray}
1&=&\beta\label{eq:uno}\\
1&=&\alpha\label{eq:dos}\\
1&=&3\alpha+\beta\label{eq:tres}\\
\end{eqnarray}
De las dos primeras ecuaciones ya sabemos que $\beta=1$ y $\alpha=1$, pero si reemplazamos estos valores en la tercera ecuación obtenemos: $3\alpha+\beta=4\not=1$, por lo tanto las tres ecuaciones no se pueden cumplir simultáneamente, $\alpha$, $\beta$ no existen.

¿Es $(1,-1,-2)$ combinaci\'on lineal de los elementos de $H$?.
}
\end{ejem}


\subsection{Subespacio generado por un conjunto de vectores}

\begin{defi}[Conjunto Generador] Dado un espacio vectorial $V$ sobre un cuerpo $\K$, y $\{v_1,v_2,\dots,v_k\}$ un subconjunto de $V$. El conjunto de todas las combinaciones lineales de $\{v_1,v_2,\dots,v_k\}$ se denomina {\em conjunto generado} por $\{v_1,v_2,\dots,v_k\}$ y se denota por:
$$\langle\{v_1,v_2,\dots,v_k\}\rangle,$$
luego, $u\in\langle\{v_1,v_2,\dots,v_k\}\rangle$ si y solo si existen $\alpha_1,\alpha_2,\dots,\alpha_k\in\K$ tales que:
$$u=\alpha_1v_1+\alpha_2v_2+\dots+\alpha_kv_k.$$
Dicho en otras palabras:
$$\langle\{v_1,v_2,\dots,v_k\}\rangle=\{\alpha_1v_1+\alpha_2v_2+\dots+\alpha_kv_k\ :\  \alpha_1,\alpha_2,\dots,\alpha_k\in\K\}.$$
Adem\'as, si $S=\langle\{v_1,v_2,\dots,v_k\}\rangle$ entonces se dice que $S$ {\em es generado por} $\{v_1,v_2,\dots,v_k\}$, y que $\{v_1,v_2,\dots,v_k\}$ \emph{genera a} $S$ o que \emph{es generador de} $S$.

Por \'ultimo, se define $\langle \phi\rangle=\{\Theta\}$.
\end{defi}


\begin{ejem} {\em No es difícil ver que $\R^2=\langle\{(1,0),(0,1)\}\rangle$, en efecto:
$$ (x,y)=x(1,0)+y(0,1)$$
}  
\end{ejem}

\vspace{0.2 cm}

\begin{ejem}{\em
Si consideramos el espacio vectorial de los polinomios de grado menor o igual a 4: $\mathcal{P}_3(\R)$, y consideramos los polinomios siguientes:
$$p(x)=1, q(x)=x, r(x)=x^2, s(x)=x^3,$$ vemos fácilmente que $\mathcal{P}_3(\R)=\langle\{p,q,r,s\}\rangle$, en efecto, si tomamos un polinomio arbitrario de $\mathcal{P}_3(\R)$: $t(x)=a_0+a_1x+a_2x^2+a_3x^3$, tenemos que:
$$t=a_0p+a_1q+a_2r+a_3s$$
%Encuentre un conjunto generador de $U=\left\{ax^2+bx+c\in\P(\R): a+2b-c=0\right\}\subseteq \P(\R)$.\\
%
%Considere un elemento arbitrario de $U$, sea $ax^2+bx+c\in U$, luego $a+2b-c=0$ y por tanto $a=c-2b$, as\'i $ax^2+bx+c=(c-2b)x^2+bx+c$, de aqu\'i:\\
%
%\hspace{5 cm}$ax^2+bx+c=b(x-2x^2)+c(1+x^2),$\hspace{1 cm} $b,c\in\R$\\
%
%es decir, todo elemento de $U$ es una combinaci\'on lineal de $x-2x^2$ y $1+x^2$, por tanto el conjunto generador de $U$ es $\{x-2x^2,1+x^2\}$ y esto se expresa escribiendo:
%
%$$U=\langle\{x-2x^2,1+x^2\}\rangle.$$
}  
\end{ejem}

\vspace{0.2 cm}

\begin{ejem}
{\em Ya habíamos visto que 
$$(1,5)=2(2,1)+3(-1,1),$$
por tanto $(1,5)\in\langle\{(2,1),(-1,1)\}\rangle$.

La notaci\'on es importante, $\{(2,1),(-1,1)\}$ es un conjunto de 2 elementos, en cambio\\
 $\langle\{(2,1),(-1,1)\}\rangle$ considera todas las combinaciones lineales de esos dos vectores.
\begin{itemize}
\item $(1,5)\in \langle\{(2,1),(-1,1)\}\rangle$.
\item $(1,5)\notin\{(2,1),(-1,1)\}$.
\end{itemize}
También es importante no confundir confundir $\in$ con $\subseteq$ (ni $\phi$ con $\Theta$).



¿Qué otros vectores hay en $\langle\{(2,1),(-1,1)\}\rangle$? exploremos esta pregunta de manera gráfica primero para adquirir una intuición en este tema.

Por ejemplo, si $v=(2,1)\in\R^2$, entonces $\langle\{v\}\rangle$ contendrá todas las ponderaciones de $v$ por todos los escalares que podamos imaginar. El conjunto resultante se grafica así:

\begin{center}
\begin{tikzpicture}
 	\draw[arrows=->] (-5,0) -- (5,0) node[below] {};
      	\draw[arrows=->] (0,-2.6) -- (0,2.6) node[left] {};
	\foreach \pos in {-4,-3,-2,-1,1,2,3,4}
    		\draw[shift={(\pos,0)}] (0,.1) -- (0,-.1) node[below] {$\pos$};
	\foreach \pos in {-2,-1,1,2}
    		\draw[shift={(0,\pos)}] (.1,0) -- (-.1,0) node[left] {$\pos$};
	
	\coordinate[label=right:$v$] (J) at (2,1);
	\coordinate[label=left:\color{red}{$\langle\{v\}\rangle$}] (M) at (-2,-1);
	\draw[red] (-4,-2) -- (4,2);
	\draw[arrows=->,color=black,very thick] (0,0)--(J);
\end{tikzpicture} 
\end{center}
Es la recta por el origen que es paralela al vector $v$ y es un subespacio vectorial.

Ahora si $u=(-1,1)$ entonces $\langle\{u,v\}\rangle$ contendrá todas las ponderaciones de $v$, todas las ponderaciones de $u$ y todos los vectores que resulten de esas ponderaciones:

\begin{center}
\begin{tikzpicture}
 	\draw[arrows=->] (-5,0) -- (5,0) node[below] {};
      	\draw[arrows=->] (0,-2.6) -- (0,2.6) node[left] {};
	\foreach \pos in {-4,-3,-2,-1,1,2,3,4}
    		\draw[shift={(\pos,0)}] (0,.1) -- (0,-.1) node[below] {$\pos$};
	\foreach \pos in {-2,-1,1,2}
    		\draw[shift={(0,\pos)}] (.1,0) -- (-.1,0) node[left] {$\pos$};
	
	\coordinate[label=right:$v$] (J) at (2,1);
	\coordinate[label=left:\color{red}{$\langle\{v\}\rangle$}] (M) at (-2,-1);
	\draw[red] (-4,-2) -- (4,2);
	\draw[arrows=->,color=black,very thick] (0,0)--(J);

	\coordinate[label=right:$u$] (I) at (-1,1);
	\coordinate[label=left:\color{blue}{$\langle\{u\}\rangle$}] (P) at (-2,2);
	\draw[blue] (-2,2) -- (2,-2);
	\draw[arrows=->,color=black,very thick] (0,0)--(I);
	
	\draw[dotted, fill=black] (-1.5,1.5) --++(-2,-1.0) circle(0.03) --++(1.5,-1.5);
	\draw[dotted, fill=black] (.5,-.5) --++(4,2.0) circle(0.03) --++(-.5,.5);
	\draw[dotted, fill=black] (1.5,-1.5) --++(1,.5) circle(0.03) --++(-1.5,1.5);
	\draw[dotted, fill=black] (-.5,.5) --++(2,1.0) circle(0.03) --++(.5,-.5);
\end{tikzpicture} 
\end{center}
Gráficamente vemos que $\left\langle\left\{u,v\right\}\right\rangle=\R^2$, sin embargo también podemos verlo algebraicamente:

\begin{eqnarray*}
(x,y)\in\langle\{(2,1),(-1,1)\}\rangle&\Leftrightarrow&\exists \alpha,\beta\in\R,\ (x,y)=\alpha(2,1)+\beta(-1,1)\\
&\Leftrightarrow&\exists \alpha,\beta\in\R,\ x=2\alpha-\beta \textrm{ e } y=\alpha+\beta\\
&\Leftrightarrow&\exists \alpha,\beta\in\R,\ x=2\alpha-\beta \textrm{ e } x+y=3\alpha\\
&\Leftrightarrow&\exists \alpha,\beta\in\R,\ x-2\alpha=-\beta \textrm{ e } \frac{x+y}{3}=\alpha\\
&\Leftrightarrow&\exists \alpha,\beta\in\R,\ x-2\frac{x+y}{3}=-\beta \textrm{ e } \frac{x+y}{3}=\alpha\\
&\Leftrightarrow&\exists \alpha,\beta\in\R,\ \frac{x-2y}{3}=-\beta \textrm{ e } \frac{x+y}{3}=\alpha\\
&\Leftrightarrow& \tt{Verdadero}
\end{eqnarray*}

Como resolvimos el sistema, y encontramos que su solución existe para cualquier $x$, $y\in\R$, se concluye que todo par $(x,y)\in\langle\{(2,1),(-1,1)\}\rangle$, es decir $\in\langle\{(2,1),(-1,1)\}\rangle=\R^2$.
Podemos decir entonces que $\{(2,1),(-1,1)\}$ genera a $\R^2$.

}
\end{ejem}


\begin{prop}
$\langle \{v_1,\dots,v_k\}\rangle$ es un s. e. v.
\end{prop}
{\bf Demostración.} {
Primero vemos que el conjunto generado contiene al vector nulo (basta tomar todos los ``alfas'' iguales a 0): 
$$\Theta=0\cdot v_1+0\cdot v_2+\cdots+0\cdot v_k.$$

Además la suma de dos combinaciones lineales es también una combinación lineal, en efecto, si $p,q\in\langle\{v_1,v_2,\dots,v_k\}\rangle$, entonces $p$ se escribe como $\sum_{i=1}^k\alpha_iv_i$, y $q$ como $\sum_{i=1}^k\beta_iv_i$, por lo tanto

$$p+q=\sum_{i=1}^k\alpha_iv_i+\sum_{i=1}^k\beta_iv_i=\sum_{i=1}^k(\alpha_i+\beta_i)v_i\in\langle \{v_1,..,v_k\}\rangle.$$

Por otra parte, la ponderación por escalar de una combinación lineal también es una combinación lineal, en efecto, si $\lambda\in\K$, entonces 
$$\lambda p=\lambda\sum_{i=1}^k\alpha_iv_i=\sum_{i=1}^k\lambda\alpha_iv_i\in \langle \{v_1,..,v_k\}\rangle.$$
\hfill $\Box$
    }

Pero podemos decir todavía más, les dejamos esta propiedad adicional para que la saboreen.


\begin{prop}
$\langle \{v_1,\dots,v_k\}\rangle$ coincide con el s. e. v. m\'as peque\~no que contiene a $\{v_1,\dots,v_k\}$.
\end{prop}
    {\bf Demostración.} {
Ya vimos que $\langle \{v_1,\dots,v_k\}\rangle$ es un s.e.v. 

Además, contiene a $\{v_1,\dots,v_k\}$, para verlo basta tomar igual a 1 el ``alfa'' que acompaña el vector que queremos, y nulos los demás, por ejemplo: $v_2=0\cdot v_1+1\cdot v_2+0\cdot v_3+\cdots+0\cdot v_k$.

Para ver que es el más pequeño de todos los s.e.v que contienen a $\{v_1,..,v_k\}$, consideremos otro s.e.v $S$ que también contiene a $\{v_1,\dots,v_k\}$.

Como $S$ es cerrado para la ponderación, entonces contiene a $\alpha_iv_i$, para cualquier $i$ y cualquier $\alpha_i\in\K$.

Como $S$ es cerrado para la suma, entonces contiene $\sum_{i=1}^k\alpha_iv_i$.

Por lo tanto contiene a $\langle \{v_1,\dots,v_k\}\rangle$.
\hfill $\Box$
    }

\section{Independencia Lineal}

\begin{defi}
\textbf{\emph{(Dependencia e Independencia Lineal)}} Sea $V$ un espacio vectorial sobre un cuerpo $\K$. El subconjunto $\{v_1,v_2,\dots,v_k\}\subset V$ se dice que es {\em linealmente independiente (l.i.)} si y s\'olo si la única manera de obtener 
$$\alpha_1v_1+\alpha_2v_2+\dots+\alpha_kv_k=\Theta$$
es mediante 
$$\alpha_1=\alpha_2=...=\alpha_k=0$$
Por el contrario, el conjunto $\{v_1,v_2,\dots,v_k\}$ se dice que es {\em linealmente dependiente (l.d.)} cuando NO es linealmente independiente, es decir, si y s\'olo si existen $\alpha_1,\alpha_2,\dots,\alpha_k\;\in\;\K$, no todos iguales a cero, tales que $\alpha_1v_1+\alpha_2v_2+\dots+\alpha_kv_k=\Theta$.
\end{defi}



\begin{ejem}
{\em
\begin{itemize}
\item El conjunto $\{(1,0,0),(0,1,0),(0,0,1)\}$ es linealmente independiente en $\R^3 $ sobre $\R$, pues si planteo la ecuación
$$\alpha (1,0,0)+\beta(0,1,0)+\gamma(0,0,1)=(0,0,0)$$
Obtengo directamente que $\alpha=0$, $\beta=0$, y $\gamma=0$.
\item Si nos permitimos denotar un polinomio a través de su fórmula, entonces el siguiente es un conjunto de polinomios y es linealmente independiente $\{1,x,x^2,x^3\}$.
En efecto, si planteamos la ecuación
$$\forall x\in\R,\ \alpha(1)+\beta(x)+\gamma(x^2)+\delta(x^3)=0$$
vemos que la única manera que se cumpla en todo $x$ es que todos los coeficientes del polinomio resultante sean nulos, es decir que $\alpha=\beta=\gamma=\delta=0$.
\item El conjunto $\{(2,1),(-1,1)\}$ también es l.i. en $\R^2$ sobre $\R$.
Para probarlo planteamos la ecuación de independencia lineal:
$$\alpha(2,1)+\beta(-1,1)=(0,0)$$
Esta equivale a: $2\alpha-\beta=0$ y $\alpha+\beta=0$. 
De la primera obtengo $\beta=2\alpha$, si reemplazo en la segunda, obtengo $\alpha+2\alpha=3\alpha=0$, de donde $\alpha=0$ y por consiguiente $\beta=0$.
\end{itemize}
}
\end{ejem}

El interés principal de la independencia lineal es que nos dice que si un vector es generado por un conjunto l.i., entonces hay una sola manera de obtenerlo como combinación lineal del conjunto.

\begin{prop}
Si $\{v_1,\dots,v_n\}$ es l. i. y  $\displaystyle{ \sum_{i=1}^n \alpha_iv_i=\sum_{i=1}^n \beta_iv_i}$, entonces $\alpha_1=\beta_1$, $\alpha_2=\beta_2$,...,$\alpha_n=\beta_n$.
\end{prop}
{\bf Demostración.} {
Analicemos la hipótesis.
$$\alpha_1v_1+\alpha_2v_2+\cdots+\alpha_n v_n=\beta_1v_1+\beta_2v_2+\cdots+\beta_n v_n$$
Restemos lado derecho a la ecuación y usemos la distributividad.
$$(\alpha_1-\beta_1)v_1+(\alpha_2-\beta_2)v_2+\cdots+(\alpha_n-\beta_n) v_n=\Theta$$
Ahora usamos la hipótesis de independencia lineal del conjunto $\{v_1,..,v_n\}$, que nos dice que si tenemos una ecuación como la anterior, entonces los escalares que ponderan a cada vector del conjunto deben ser nulos:
$$\forall i\in\{1,2,\dots,n\},\ \alpha_i-\beta_i=0$$
Es decir, $\forall i\in\{1,2,\dots,n\},\ \alpha_i=\beta_i$, que era lo que se quería demostrar.\hfill $\Box$
}

Cuando el conjunto es linealmente dependiente la representación de los vectores que genera no es única.

\begin{ejem}
{\em 
\begin{itemize}
\item El conjunto $\{(1,5),(2,1),(-1,1)\}$ es linealmente dependiente en $\R^2$ sobre $\R$, en efecto:
$$-(1,5)+2(2,1)+3(-1,1)=(0,0),$$
es decir, (0,0) se puede obtener de dos maneras diferentes, la que acabamos de dar, y la trivial: 
$$0(1,5)+0(2,1)+0(-1,1)=(0,0).$$
\item El conjunto $\{(0,0,0)\}$ es linealmente dependiente, por la misma razón anterior:
\begin{eqnarray*}
3(0,0,0)&=&(0,0,0)\\
0(0,0,0)&=&(0,0,0)
\end{eqnarray*}
En general cualquier conjunto de vectores que contenga al nulo será linealmente dependiente ya que permitirá obtener el nulo ponderando por un escalar no nulo.
\item El conjunto $\{(1,-1,0),(0,1,-1),(-1,0,1)\}$ es linealmente dependiente, de hecho:
$$(-1,0,1)=-(0,1,-1)-(1,-1,0)$$
Lo cual me permite llegar a que:
$$(1,-1,0)+(0,1,-1)-(-1,0,1)=(0,0,0),$$
lo cual es una combinación no trivial que me da el vector nulo.
\item Si $\{u,v\}\subseteq V$ es l.d., entonces existen $\alpha,\beta\in\K$ no ambos nulos tales que $\alpha u+\beta v=\Theta$.
Esto implica que ya sea $u=\frac{\beta}{\alpha}v$ o bien $v=\frac{\alpha}{\beta} u$, en cualquier caso uno de los dos vectores es ponderación del otro.

Recíprocamente, si $u=\gamma v$, entonces $\gamma v-u=\Theta$, por lo tanto $\{u,v\}$ es l.d.
\end{itemize}
}
\end{ejem}

En los ejemplos anteriores vimos que en los conjuntos dependientes uno de los vectores era combinación lineal de los demás. Este es siempre el caso, tal como establece el siguiente lema.

\begin{prop}[Lema de Dependencia Lineal]
Sea $\{v_1,v_2,\dots,v_k\}$ un conjunto linealmente dependiente en $V$ sobre $\K$.
Entonces existe $j\in\{1,2,\dots,k\}$ que cumple las dos afirmaciones siguientes.
\begin{enumerate}
\item $v_j\in\langle\{v_1,v_2,\dots,v_{j-1}\}\rangle$
\item $\langle\{v_1,v_2,\dots,v_k\}\setminus\{v_j\}\rangle=\langle\{v_1,v_2,\dots,v_k\}\rangle$
\end{enumerate}
\end{prop}
{\bf Demostración.} {
Dado que $\{v_1,v_2,\dots,v_k\}$ es linealmente dependiente, han de existir escalares $\alpha_1,\alpha_2,\dots,\alpha_k\in\K$, no todos nulos, tales que 
$$\alpha_1v_1+\alpha_2v_2+\cdots+\alpha_k v_k=\Theta$$
Sea $j$ el índice más grande tal que $\alpha_j\not=0$, entonces tenemos:
$$\alpha_1v_1+\alpha_2v_2+\cdots+\alpha_j v_j=\Theta.$$
Despejemos $v_j$ de la ecuación dividiendo por $\alpha_j$.
\begin{eqnarray}
\frac{\alpha_1}{\alpha_j}v_1+\frac{\alpha_2}{\alpha_j}v_2+\cdots+\frac{\alpha_j}{\alpha_j} v_j&=&\Theta\\
v_j&=&-\frac{\alpha_1}{\alpha_j}v_1-\frac{\alpha_2}{\alpha_j}v_2-\cdots-\frac{\alpha_{j-1}}{\alpha_j} v_{j-1}\label{eq:vj}
\end{eqnarray}
Esto prueba que $v_j\in\langle\{v_1,v_2,\dots,v_{j-1}\}\rangle$.

Por otra parte es claro que $\langle\{v_1,v_2,\dots,v_k\}\setminus\{v_j\}\rangle\subseteq\langle\{v_1,v_2,\dots,v_k\}\rangle$ ya que en el conjunto de la derecha siempre puedo ponderar al vector $v_j$ por 0, dando así lugar a las combinaciones lineales del conjunto de la izquierda.

Para probar la inclusión en el otro sentido tomemos un elemento arbitrario $u\in\langle\{v_1,v_2,\dots,v_k\}\rangle$.Esto significa que existen escalares $\beta_1,\beta_2,\dots,\beta_k\in\K$ tales que 
$$u=\beta_1 v_1+\beta_2 v_2+\cdots+\beta_k v_k.$$
Usamos (\ref{eq:vj}) para reemplazar a $v_j$ en esta ecuación, por una c.l. de los demás vectores, obtenemos
$$u=\left(\beta_1- \frac{\beta_j\alpha_1}{\alpha_j}\right)v_1+\left(\beta_2- \frac{\beta_j\alpha_2}{\alpha_j}\right)v_2+\cdots+\left(\beta_{j-1}- \frac{\beta_j\alpha_{j-1}}{\alpha_j}\right) v_{j-1}+\beta_{j+1}v_{j+1}+\dots+\beta_k v_k.$$
así probamos que $u$ está también en $\langle\{v_1,v_2,\dots,v_k\}\setminus\{v_j\}\rangle$.\hfill $\Box$
}

La primera consecuencia de este lema es que cada conjunto contiene un subconjunto linealmente independiente que genera el mismo espacio.
Pueden revisar el siguiente enlace quie contiene un video que ilustra este lema: \url{https://player.vimeo.com/video/596627119?h=86c4fecb0c}.

\begin{ejem} {\em
El conjunto $\{(1,-1,0),(0,1,-1),(-1,0,1)\}$ es linealmente dependiente, y de 
$$(1,-1,0)+(0,1,-1)-(-1,0,1)=(0,0,0)$$
podemos despejar $(-1,0,1)=-(0,1,-1)-(1,-1,0)$; según el lema anterior tenemos que 
$$\langle\{(1,-1,0),(0,1,-1),(-1,0,1)\}\rangle=\langle\{(1,-1,0),(0,1,-1)\}\rangle.$$
Notamos que en la primera ecuación podemos despejar otro vector como c.l. de los demás, entonces obtenemos:
$$\langle\{(1,-1,0),(0,1,-1),(-1,0,1)\}\rangle=\langle\{(1,-1,0),(0,1,-1)\}\rangle=\langle\{(1,-1,0),(-1,0,1)\}\rangle.$$
En otras palabras, los 3 vectores que participan de la combinación lineal $(1,-1,0)+(0,1,-1)-(-1,0,1)=(0,0,0)$ ponderados por un escalar no nulo son intercambiables entre sí.
}
\end{ejem}

El siguiente teorema será clave para poder definir el concepto de \emph{dimensión} la próxima clase.

%%%%%%%%%%%%%%%%%%%%%%
\begin{teo}\label{teo:<}
Sea $S$ un s.e.v de $V$ y $B=\{u_1,\dots,u_k\}$ un generador de $S$.
Si $D=\{v_1,\dots,v_m\}\subset S$ con $m>k$ entonces $D$ es l.d.
\end{teo}
{\bf Demostración.} {
La presente demostración es bastante delicada, la idea consiste en ir reemplazando uno a uno los vectores de $B$  por los de $D$, para lo cual usaremos el Lema de Dependencia Lineal repetidas veces.
La clave para lograr esto está en la hipótesis de que $B$ es generador de $S$, por lo tanto todos los elementos de $S$ son combinación lineal de los elementos de $B$, en particular los elementos de $D$ son c.l. de los de $B$.

Para ayudarnos, razonaremos por reducción al absurdo, vamos a suponer que $D$ es linealmente independiente, y llegaremos a una contradicción.
Esta hipótesis implica en particular que $D$ no contiene al vector nulo.
\begin{itemize}
\item[Paso 1.] Por lo dicho anteriormente, $v_1$ es c.l. de $B$, por lo tanto $B\cup\{v_1\}=\{v_1,u_1,\dots,u_k\}$ es l.d.
Usando el Lema de Independencia Lineal, y el supuesto de que $v_1\not=\Theta$, sabemos que existe $j\in\{1,\dots,k\}$ tal que $u_j\in\langle\{v_1,u_1,\dots,u_k\}\rangle$, el cual puede ser eliminado de la lista, con lo cual tenemos un nuevo generador de $S$ conformado por $v_1$ y $k-1$ vectores de $B$.
\item[Paso 2.] Podemos repetir el argumento, tomamos ahora $v_2$, dado que está en $S$, es c.l. del conjunto generador definido en el Paso 1, podemos agregarlo a este.
Por el Lema de Independencia Lineal sabemos que existe un vector que se puede sacar de este nuevo conjunto por ser c.l. de los anteriores.
Tal vector NO es $v_1$, ya que estamos suponiendo que $v_1$ no es c.l. de $v_2$.
Entonces el vector que podemos sacar es uno de los vectores de $B$, disminuyendo así su participación en este nuevo generador.
\item[Paso $n$.] Podemos repetir el argumento previo $n$ veces: tomamos $v_n$, con $n\le k$, y lo agregamos al último generador obtenido con el procedimiento antes descrito.
Como $v_n\in S$, entonces al agregarlo al generador de $S$ obtenemos un conjunto l.d. Nuevamente el Lema de Independencia Lineal nos sirve para escoger uno de los vectores de este generador que sea c.l. de los primeros vectores de la lista.
Dado que estamos suponiendo que $D$ es l.i., el vector que escojamos lo podemos escoger fuera de $D$.
Lo sacamos, y obtenemos un nuevo generador con más elementos de $D$ que el anterior.
\end{itemize}
Como el procedimiento descrito lo podemos repetir siempre, hasta $n=k$, entonces siempre podemos reemplazar todos todos los vectores de $B$ por los primeros $k$ vectores de $D$. 
Como hemos supuesto que $D$ tiene estrictamente más de $k$ vectores, y todos están en $S$, queda claro que los $m-k$ vectores de $D$ que no fueron agregados al generador son c.l de los primeros $k$, lo cual contradice el supuesto de independencia lineal de $D$, demostrando que este supuesto es falso, y por lo tanto el teorema es verdadero. \hfill $\Box$
}



\section{Base, coordenadas y dimensión}


\begin{defi}[Base] Una \emph{base de un sub espacio $U$} es una \emph{familia ordenada} de vectores $\B=\{v_1,v_2,...,v_k\}$ que cumple que
    \begin{itemize} 
    \item $\B$ es conjunto generador de $U$ $(U=\langle\B\rangle)$ y
    \item $\B$ es linealmente independiente.
    \end{itemize}   
\end{defi}

Cuando $\B$ genera a $U$, tenemos que todo vector $u\in U$ es combinación lineal de $\B$, y además, si $\B$ es l.i., los escalares que lo producen son únicos, es decir: para todo $u\in U$, existen \'unicos $\alpha_1,\alpha_2,...,\alpha_k\;\in\;\K$ tales que:
     $$u=\alpha_1v_1+\alpha_2v_2+\dots+\alpha_kv_k.$$  
La unicidad la conocemos gracias a la propiedad 3 del apunte de la Semana 3.
Gracias a esto podemos definir la siguiente noción.

\begin{defi}[Vector de coordenadas] 
    Si $\B=\{v_1,v_2,...,v_k\}$ es una base de $U$, y $u\in U$, entonces definimos el vector de coordenadas de $u$ respecto a $\B$ como 
    $$[u]_{\B}=\left( \begin{array}{c}\alpha_1 \\\alpha_2 \\\vdots \\\alpha_k\end{array} \right) \Leftrightarrow u=\alpha_1v_1+\alpha_2v_2+\dots+\alpha_kv_k.$$
\end{defi}

\begin{ejem} {\em
\begin{itemize}
\item Una base para el espacio $\K^2$ sobre $\K$ es $\Ccal_2=\{(1,0),(0,1)\}$ y todo elemento de $\K^2$, a saber, $(x,y)\in\K^2$ se escribe como sigue.
$$(x,y)=x(1,0)+y(0,1)\textrm{, entonces: }[(x,y)]_{\Ccal_2}=\left( \begin{array}{c}x\\y\end{array} \right)  $$
$(5,-2)=5(1,0)+(-2)(0,1)$, entonces $[(5,-2)]_{\Ccal_2}=\left( \begin{array}{c}5\\-2\end{array} \right)$\\
$(i,1-2i)=i(1,0)+(1-2i)(0,1)$, entonces $[(i,1-2i)]_{\Ccal_2}=\left( \begin{array}{c}i\\1-2i\end{array} \right)$\\
Notamos que las coordenadas se toman del cuerpo, por eso es importante siempre especificar sobre qué cuerpo se está, si cambia el cuerpo, seguramente cambiará la base.

\item $\widetilde{\B}_2=\{(1,2),(1,0)\}$ tambi\'en es base de $\K^2$ sobre $\K$, en este caso, todo vector $(x,y)\in\R^2$ se escribe como:
$$(x,y)=\dfrac{y}{2}(1,2)+\Big(x-\dfrac{y}{2}\Big)(1,0)\textrm{, entonces: }[(x,y)]_{\B_2}=\left( \begin{array}{c}\frac{y}{2}\\x-\frac{y}{2}\end{array} \right) $$

%as\'i, el vector $(5,-2)\in\R^2$ se escribe como $(5,-2)=(-1)(1,2)+6(1,0)$.
%El vector $(i,1-2i)\in\C^2$ se escribe como $(i,1-2i)=\Big(\dfrac{1}{2}-i\Big)(1,2)+\Big(-\dfrac{1}{2}+2i\Big)(1,0)$.

\item Los conjuntos $\Ccal_3=\{(1,0,0),(0,1,0),(0,0,1)\}$ y \\
$\Ccal_4=\{(1,0,0,0),(0,1,0,0),(0,0,1,0),(0,0,0,1)\}$ son base de $\K^3$ y $\K^4$ sobre $\K$ respectivamente.
Las bases $\Ccal_2$, $\Ccal_3$ y $\Ccal_4$ se denominan bases can\'onicas de $\K^2$, $\K^3$ y $\K^4$ sobre $\K$ respectivamente, as\'i para cualquier $n\in\N$, la base can\'onica de $\K^n$ sobre $\K$ est\'a dada por:
$$\Ccal_n=\{(1,0,0,\dots,0),(0,1,0,\dots,0),\dots,(0,0,0,\dots,1)\}.$$
El vector de coordenadas de un vector de $\K^n$ sobre $\K$ respecto a la base canónica de este espacio es siempre el mismo vector.
\end{itemize}
}
\end{ejem}

En el siguiente enlace encontrará una ilustración sobre cómo se ve un vector descrito usando distintas bases: \url{https://cfrd.udec.cl/proyectos/AlgebraLineal/}.

\begin{ejem}
Sea $S=\{(z_1,z_2,z_3)\in\C^3: iz_1-z_2=0\}$ un s.e.v de $\C^3$ sobre $\C$.
Encuentre una base para $S$.

{\em Para obtener una base de $S$ se debe calcular un conjunto generador y \'este debe ser un conjunto linealmente independiente.

\textsl{Conjunto Generador}: Sea $u\in S$, $u=(z_1,z_2,z_3)$ tal que $iz_1-z_2=0$ y por tanto $z_2=iz_1$, as\'i todo elemento de $S$ es de la forma:
$$u=(z_1,z_2,z_3)=(z_1,iz_1,z_3)=z_1(1,i,0)+z_3(0,0,1)$$
por tanto $S=\langle\{(1,i,0),(0,0,1)\}\rangle$. Sea $\B=\{(1,i,0),(0,0,1)\}$, $\B$ es conjunto generador de $S$.

\textsl{Independencia lineal}: Sea $\alpha_1,\alpha_2\in\C$ tales que:
$$\alpha_1(1,i,0)+\alpha_2(0,0,1)=(0,0,0).$$
De esta ecuación se obtiene 3: $\alpha_1=0$, $i\alpha_1=0$ y $\alpha_2=0$, es decir, $\alpha_1=\alpha_2=0\in\C$, es decir, $\B$ es l.i.

Como $\B$ es conjunto generador y l.i entonces $\B=\{(1,i,0),(0,0,1)\}$ es base de $S$.

Si ahora tomamos, por ejemplo, vector $v=(1+i,i-1,2+i)$ que pertenece a $S$ (verificar que cumple la ecuación), no es difícil ver que para obtener
$$\alpha_1(1,i,0)+\alpha_2(0,0,1)=(1+i,i-1,2+i),$$
basta con tomar $\alpha_1=1+i$ y $\alpha_2=2+i$, as\'i:
$$\left[\left(\begin{array}{c}1+i\\i-1\\2+i\end{array}\right)\right]_{\B}=\left(\begin{array}{c}1+i\\ 2+i \end{array}\right).$$
}
\end{ejem}


En $\P_n(\R)$ sobre $\R$ también podemos definir una base canónica, aunque a veces no hay acuerdo entre los libros respecto a cuál es esta.
Acá consideraremos que la base canónica de $\P_n(\R)$ sobre $\R$ es la siguiente.
$$\Ccal_{\P_n}=\{1,x,x^2,\dots,x^n\}$$
De donde tenemos que el vector de coordenadas del polinomio $p(x)=ax^2+bx+c$ respecto a $\Ccal_{\P_2}$ es:
$$\left[ax^2+bx+c\right]_{\Ccal_{\P_2}}=\left(\begin{array}{c} c\\b\\a \end{array}\right).$$
Esto nos evidencia la importancia de los coeficientes del polinomio, con estos basta para conocerlo completamente, sus coeficientes son sus coordenadas respecto a la base canónica de los polinomios.


Notamos que el orden es importante en una base, pues al cambiar el orden de los vectores de la base, cambia el orden de las coordenadas del vector de coordenadas.



\subsection{Dimensi\'on}


\begin{teo}
Sea $S$ un s.e.v de $V$. Si $\{v_1,\dots,v_k\}$ y $\{u_1,\dots,u_m\}$ son bases de $S$ entonces $k=m$.
\end{teo}
    {\bf Demostración.}
    Si $m\not=k$ entonces se ha de tener que $m<k$ o bien $m>k$.
    Supongamos sin pérdida de generalidad que $m>k$. Entonces podemos usar el Teorema 1 del apunte de la Semana 3 para concluir que $\{u_1,\dots,u_m\}$ es l.d., pero esto contradice la hipótesis, por lo tanto ha de tenerse que $m=k$.
    \hfill $\Box$
    

Nos dice que \emph{todas} las bases de un espacio vectorial tienen exactamente la misma cantidad de elementos.
El tama\~no de sus bases caracteriza a un s. e. v. sobre un determinado cuerpo $\K$, por lo tanto podemos darle nombre a este atributo.

\begin{defi}[Dimensi\'on] Sea $S$ un s.e.v de $V$ con base $\{v_1,\dots,v_k\}$. La dimensi\'on de $S$, la cual se denota por {\em dim}$(S)$, es la cardinalidad de su base, es decir {\em dim}$(S)=${\em card}$(\{v_1,\dots,v_k\})=k$. 
\end{defi}

\begin{ejem}
Calcule la dimensi\'on de $S=\{(z_1,z_2,z_3)\in\C^3: iz_1-z_2=0\}$.

{\em Más arriba se mostr\'o que $\B=\{(1,i,0),(0,0,1)\}$ es una base de $S$, luego $\dim(S)=\card(\{(1,i,0),(0,0,1)\})=2$.
 
 Ac\'a la notaci\'on tambi\'en es importante, escribir $\dim(\{(1,i,0),(0,0,1)\})$ o $\dim(\B)$ est\'a equivocado puesto que el concepto de dimensi\'on se aplica a un subespacio vectorial y no a un conjunto cualquiera, $\B=\{(1,i,0),(0,0,1)\}$ no es un sub espacio vectorial.
 
De igual forma, escribir $\card(S)$ o  $\card(\langle\B\rangle)$ no est\'a correcto ya que $S=\langle\B\rangle$ es un s.e.v., por lo tanto su cardinalidad es mucho mayor a su dimensión (en general es infinita).}
\end{ejem}


\begin{obs}
Así tenemos que $\dim(\K^n)=n$, ya que su base canónica, cuando el cuerpo es $\K$, tiene $n$ elementos.

Notar que $\{\Theta\}$ no es un conjunto l. i., ya que $1\cdot \Theta=\Theta$ es una c.l. de $\{\Theta\}$ no trivial que produce $\Theta$.
Pero el conjunto vacío $\phi$ sí lo es, y ya que $\langle \phi\rangle=\{\Theta\}$, tenemos: $\dim(\{\Theta\})=\card(\phi)=0$: ¡la dimensión del espacio nulo es 0! 

También vimos una base canónica para $\mathcal{P}_{n}(\K)$ sobre $\K$:
$$\Ccal_{\P_n}=\{1,x,x^2,\dots,x^n\},$$
la cual posee $n+1$ vectores, por lo tanto $\dim(\mathcal{P}_{n}(\K))=n+1$.

Pero atención: la dimensión, igual que la base dependen del cuerpo que se está considerando, y aunque la notación ``dim'' no menciona el cuerpo, este es importante y debe tenerse en cuenta a la hora de concluir.
\end{obs}


{\bf ¿Cómo obtenemos una base para un subespacio vectorial dado?}
primero supongamos que ya tenemos un sistema generador.
Vimos que cuando el sistema no es {l. i.}, entonces hay vectores que son prescindibles, los podemos eliminar del sistema sin pérdida.

\begin{prop}
  Si un conjunto finito $\B$ genera a un s.e.v. $U$, entonces $\B$ contiene a una base de $U$
  \end{prop}
{\bf Demostración.} En efecto, gracias al Lema de Independencia Lineal, sabemos que cuando un conjunto no es l.i., podemos escoger uno de sus vectores para quitárselo sin que pierda su poder generativo; si aún asi no obtenemos un conjunto l.i., podemos escoger otro vector y sacarlo; y así sucesivamente, sin que nunca el conjunto deje de dejar de generar a $U$. En algún momento obtendremos un conjunto l.i. y por lo tanto una base de $U$.\hfill $\Box$ 

A partir de esta propiedad deducimos que cada vector que resulte ser c.l. de los demás puede ser eliminado, uno a uno, del sistema generador, sin que el espacio generado se vea disminuido.
Así, para obtener una base, basta con eliminar estos vectores, uno a uno.

{\bf ¿Cómo sabemos qué vectores eliminar?}
Veamos un ejemplo para ilustrarlo.

{\small Se vi\'o que $B=\left\{\left(\begin{array}{c} 1\\ 0\\ 2\end{array}\right),\left(\begin{array}{c} 2\\ 1\\ 1\end{array}\right),\left(\begin{array}{c} -4\\ -3\\ 1\end{array}\right)\right\}$ es linealmente dependiente.
Resolviendo el sistema de independencia lineal se obtiene que $\alpha_2=3\alpha_3$ y $\alpha_1=-2\alpha_3$ con $\alpha_3\in\R$.

Podemos escoger $\alpha=1$, y entonces obtenemos:
$$\left(\begin{array}{c} -4\\ -3\\ 1\end{array}\right)=2\left(\begin{array}{c} 1\\ 0\\ 2\end{array}\right)-3\left(\begin{array}{c} 2\\ 1\\ 1\end{array}\right)$$ 

Por lo tanto podemos eliminar $\left(\begin{array}{c} -4\\ -3\\ 1\end{array}\right)$, ya que los otros dos vectores lo pueden reemplazar sin problemas.
}  


\begin{ejem}
Calcule la dimensión de $H=\langle\{x+1,x(x+1),(x-1)(x+1)\}\rangle$ subespacio de $\P_2(\R)$ sobre $\R$.

{\em
  El generador de $H$ que nos están dando es $\D=\{x+1,x(x+1),(x-1)(x+1)\}$, veamos primero si es l.i. o no.
  Planteamos la ecuación de independencia lineal:
  $$\forall x\in\R, a(x+1)+bx(x+1)+c(x-1)(x+1)=0.$$
desarrollamos los productos y la suma para obtener los coeficientes del polinomio resultante.
\begin{eqnarray*}
  a(x+1)+bx(x+1)+c(x-1)(x+1)&=&ax+a+bx^2+bx+cx^2-c\\
  &=&(b+c)x^2+(a+b)x+a-c
\end{eqnarray*}
Igualando cada coeficiente a 0 obtenemos: $a-c=0$, $a+b=0$ y $b+c=0$.
De la primera ecuación obtenemos $a=c$, reemplazamos esto en las demás ecuaciones para eliminar $a$, y obtenemos:
$c+b=0$ $b+c=0$, vemos que las dos ecuaciones quedan iguales, por lo tanto concluimos que $b=-c$ y no podemos seguir trabajando.

Por lo tanto una solución al sistema es $a=1, c=1, b=-1$, lo que demuestra que $\D$ no es l.i.

Esta solución me dice que 
  $$\forall x\in\R, (x+1)-x(x+1)+(x-1)(x+1)=0.$$
 es decir que $x(x+1)=(x+1)+(x-1)(x+1)$, podemos entonces eliminar este vector del conjunto generador, para obtener $\D'=\{x+1,(x-1)(x+1)\}$, que gracias al Lema de Independencia Lineal sigue generando a $H$.

Además, vemos que $\D'$ es l.i. ya que ninguno de estos polinomio es ponderación del otro por escalar, entonces $\D'$ es base de $H$, y por lo tanto $\dim(H)=2$.
}
\end{ejem}


\begin{teo}[Completación]\label{teo:crece}
Si $\{u_1,\dots,u_k\}\subset V$ es l. i. y  $u\not\in\langle\{u_1,\dots,u_k\}\rangle$, entonces $\{u,u_1,\dots,u_k\}$ es también l. i.
\end{teo}
{\bf Demostración.} {
  Suponemos que no es así, y tomamos una combinación lineal de $\langle\{u, u_1,\dots,u_k\}\rangle$ que da $\Theta$, con los escalares $a$, $a_1,\dots,a_k$.
  $$au+\sum_{i=1}^k a_i u_i=\Theta, \quad \textrm{despejando }au:\quad au=-\sum_{i=1}^k a_i u_i$$
  {\bf Caso 1 ($a=0$):} entonces $\sum_{i=1}^k a_i u_i=\Theta $, pero como los vectores participantes de esta suma son l.i., entonces $a_1=\cdots =a_k=0$.\\
  {\bf Caso 2 ($a\not=0$):} dividimos por $a$ en ambos lados de la ecuación y obtenemos:
  $$u=\sum_{i=1}^k\left(\frac{-a_i}{a}\right) u_i,$$
  de donde se deduce que $u\in\langle\{u_1,\dots,u_k\}\rangle$, lo cual es una contradicción.
  \hfill $\Box$
}

\vspace{.2cm}

{\bf ¿Cómo obtenemos una base de un s.e.v. $U$ si solo tenemos un conjunto l.i. dentro de éste, pero que no es generador?}

Si el conjunto $\{u_1,\dots,u_k\}$ es l.i., y está en $U$ pero no lo genera, significa que existe un vector $u$ en $U$ que no es c.l. de $\{u_1,\dots,u_k\}$.
Podemos agregarlo al conjunto, y por el Teorema~\ref{teo:crece}, el nuevo conjunto sigue siendo l.i. y estando en $U$.

Si aún con haber agregado este vector, no logramos generar todo $U$, es porque aún quedan vectores que agregar: buscamos uno, lo agregamos y obtenemos un generador más grande; y así sucesivamente continuamos hasta obtener un generador de $U$.

El proceso antes descrito termina cuando hayamos obtenido tantos vectores l.i. dentro de $U$ como sea su dimensión... ¿pero es esta finita?\footnote{Hay espacios vectoriales de dimensión ``infinita'', pero de eso no hablaremos en este curso...}

Si $U$ es s.e.v. de $V$ y $V$ tiene dimensión finita, entonces $U$ también ha de tenerla, de hecho, por el Teorema 1 del apunte de la Semana 3, sabemos que $V$ no puede contener ningún conjunto l.i. que tenga más de $\dim(V)$ elementos, por lo tanto nuestro proceso termina antes de $\dim(V)$ pasos.

Este argumento muestra además que $\dim(U)\le\dim(V)$


\begin{ejem}\label{ejem:+1} {\em
  Sea $\D=\{(2,3,0,1),(0,0,1,0)\}\subseteq W=\{(a,b,c,d)\in\R^4\ :\ a+d=b\}$. 
  En efecto, vemos que $2+1=3$ y $0+0=0$.
  
  Es fácil ver que $\langle\D\rangle \subseteq W$, ya que $W$ es un s.e.v. y por lo tanto es cerrado para combinaciones lineales.
  Por otra parte, $\D$ es l.i., ya que ninguno de sus dos vectores es ponderación del otro (cuando hay solo dos vectores, es muy fácil chequear independencia lineal).
  
  Veamos si es que $\langle \D\rangle = W$ o no.
  Para ello calculamos estudiemos $\langle \D\rangle$.
  \begin{eqnarray*}
    \langle\D\rangle&=&\{((2a,3a,b,a)\ :\ a,b\in\R\}\\
    &=& \{(x,y,z,t)\in\R^4\ : \ x=2t\textrm{ e } y=3t, z\in \R \textrm{ libre} \}
  \end{eqnarray*}
No es difícil encontrar un vector en $W\setminus \langle\D\rangle$, por ejemplo $(1,1,0,0)$ cumple la ecuación de $W$, pero no las ecuaciones de $\langle\D\rangle$.
  
Entonces $\D\cup\{(1,1,0,0)\}$ es l.i., gracias al Teorema~\ref{teo:crece}.
  $\langle\D\cup\{(1,1,0,0)\}\rangle$ es un subespacio más grande que $\langle \D\rangle$, y sigue contenido en $W$, ¿será igual a $W$? 
  }
\end{ejem}


\begin{prop}\label{teo:leq}
  Si $U$ y $W$ son s.e.v. de $V$ sobre $\K$ y $U\subseteq W$, entonces
  \begin{eqnarray}\label{teo:le}
    \textup{dim}(U)\le \textup{dim}(W).
  \end{eqnarray} 
  Si además $\textup{dim}(U)=\textup{dim}(W)$, entonces
  \begin{eqnarray}\label{teo:igual}
    U=W.
  \end{eqnarray}
\end{prop}
    {\bf Demostración.} {
      Ya demostramos (\ref{teo:le}) más arriba.

      Demostremos ahora (\ref{teo:igual}): supongamos por el contrario, que $U\varsubsetneq W$.
      Esto significa que existe $w\in W\setminus U$.
      Si $U$ tiene una base $\B$ de tamaño $k=\dim(U)=\dim(W)$, entonces $w\not\in\langle\B\rangle=U$.
      El teorema~\ref{teo:crece} de la clase de hoy nos dice que entonces $\{w\}\cup\B$ es l.i.
      Por lo tanto $\{w\}\cup\B$ es base de $\langle\{w\}\cup\B\rangle$, y como $\{w\}\cup\B\subset W$, $\langle\{w\}\cup\B\rangle$ sería un s.e.v. de $W$ de dimensión $k+1>k=\dim(W)$, lo cual contradice (\ref{teo:le}) recién demostrado.
      \hfill $\Box$
    }

\begin{ejem} {\em
  Retomando el ejemplo~\ref{ejem:+1}.
  Tenemos que $\D\cup\{(1,1,0,0)\}\subseteq W$, y entonces $\langle\D\cup\{(1,1,0,0)\}\rangle\subseteq W$.
  Como $\D\cup\{(1,1,0,0)\}$ es l.i., entonces $\dim(\langle \D\cup\{(1,1,0,0)\}\rangle)=|\D\cup\{(1,1,0,0)\}|=3$, por lo tanto
  $3\le\dim(W)$.

  Por otra parte, es claro que $W\not=\R^4$, ya que por ejemplo $(1,0,0,0)\not\in W$.
  Por lo tanto $\dim(W)<4$.

  En conclusión $\dim(W)=3$.
  De donde concluimos que $W= \langle \D\cup\{(1,1,0,0)\}\rangle$, gracias a la propiedad~\ref{teo:leq} recién demostrada.
  }
\end{ejem}

Con todo lo trabajado se pueden deducir las siguientes propiedades (pensar por qué).

\begin{prop}\label{teo:general} Dado un e.v. $V$ de dimensión finita, se cumple lo siguiente.
\begin{enumerate}[a)]
\item El conjunto l. i. más grande de $V$, tiene $\textup{dim}(V)$ elementos.
\item Todo conjunto l. i. de $V$ con $\textup{dim}(V)$ elementos, es base de $V$.
\item El conjunto generador de $V$ más pequeño tiene $\textup{dim}(V)$ elementos.
\item Todo conjunto generador de $V$ con $\textup{dim}(V)$ elementos es base de $V$.
\end{enumerate}
\end{prop}

Apliquemos lo aprendido.

\begin{ejem}
Sea $\B=\{x^2+2x\}\subseteq\P_2(\R)$ sobre $\R$, complete $\B$ hasta obtener una base de $\P_2(\R)$.

\underline{Soluci\'on:}
{\em
La dimensi\'on de $\P_2(\R)$ sobre $\R$ es $3$, luego todo conjunto linealmente independiente que tenga $3$ vectores ser\'a base de $\P_2(\R)$ sobre $\R$, por lo tanto a $\B$ hay que agregarle dos vectores: $p(x)$ y $q(x)$, de manera que el conjunto resultante $\B\cup \{p(x),q(x)\}$ sea l.i.

Ya se sabe cu\'antos vectores agregar, ahora la pregunta es: ¿cuáles son esos vectores?.

El teorema~\ref{teo:crece} dice que si a un conjunto linealmente independiente se le agrega un vector que no es combinaci\'on lineal de los elementos de éste, entonces el nuevo conjunto es l.i.
Por tanto, para este caso, $p(x)$ puede ser $p(x)=x^2$ ya que $x^2$ no es ponderación de $x^2+2x$, luego el nuevo conjunto $\{x^2+2x, x^2\}$ es l.i. 

El segundo vector a agregar puede ser $q(x)=1$ puesto que $1\not\in\langle\{x^2+2x, x^2\}\rangle$, esto es evidente del hecho que toda c.l. de estos vectores tendrá el término libre nulo.
Luego $\{x^2+2x, x^2,1\}$ es l.i y tiene $3$ elementos, en consecuencia, es base de $\P_2(\R)$ sobre $\R$.
}
\end{ejem}

\section{Teorema de Grassmann}

\begin{teo}[Grassmann, XIX]
Dados dos s.e.v. $U$, $W$ de dimensi\'on finita de un espacio vectorial $V$ sobre $\K$, se cumple:

\[ \textup{dim}(U+W)=\textup{dim}(U)+\textup{dim}(W)-\textup{dim}(U\cap W).\]
\end{teo}
{\bf Demostración.} { 
Espero que la disfruten la demostración como espectadores, es muy elaborada, y hace uso de los teoremas que hemos visto. Sin embargo no tiene utilidad en sí para los ejercicios que vemos en este curso (lo que sí tiene utilidad es el teorema).

Antes que nada, llamemos $k=\dim(U\cap W)$, $m=\dim(U)$ y $n=\dim(W)$.

Tomemos una base de $U\cap W$, digamos $\B=\{u_1,u_2,\dots,u_k\}$.
Usando el teorema de completación, y dado que $(U\cap W)\subseteq U$, completemos esta base hasta obtener una base de $U$, digamos que hemos agregado un conjunto de vectores $\Ccal=\{u_{k+1},\dots,u_m\}$ a $\B$, es decir, que la base de $U$ es: $\B\sqcup\Ccal$.

Ahora, dado que $(U\cap W) \subseteq W$, completemos $\B$ para obtener una base de $W$, digamos que agregamos el conjunto de vectores $\D=\{w_1,w_2,\dots,w_{n-k}\}$, es decir que la base de $W$ es: $\B\sqcup\D$.

Entonces, por la propiedad anterior, tenemos un generador para $U+W$:
$$U+W=\langle(\B\sqcup\Ccal)\cup(\B\sqcup\D)\rangle=\langle\B\sqcup\Ccal\sqcup\D\rangle=\langle\{u_1,u_2,\dots,u_m,w_1,\dots,w_{n-k}\}.$$
este generador tiene justamente $m+n-k=\dim(U)+\dim(W)-\dim(U\cap W)$ vectores, si fuera l.i., habríamos entonces demostrado el Teorema de Grassmann.
Estudiemos entonces su independencia lineal.
Como hipótesis tenemos que $\B$, $\Ccal$ y $\D$ son l.i.
Sean entonces escalares $\alpha_1,\dots,\alpha_m,\beta_1,\dots,\beta_{n-k}$ tales que:
$$\sum_{i=1}^m\alpha_i u_i+\sum_{j=1}^{n-k} \beta_j w_j=\Theta.$$
Si restamos la segunda sumatoria nos queda:
$$\sum_{i=1}^m\alpha_i u_i=-\sum_{j=1}^{n-k} \beta_j w_j\in W.$$
Pero esto indicaría que $\sum_{j=1}^{n-k} \beta_j w_j$ pertenece a $U\cap W$, entonces debería ser combinación lineal de los vectores de $\B=\{u_1,u_2,\dots,u_k\}$, pero estos no aparecen en la suma, y por su parte los vectores de $\D=\{w_1,\dots,w_{n-k}\}$ los escogimos justamente fuera de $\langle\B\rangle$, entonces la única manera de que esto sea posible es que que sea el vector nulo:
$$\sum_{j=1}^{n-k} \beta_j w_j=\Theta,$$
y dado que $\D$ es l.i., esto implica que $\beta_1=\beta_2=\dots=\beta_{n-k}=0$.
Pero esto nos deja con la siguiente igualdad:
$$\sum_{i=1}^m\alpha_i u_i=\Theta,$$
y esta es una c.l. de $\B\sqcup\Ccal$ que es base de $U$, entonces $\alpha_1=\alpha_2=\dots=\alpha_m=0$.
Hemos demostrado que $\B\sqcup\Ccal\sqcup\D$ es l.i
\hfill $\Box$
}


\begin{defi}
Dos s. e. v. $W$, $U$ de $V$ se dicen en \emph{suma directa} si $W\cap U=\{\Theta\}$, en tal caso denotamos $W\oplus U$ en lugar de $W+U$.
\end{defi}

La suma directa es de particular importancia debido a la siguiente propiedad, la cual dice que cuando la suma de dos espacios es del tipo ``\emph{directa}", entonces los vectores del espacio suma, se descomponen de manera única como suma de vectores de los espacios.

\begin{prop}
Si $W$ y $U$ son dos s. e. v. de $V$ que est\'an en suma directa, entonces se cumple:

\[ (\forall v\in W\oplus U)(\exists! w\in W)(\exists! u\in U)\ v=w+u.\]
\end{prop}
{\bf Demostración.} {
Nuestra hipótesis principal es que $W\cap U=\{\Theta\}$.

Sea  $v\in W\oplus U$, supongamos que hay dos maneras diferentes de escribir $v$ como suma de un elemento de $W$ más uno de $U$, es decir, supongamos que existen $w_1,w_2\in W$ y $u_1, u_2\in U$ tales que:

$$ v= w_1+u_1\qquad\textrm{y}\qquad v=w_2+u_2.$$
Entonces podemos afirmar que:
$$w_1+u_1=w_2+u_2.$$
Sumamos $-u_1-w_2$ a ambos lados:
$$w_1-w_2=u_2-u_1\in U.$$
Pero esto significa que $w_1-w_2\in U\cap W=\{\Theta\}$, por lo tanto $w_1-w_2=\Theta$, es decir $w_1=w_2$.
Además, por la igualdad, también tenemos que $u_2-u_1=\Theta$, por lo tanto $u_1=u_2$.

En otras palabras, las dos descomposiciones no eran dos si no una sola, lo que prueba la propiedad.
\hfill $\Box$
}
\pq

{\bf Última observación:} {\em si $U$ y $W$ est\'an en suma directa, entonces 

\[\textup{dim}(U\oplus W)=\textup{dim}(U)+\textup{dim}(W).\]
}

\chapter{Aplicaciones Lineales}

El estudio de funciones juega un rol fundamental en la ciencia y en la ingeniería.
Una función es una operación que transforma un objeto en otro.
Estudiaremos funciones entre espacios vectoriales, es decir funciones que a un vector le asocian otro vector (eventualmente de un espacio vectorial distinto).

En este capítulo nos centraremos en una clase particular de funciones, llamadas \emph{lineales}.
Veremos que son simples y que hay una vasta teoría que nos permite comprenderlas profundamente.
No es este el caso de las funciones en general, por esto mismo, las funciones lineales juegan un rol fundamental en la ingeniería, pues son prácticamente las únicas que podemos representar totalmente en un computador, y las únicas que podemos ``resolver" de manera directa.
Verán ustedes dentro de unos años más que, este es uno de los temas que más usarán en sus carreras profesionales.
Pueden encontrar un video motivacional respecto a este tema en el siguiente enlace: \url{https://vimeo.com/596627375/f9ab67785d}.

\begin{defi}
Dados dos espacios vectoriales $V$ y $W$ sobre un mismo cuerpo $\K$, decimos que la función $T:\ V\rightarrow W$ es una \emph{Transformación Lineal}  si cumple las siguientes propiedades.
\begin{enumerate}
\item $(\forall u,v\in V)\ T(u+v)=T(u)+T(v)$.
\item $(\forall v\in V)(\forall \alpha \in \K)\ T(\alpha v)=\alpha T(v)$.
\end{enumerate}
\end{defi}
En otras palabras, una transformación lineal respeta la suma y respeta la ponderación, y por lo tanto preserva los subespacios vectoriales, preserva las líneas rectas.

\bigskip


\begin{ejem}
Las funciones lineales más básicas y que merecen un nombre propio son las siguientes.
\begin{itemize}
\item {\bf La función constante igual a $\Theta$.} Si llamamos $\Theta_W$ al vector nulo de $W$, podemos definir: $\mathcal{O}:V\rightarrow W$ como $\mathcal{O}(v)=\Theta_W$, para todo $v\in V$.

Es lineal ya que si $u,v\in V$, entonces $\mathcal{O}(u+v)=\Theta_W=\Theta_W+\Theta_W=\mathcal{O}(u)+\mathcal{O}(v)$.

Por su parte si $\alpha \in \K$, entonces $\mathcal{O}(\alpha v)=\Theta_W=\alpha\Theta_W=\alpha\mathcal{O}(v)$. Respeta la suma y la ponderación.

\item {\bf La función identidad.} $id:V\rightarrow V$ definida por $id(v)=v$ es lineal. En efecto.

$id(u+v)=u+v=id(u)+id(v)$\\
$id(\alpha v)=\alpha v=\alpha id(v)$
\end{itemize}
\end{ejem}

\begin{ejem}\label{eje:primera}
La funciones que estudiamos, transforman un vector en otro vector.
Consideremos por ejemplo la transformación $L:\R^2\rightarrow \R$ dada por $L(x,y)=3x-2y$.
Lo primero que debemos hacer para entenderla es calcularla en alguna parte, veamos algunos ejemplos.

$L(1,-1)=3\cdot 1-2\cdot(-1)=3+2=5$\\
$L(0,10)=3\cdot 0 -2\cdot10=0-20=-20$\\
$L(3a,a+b)=3\cdot3a-2(a+b)=9a-2a-2b=7a-2b$

Resulta que $L$ es lineal, ya que cumple la definición, si $u=(x,y)$ y $v=(a,b)$ entonces

$L(u+v)=L(x+a,y+b)=3(x+a)-2(y+b)=3x+3a-2y-2b=3x-2y+3a-2b=L(u)+L(v)$ 

y si $\alpha\in\R$

$L(\alpha u)=L(\alpha x,\alpha y)=3\alpha x-2\alpha y=\alpha(3x-2y)=\alpha L(u)$.
\end{ejem}

Ejemplos célebres de  transformaciones lineales de $\R^2$ en $\R^2$, son las reflexiones, y rotaciones.

\bigskip


\begin{ejem}
{\bf Rotación en 90 grados} { La función $G:\R^2\rightarrow\R^2$ definida por $G(x,y)=(-y,x)$ rota cada vector de $R^2$ en 90 grados, para verlo, basta calcular y dibujar.

\begin{center}
\begin{tikzpicture}[scale=0.8]
        \draw[arrows=->] (-3,0) -- (3,0) node[below] {$x$};
        \draw[arrows=->] (0,-3) -- (0,3) node[left] {$y$};
        %\draw[scale=1,domain=-10:5,smooth,variable=\x] plot ({\x},{5/(1+2^(-\x))});
        %\draw[scale=1,domain=5:10,smooth,variable=\x] plot ({\x},{5/(1+2^(-5)+1/7+1/(\x-12)});
        %\coordinate[label=below:$(1,0)$] (A) at (1,0);
        %\coordinate[label=below:$(1,2)$] (B) at (1,2);
        %\coordinate[label=left:$(1,1)$] (C) at (1,1);
        \draw[arrows=->,thick] (0,0)--(1,0) node[above] {$(1,0)$};
         \draw[arrows=->,thick] (0,0)--(2,-1) node[right] {$(2,-1)$};
         \draw[arrows=->,thick] (0,0)--(1,1) node[above] {$(1,1)$};
        %\draw[dashed] (0,7)--(10,7);
        %\draw[dashed] (10,7)--(10,0);
        %\draw[fill=black] (-10,0) circle(0.3);
        %\draw[fill=black] (10,7) circle(0.3);
        \draw[arrows=->,thick] (4,0) -- (4.5,0);
        \draw[arrows=->,thick,white] (4.5,0) -- (5,0);
\end{tikzpicture} 
\begin{tikzpicture}[scale=.8]
        \draw[arrows=->] (-3,0) -- (3,0) node[below] {$x$};
        \draw[arrows=->] (0,-3) -- (0,3) node[left] {$y$};
        %\draw[scale=1,domain=-10:5,smooth,variable=\x] plot ({\x},{5/(1+2^(-\x))});
        %\draw[scale=1,domain=5:10,smooth,variable=\x] plot ({\x},{5/(1+2^(-5)+1/7+1/(\x-12)});
        %\coordinate[label=below:$(1,0)$] (A) at (1,0);
        %\coordinate[label=below:$(1,2)$] (B) at (1,2);
        %\coordinate[label=left:$(1,1)$] (C) at (1,1);
        \draw[arrows=->,thick] (0,0)--(0,1) node[right] {$G(1,0)$};
         \draw[arrows=->,thick] (0,0)--(1,2) node[above] {$G(2,-1)$};
         \draw[arrows=->,thick] (0,0)--(-1,1) node[left] {$G(1,1)$};
        %\draw[dashed] (0,7)--(10,7);
        %\draw[dashed] (10,7)--(10,0);
        %\draw[fill=black] (-10,0) circle(0.3);
        %\draw[fill=black] (10,7) circle(0.3);
\end{tikzpicture}
\end{center}
Vemos que esta transformación respeta todo, la magnitud, los ángulos relativos entre los vectores, también la suma y la ponderación.
}

\newpage

{\bf Reflexión respecto al eje Y} { La función $R:\R^2\rightarrow\R^2$ definida por $R(x,y)=(-x,y)$ refleja cada vector de $R^2$, como si en el eje Y hubiera un espejo, para verlo, basta calcular y dibujar.

\begin{center}
\begin{tikzpicture}[scale=.9]
        \draw[arrows=->] (-3,0) -- (3,0) node[below] {$x$};
        \draw[arrows=->] (0,-3) -- (0,3) node[left] {$y$};
        %\draw[scale=1,domain=-10:5,smooth,variable=\x] plot ({\x},{5/(1+2^(-\x))});
        %\draw[scale=1,domain=5:10,smooth,variable=\x] plot ({\x},{5/(1+2^(-5)+1/7+1/(\x-12)});
        %\coordinate[label=below:$(1,0)$] (A) at (1,0);
        %\coordinate[label=below:$(1,2)$] (B) at (1,2);
        %\coordinate[label=left:$(1,1)$] (C) at (1,1);
        \draw[arrows=->,thick] (0,0)--(1,0) node[above] {$(1,0)$};
         \draw[arrows=->,thick] (0,0)--(2,-1) node[right] {$(2,-1)$};
         \draw[arrows=->,thick] (0,0)--(1,1) node[above] {$(1,1)$};
        %\draw[dashed] (0,7)--(10,7);
        %\draw[dashed] (10,7)--(10,0);
        %\draw[fill=black] (-10,0) circle(0.3);
        %\draw[fill=black] (10,7) circle(0.3);
        \draw[arrows=->,thick] (4,0) -- (4.5,0);
        \draw[arrows=->,thick,white] (4.5,0) -- (5,0);
\end{tikzpicture} 
\begin{tikzpicture}[scale=.9]
        \draw[arrows=->] (-3,0) -- (3,0) node[below] {$x$};
        \draw[arrows=->] (0,-3) -- (0,3) node[left] {$y$};
        %\draw[scale=1,domain=-10:5,smooth,variable=\x] plot ({\x},{5/(1+2^(-\x))});
        %\draw[scale=1,domain=5:10,smooth,variable=\x] plot ({\x},{5/(1+2^(-5)+1/7+1/(\x-12)});
        %\coordinate[label=below:$(1,0)$] (A) at (1,0);
        %\coordinate[label=below:$(1,2)$] (B) at (1,2);
        %\coordinate[label=left:$(1,1)$] (C) at (1,1);
        \draw[arrows=->,thick] (0,0)--(-1,0) node[above] {$R(1,0)$};
         \draw[arrows=->,thick] (0,0)--(-2,-1) node[left] {$R(2,-1)$};
         \draw[arrows=->,thick] (0,0)--(-1,1) node[above] {$R(1,1)$};
        %\draw[dashed] (0,7)--(10,7);
        %\draw[dashed] (10,7)--(10,0);
        %\draw[fill=black] (-10,0) circle(0.3);
        %\draw[fill=black] (10,7) circle(0.3);
\end{tikzpicture} 
\end{center}
¿Cómo haría usted para definir la reflexión respecto a otras rectas, por ejemplo respecto al eje X?
¿Cómo haría usted para definir una rotación en otro ángulo, por ejemplo 180 grados, o en 45 grados?
}
\end{ejem}

\bigskip


\begin{ejem}{
Otros ejemplos clásicos de funciones lineales son los siguientes.
\begin{itemize}
\item $D:\P(\R)\rightarrow\P(\R)$, definida por $D(p)=p'$, donde $p'$ es la \emph{derivada} de $p$.\\
Es lineal, en efecto:\\
$D(p+q)=(p+q)'=p'+q'=D(p)+D(q)$, y\\
$D(\lambda p)=(\lambda p)'=\lambda p'=\lambda D(p)$.
\item $I:\P(\R)\rightarrow\R$, definida por $I(p)={\displaystyle \int_0^1 p(x)\ dx}\in\R$.\\
Es lineal, en efecto:\\
$I(p+q)=\int_0^1(p+q)(x)\ dx=\int_0^1p(x)+q(x)\ dx=\int_0^1p(x)\ dx+\int_0^1q(x)\ dx=I(p)+I(q)$, y\\
$I(\lambda p)=\int_0^1(\lambda p)(x)\ dx=\int_0^1\lambda p(x)\ dx=\lambda \int_0^1p(x)\ dx=\lambda I(p)$.
\item $A:\R^2\rightarrow\R^3$, $A(x,y)=(2x-y,x+2y,y)$.\\
Es lineal, y la demostración se parece mucho a la del Ejemplo~\ref{eje:primera}, así que mejor veamos un pequeño teorema que sirva para todas las funciones de este tipo de una sola vez.
\end{itemize}
}
\end{ejem}

\bigskip

\begin{prop}
Considerando a $\K^m$ y $\K^n$ como espacios vectoriales sobre $\K$.
Si $T:\K^m\rightarrow \K^n$ es una función definida como sigue:
$$T(x_1,x_2,\dots,x_m)=(a_{11}x_1+a_{12}x_2+\cdots+a_{1m}x_m,a_{21}x_1+\cdots + a_{2m}x_m,\dots,a_{n1}x_1+\cdots + a_{nm}x_m),$$
donde $a_{ij}\in\K$, con $i\in\{1,\dots,n\}$ y $j\in\{1,\dots,m\}$, son $nm$ constantes  fijas, entonces $T$ es lineal.
\end{prop}
{\bf Demostración.} {\small
Tomamos dos vectores arbitrarios de $\K^m$: $u=(x_1,\dots,x_m)$ y $v=(y_1,\dots, y_m)$, tomamos también un escalar cualquiera $\alpha\in\K$, calculamos:
\begin{eqnarray*}
T(u+v)&=&T(x_1+y_1,x_2+y_2,\dots,x_m+y_m)\\
&=&(a_{11}(x_1+y_1)+a_{12}(x_2+y_2)+\cdots+a_{1m}(x_m+y_m),\dots,a_{n1}(x_1+y_1)+\cdots + a_{nm}(x_m+y_m))\\
&=&(a_{11}x_1+a_{11}y_1+a_{12}x_2+a_{12}y_2+\cdots+a_{1m}x_m+a_{1m}y_m,\dots,a_{n1}x_1+a_{n1}y_1+\cdots + a_{nm}x_m+a_{nm}y_m)\\
&=&(a_{11}x_1+a_{12}x_2+\cdots+a_{1m}x_m,a_{21}x_1+\cdots + a_{2m}x_m,\dots,a_{n1}x_1+\cdots + a_{nm}x_m)\\
&&+(a_{11}y_1+a_{12}y_2+\cdots+a_{1m}y_m,a_{21}y_1+\cdots + a_{2m}y_m,\dots,a_{n1}y_1+\cdots + a_{nm}y_m)\\
&=&T(u)+T(v)\\
\end{eqnarray*}
Respeta la suma, veamos ahora qué pasa con la ponderación.
\begin{eqnarray*}
T(\alpha u)&=&(a_{11}\alpha x_1+a_{12}\alpha x_2+\cdots+a_{1m}\alpha x_m,a_{21}\alpha x_1+\cdots + a_{2m}\alpha x_m,\dots,a_{n1}\alpha x_1+\cdots + a_{nm}\alpha x_m)\\
&=&\alpha (a_{11}x_1+a_{12}x_2+\cdots+a_{1m}x_m,a_{21}x_1+\cdots + a_{2m}x_m,\dots,a_{n1}x_1+\cdots + a_{nm}x_m)\\
&=&\alpha T(u)
\end{eqnarray*}}
\hfill $\Box$

¿Qué funciones NO son lineales? Bueno, si $V=W=\R$, un ejemplo clásico de función que no es lineal es $\sen$, ya que $\sen(x+y)\not = \sen(x)+\sen(y)$. ¿Qué otros ejemplos conocen?

\bigskip


\begin{ejem}
\begin{itemize}
\item $F:\R^2\rightarrow\R^2$, definida por $F(x,y)=(x+1,y-1)$.\\
No es lineal, veamos:\\
$F((x,y)+(a,b))=F(x+a,y+b)=(x+a+1,y+b-1)$, y\\
$F(x,y)+F(a,b)=(x+1,y-1)+(a+1,b-1)=(x+a+2,y+b-2),$.\\
No son iguales, por lo tanto $F$ no cumple la definición de función lineal, no es lineal.
\item $E:\R^3\rightarrow\R^2$, definida por $E(x,y,z)=(xy,xz,zx)$.\\
No es lineal, veamos:\\
$E(\alpha(x,y,z))=E(\alpha x,\alpha y,\alpha z)=(\alpha x\alpha y,\alpha y\alpha z,\alpha z\alpha x)=\alpha^2(xy,xz,zx)$\\
$\alpha E(x,y,z)=\alpha(xy,xz,zx)$
No son iguales para $\alpha=2$ por ejemplo, por lo tanto $E$ no  es lineal.
\end{itemize}
\end{ejem}


Resulta que las funciones lineales heredan las siguientes propiedades.

\begin{prop}\label{teo:basic}
  Si $T:V\rightarrow W$ es lineal, entonces:
\begin{itemize}
\item $T(\Theta_V)=\Theta_W$, donde $\Theta_V$ es el vector neutro de $V$ y $\Theta_W$ el de $W$.
\item $\forall v\in V,\ T(-v)=-T(v)$.
\item $\forall \alpha_1,..,\alpha_k\in \K, \forall u_1,..,u_k\in V,\ {\displaystyle T\left(\sum_{i=1}^k \alpha_i u_i\right)=\sum_{i=1}^k \alpha_i T(u_i)}$.
\end{itemize}
\end{prop}

\newpage


{\bf Demostración.} {\small Usamos que $T$ es lineal en cada paso.
\begin{itemize}
\item $T(\Theta_V)=T(0\cdot \Theta_V)=0\cdot T(\Theta_V)=\Theta_W$.
\item Sea $v\in V$, $T(-v)=T(-1\cdot v)=-1\cdot T(v)=-T(v)$.
\item Sean $\alpha_1,..,\alpha_k\in \K$, y sean $u_1,..,u_k\in V,$ razonamos de manera inductiva:
$$\ {\displaystyle T\left(\sum_{i=1}^k \alpha_i u_i\right)=
T\left(\sum_{i=1}^{k-1} \alpha_i u_i\right)+T(\alpha_ku_k)=\cdots=
\sum_{i=1}^k T(\alpha_i u_i)=
\sum_{i=1}^k \alpha_i T(u_i)}.\quad\Box$$
\end{itemize}
}

\bigskip


\begin{ejem}
Decida cuáles de las siguientes funciones son lineales y cuáles no.
\begin{enumerate}
\item $G:\C^2\rightarrow \C^3$, $G(x,y)=(ix+y, 0, iy-x)$
\item $H:\C^3\rightarrow \C^2$, $H(x,y,z)=(\overline{x},y+z)$
\item $Q:\P(\R)\rightarrow \P(\R)$, $Q(p)(x)=x^2p(x)$
\item $M:\P(\R)\rightarrow \P(\R)$, $M(p)(x)=p(x)+x^2$
\item $N:\P_2(\R)\rightarrow \R^2$, $N(ax^2+bx+c)=(2a+b,a+b+c)$
\end{enumerate}
\end{ejem}


\vfill
{\small \rotatebox{180}{1. Sí. 2. depende si el cuerpo es $\R$ o $\C$. 3. Sí. 4. No. 5. Sí.}}

\section{Núcleo e Imagen}

\begin{defi}
Dada $T:V\rightarrow W$ lineal, se definen los siguientes conjuntos.

\begin{eqnarray*}
Ker(T)&=&\{ v\in V\ :\ T(v)=\Theta_W\}\\
\\
Im(T)&=&\{ T(v)\in W\ :\ v\in V\}\\
&=&\{ w\in W\ :\ (\exists v\in V)\ T(v)=w\}
\end{eqnarray*}
$Ker(T)$ se llama \emph{nucleo de $T$} y $Im(T)$ se llama \emph{imagen de $T$}.
\end{defi}
    
Estos conceptos no son nuevos, en realidad $Im(T)=rec(T)$, el recorrido de la función $T$, y $Ker(T)$ es el conjunto de ceros o raíces de $T$, solo que en el caso de las funciones lineales, el conjunto de raíces suele ser infinito, como se ver\'a.
El siguiente video ilustra estos conceptos: \url{https://vimeo.com/596627180/e895dd5db8}.

\begin{ejem}{\em 
Considerando $R:\R^2\rightarrow \R^2$ dada por $R(x,y)=(-x,y)$, entonces, 
\begin{eqnarray*}
Ker(R)&=&\{ (x,y)\in\R^2 \ :\ R(x,y)=(0,0)\}\\
&=&\{(x,y)\in\R^2: (-x,y)=(0,0)\}\\
&=&\{(x,y)\in\R^2: x=0\wedge y=0\}\\
&=&\{ (0,0)\},\\
\\
Im(R)&=&\{R(x,y) :\ (x,y)\in\R^2\}\\
&= &\{(-x,y)\ :\ x,y\in\R\}\\
&=&\R^2.
\end{eqnarray*}
}
\end{ejem}

\begin{ejem}{\em
Considerando $D:\P(\R)\rightarrow\P(\R)$, definida por $D(p)=p'$, entonces,
\begin{eqnarray*}
Ker(D)&=&\{ p\in\P(\R) \ :\ \forall x\in\R,\ p'(x)=0\}\\
&=&\{p\in\P(\R) \ : p\textrm{ es constante}\}\\
&=&\langle\{ a\}\rangle,\\
\\
Im(R)&=&\{p\in\P(\R) :\ \exists q\in\P(\R),\  p=q'\}\\
&=&\{p\in\P(\R) :\ p \textrm{ tiene primitiva en }\P(\R)\}\\
&=&\P(\R).
\end{eqnarray*}
Ya que todo polinomio tiene primitiva y ésta es también un polinomio.
}
\end{ejem}


 \begin{prop}
 Dada $T:V\rightarrow W$ lineal, se cumple que 
 \begin{itemize}
 \item $Ker(T)$ es un subespacio vectorial de $V$, e
 \item $Im(T)$ es un subespacio vectorial de $W$.
 \end{itemize}
 \end{prop}
 
 \underline{Demostración}:
Sea $T:V\rightarrow W$. Probaremos las 3 propiedades necesarias y suficientes para cada subespacio.\\

Primero $Ker(T)=\{ v\in V\ :\ T(v)=\Theta_W\}$.
\begin{enumerate}
\item $\Theta_V\in Ker(T)$ pues $T(\Theta_V)=\Theta_W$ por propiedad de transformaci\'on lineal.
\item Sean $u,v\in Ker(T)$, esto es $T(u)=T(v)=\Theta_W$, luego $T(u+v)=T(u)+T(v)=\Theta_W+\Theta_W=\Theta_W$ y por tanto $u+v\in Ker(T)$.
\item Sea $\alpha \in\K$ y sea $u\in Ker(T)$, luego $T(u)=\Theta_W$ y por tanto $T(\alpha u)=\alpha T(u)=\alpha \Theta_W=\Theta_W$ y en consecuencia $\alpha u\in Ker(T)$.
\end{enumerate}

\vspace{0.3 cm}


Por otro lado,  $Im(T)=\{ w\in W\ :\ (\exists v\in W)\ T(v)=w\}$.
\begin{enumerate}
\item $\Theta_W\in Im(T)$ ya que existe una preimagen: $T(\Theta_V)=\Theta_W$.
\item Sean $r,s\in Im(T)$, esto es, existen $u,v\in V$ tales que  $T(u)=r$ y $T(v)=s$.
Luego, $T(u+v)=T(u)+T(v)=r+s$, es decir, para $r+s$ existe una preimagen $u+v\in V$, por consiguiente $r+s\in Im(T)$.
\item Sea $\alpha \in\K$ y sea $r\in Im(T)$, esto es, existe $u\in V$ tal que $T(u)=r$.
De aqu\'i $T(\alpha u)=\alpha T(u)=\alpha r$, es decir, $\alpha r$ tiene preimagen por $T$ y es $\alpha u \in V$, en consecuencia $\alpha r \in Im(T)$.  \hfill $\Box$

\end{enumerate}
 

\vspace{0.2 cm}

 Dado que son subespacios, es posible calcular su dimensión, y ésta dir\'a mucho sobre la función:

\begin{defi}
Dada $T:V\rightarrow W$ lineal, se define:

\begin{itemize}
\item $\eta(T)=dim(Ker(T))$, y se llama \emph{nulidad} de $T$, y
\item $r(T)=dim(Im(T))$, y se llama \emph{rango} de $T$.
\end{itemize}
\end{defi}

\begin{ejem}{\em 
  En los ejemplos anteriores vemos que:  
  \begin{itemize}
  \item $\eta(R)=0$ y $r(R)=2$
  \item $\eta(D)=1$ y $r(D)$ no está definida pues $\P(R)$ NO tiene dimensión finita.
  \end{itemize}
  }
  \end{ejem}



\begin{ejem} {\em
Si $Q:\R^2\rightarrow \R^3$ está dada por $Q(x,y)=(x-y,0,y-x)$, entonces, 
\begin{eqnarray*}
Ker(Q)&=&\{ (x,y)\in\R^2\ :\ Q(x,y)=(0,0,0)\}\\
&=&\{ (x,y)\in\R^2\ :\ (x-y,0,y-x)=(0,0,0)\}\\
&=&\{ (x,y)\in\R^2\ :\ x-y=0\wedge y-x=0\}\\
&=&\{ (x,y)\in\R^2\ :\ x=y\}\\
&=&\{ (x,x)\in\R^2\ :\ x\in\R\}\\
&=&\langle\{ (1,1)\}\rangle\\
\\
Im(Q)&=&\{Q(x,y)\in\R^3 \ :\ (x,y)\in\R^2\}\\
&=&\{(x-y,0,y-x)\in\R^3 \ :\ (x,y)\in\R^2\}\\
&=&\{(x-y,0,y-x)=x(1,0,-1)+y(-1,0,1)\in\R^3 \ :\ x,y\in\R\}\\
&=&\langle\{(1,0,-1),(-1,0,1)\}\rangle\\
&=&\langle\{(1,0,-1)\}\rangle
\end{eqnarray*}
Así la nulidad de $Q$ es 1 y su rango también es 1.}
\end{ejem}

La siguiente propiedad es muy útil y es la manera que realmente se usa para calcular la imagen de una transformación lineal.

\begin{prop}\label{teo:imagen}
Dada $T:V\rightarrow W$ lineal, y $B=(v_1,..,v_m)$ una base de $V$, se tiene que:

$$ Im(T)=\langle \{T(v_1),..,T(v_m)\}\rangle.$$
\end{prop}

\underline{Demostración}: Si $\{v_1,..,v_m\}$ es una base de $V$, entonces se cumple
$$v\in V\Leftrightarrow \exists \alpha_1,\dots,\alpha_m\in\K, v=\sum_{i=1}^m \alpha_i v_i, $$
de aqu\'i:
\begin{eqnarray*}
Im(T)&=&\{ T(v)\ :\ v\in V\}\\
&=&\left\{ T(v)\ :\ \exists \alpha_1,\dots,\alpha_m, v=\sum_{i=1}^m\alpha_i v_i\right\}\\
&=&\left\{ T\left(\sum_{i=1}^m \alpha_iv_i\right)\ :\  \alpha_1,\dots,\alpha_m\in\K\right\}\\
&=&\left\{ \sum_{i=1}^m \alpha_iT(v_i)\ :\  \alpha_1,\dots,\alpha_m\in\K\right\}\\
&=&\langle\{T(v_1),..,T(v_m)\}\rangle.
\end{eqnarray*}
\hfill $\Box$\\

\begin{ejem}{\em
En uno de los ejemplos anteriores, $R:\R^2\rightarrow \R^2$ con $R(x,y)=(-x,y)$, si tomamos la base canónica $\Ccal_2=\{(1,0),(0,1)\}$ de $\R^2$ sobre $\R$, entonces:
\begin{eqnarray*}
Im(R)&=&\langle\{ R(1,0),R(0,1)\}\rangle\\
&=&\langle\{(-1,0),(0,1)\}\rangle\\
&=& \R^2.
\end{eqnarray*}
}
\end{ejem}

\begin{ejem}\label{parabola}
Consideremos la función  $P:\P_2(\R)\rightarrow \R^3$ definida por:
$$ P(p)=(p(0),p'(0),p''(0)).$$
Calculemos su núcleo e imagen nulidad y rango.
{\em
\begin{eqnarray*}
Ker(P)&=&\{p\in\P_2\ :\ (p(0),p'(0),p''(0))=(0,0,0)\}\\
&=&\{ax^2+bx+c\in\P_2\ :\ (c,b,2a)=(0,0,0)\}\\
&=&\{0\}
\end{eqnarray*}
Vemos entonces que $\eta(P)=0$.
%Dicho de otro modo, si conocemos la posición, primera derivada y segunda derivada de una parábola en 0, entonces la conocemos completamente.

\begin{eqnarray*}
Im(P)&=&\{(p(0),p'(0),p''(0))\in\R^3\ :\ p\in \P_2(\R)\}\\
&=&\{(c,b,2a)\in\R^3\ :\ a,b,c\in\R\}\\
&=&\langle\{(1,0,0),(0,1,0),(0,0,2)\}\rangle\\
&=&\R^3
\end{eqnarray*}
Vemos entonces que $r(P)=3$.
%Dicho de otro modo, la posición, primera derivada y segunda derivada de una parábola en 0 pueden tomar cualquier valor en $\R$.
}
\end{ejem}


En los ejemplos anteriores comprobamos que la nulidad es igual al rango menos la dimensión del espacio de partida $m$.
El siguiente es uno de los teoremas más importantes de esta área, y el más importante de la unidad.

\begin{teo}[Núcleo-Imagen]
Dada $T:V\rightarrow W$ lineal, se cumple:
$$ \eta(T)+r(T)=\dim(V)$$
\end{teo}
\underline{Demostración}:
  Primero que nada tomemos una base de $Ker(T)$, digamos: $\{u_1,\dots,u_k\}$, donde $k=\eta(T)$.
  \begin{itemize}
  \item Si $k=\dim(V)$, significa que $Ker(T)=V$, lo cual solo ocurre si se trata de la función nula, en ese caso $Im(T)=\{\Theta_W\}$, por lo tanto $r(T)=0$, así $\eta(T)+r(T)=\dim(V)+0=\dim(V)$, y se cumple el teorema trivialmente.

  \pq 

  \begin{quote}{\em Por ejemplo la función nula de $\K^5$ en $\K^3$ tiene fórmula: $T(x,y,z,t,s)=(0,0,0)$.}\end{quote}
      
  \item Si $k<\dim(V)$, agregamos $\dim(V)-k$ vectores a esta base, hasta obtener una base de $\dim(V)$, esto siempre es posible hacerlo, tal como vimos en el capítulo anterior.
  Obtenemos así una base de $V: \{u_1,\dots,u_k, u_{k+1},\dots,u_m\}$, donde $m=\dim(V)$\\

    A partir de la última propiedad del la clase anterior se obtiene la imagen de $T$:
    \begin{eqnarray*}
      Im(T)&=&\langle\{T(u_1),\dots,T(u_k),T(u_{k+1}),\dots,T(u_m)\}\rangle\\
      &=&\langle\{\Theta_W,\dots,\Theta_W,T(u_{k+1}),\dots,T(u_m)\}\rangle\\
      &=&\langle\{T(u_{k+1}),\dots,T(u_m)\}\rangle
    \end{eqnarray*}
    
    Se demostrar\'a que este conjunto de $m-k$ vectores generadores de $Im(T)$ es l. i. y así se concluirá el teorema.

    Sean $\lambda_{k+1},\dots,\lambda_m\in\K$ escalares tales que ${\displaystyle \sum_{i=k+1}^m\lambda_iT(u_i)=\Theta_W}$.
    
    \begin{eqnarray*}
      \sum_{i=k+1}^m\lambda_iT(u_i)=\Theta_W
      &\Leftrightarrow& T\left(\sum_{i=k+1}^m\lambda_iu_i\right)=\Theta_n\\
      &\Leftrightarrow& \sum_{i=k+1}^m\lambda_iu_i\in Ker(T)\\
      &\Leftrightarrow& \sum_{i=k+1}^m\lambda_iu_i\in \langle\{u_1,\dots,u_k\}\rangle\\
      &\Leftrightarrow& \exists \alpha_1,\dots,\alpha_k \sum_{i=k+1}^m\lambda_iu_i=\sum_{j=1}^k \alpha_ju_j\\
      &\Leftrightarrow& \exists \alpha_1,\dots,\alpha_k \sum_{j=1}^k \alpha_ju_j-\sum_{i=k+1}^m\lambda_iu_i=\Theta_m\quad\text{ ya que }\{u_1,\dots,u_m\}\text{ es base}\\
      &\Leftrightarrow& \forall i\in\{k+1,\dots,m\},\forall j\in\{1,\dots,k\},\lambda_i=0\wedge\alpha_j=0
    \end{eqnarray*}
  \end{itemize}

Así se demuestra que $\eta(T)+r(T)=k+m-k=m=\dim(V)$.\hfill $\Box$

\pq

\begin{ejem}{\em 
En el ejemplo~\ref{parabola} tenemos que $\eta(P)=0$ y $r(P)=3$, se verifica en efecto que $0+3=\dim(\P_2(\R))$.

Si hubiésemos conocido el teorema Núcleo-Imagen previamente habríamos podido adivinar $Im(P)$ sin necesidad de calcularlo, ya que al saber que $\eta(P)=0$, el teorema Núcleo-Imagen nos adelanta que $r(P)=\dim(\P_2(\R))-\eta(P)=3-0=3$, así $\dim(Im(P))=3=\dim(\R^3)$, y como $Im(P)$ es subespacio de $\R^3$, se concluye que $Im(P)=\R^3$.
}
\end{ejem}

\begin{ejem}
  Invente una aplicación lineal $G$ tal que $\eta(G)=3$ y $r(G)=2$.

  Primero que nada el espacio de partida debe tener dimensión 5, por ejemplo $\R^5$.

  Segundo, el espacio de llegada debe tener dimensión mayor o igual a 2, por ejemplo $\R^4$.

  Para no complicarnos la vida podemos definir $G$ simplemente como sigue.
  $$G:\R^5\rightarrow \R^4 $$
  $$G(a,b,c,d,e)=(a,b,0,0)$$

  Por las propiedades vistas, es evidente que $Im(G)=\langle\{(1,0,0,0),(0,1,0,0)\}\rangle$, por lo tanto $r(G)=\dim(Im(G))=2$.

  Aplicando el teorema núcleo-imagen concluimos que $\eta(G)=\dim(\R^5)-2=3$.
\end{ejem}


\section{Inyectividad y sobreyectividad}

    
Recordemos ahora conceptos importantes de la teoría de funciones.

\begin{itemize}
\item $T:A\rightarrow B$ es \emph{sobreyectiva} si y solo si $rec(T)=B$.
\item $T:A\rightarrow B$ es \emph{inyectiva} si y solo sí para todo $u\not=v$, $T(u)\not=T(v)$.
\end{itemize}

En otras palabras, una función es sobreyectiva si todo elemento del conjunto de llegada tiene al menos una preimagen, y una función es inyectiva si y solo si todo elemento del recorrido tiene una única preimagen.

\begin{obs}
Dada $T:V\rightarrow W$ lineal, se cumple la siguiente equivalencia.
$$ T\textrm{ es sobreyectiva} \Leftrightarrow Im(T)=W\Leftrightarrow r(T)=\dim(W)$$
\end{obs}
{\bf Demostración.} {\small Dado que $Im(T)$ es subespacio de $W$, si resultara que $r(T)=\dim(W)$, tendría que tenerse que $Im(T)=W$.
La recíproca es obvia.
\hfill $\Box$
}

La inyectividad por su parte está asociada a la nulidad de la transformación, gracias al siguiente teorema.

\begin{teo}
Dada $T:V\rightarrow W$ lineal, se cumple la siguiente equivalencia.
$$ T\textrm{ es inyectiva} \Leftrightarrow Ker(T)=\{\Theta_V\} \Leftrightarrow \eta(T)=0$$
\end{teo}
{\bf Demostración.} {\small
\begin{itemize}
\item[$\Rightarrow$] Si $T$ es inyectiva, $\Theta_W$ tiene una única preimagen, que es $\Theta_V$ gracias a la Propiedad~\ref{teo:basic}, lo cual implica que $Ker(T)=\{\Theta_V\}$, así $n(T)=0$.
\item[$\Leftarrow$] Supongamos que $\eta(T)=0$, esto significa que $Ker(T)=\{\Theta_V\}$, ya que el espacio nulo es el único espacio de dimensión nula.
Estudiemos ahora la inyectividad de $T$.
Para esto tomemos dos vectores arbitrarios $u,v \in V$, tales que $T(u)=T(v)$.
Trabajando esta ecuación obtenemos que: $T(u)-T(v)=\Theta_W$, entonces $T(u-v)=\Theta_W$, por lo tanto $u-v\in Ker(T)=\{\Theta_V\}$.
Pero esto significa que $u-v=\Theta_V$, ya que este es el único elemento dentro de $Ker(T)$.
Así, $u=v$, lo cual prueba que $T$ es inyectiva.\hfill $\Box$
\end{itemize}

}


\begin{ejem} La función reflexión $R$ es inyectiva claramente pues su nulidad es 0.
  Si lo pensamos ahora desde la intuición, al tratarse se una reflexion, es claro que dos vectores distintos no pueden hacerse iguales cuando se reflejan, reflejarse es una acción que no pierde información, por lo tanto es inyectiva.
\end{ejem} 

\begin{ejem}\label{parabola}
  Consideremos nuevamente el ejemplo~\ref{parabola}.
  
Teníamos que $\eta(P)=0$ por lo tanto $P$ es inyectiva.
Dicho de otro modo, si conocemos la posición, primera derivada y segunda derivada de una parábola en 0, entonces la conocemos completamente.

Por otra parte calculamos que $r(P)=3$, por lo tanto $P$ es sobreyectiva.
Dicho de otro modo, la posición, primera derivada (velocidad) y segunda derivada (aceleración) de una parábola en 0 pueden tomar cualquier valor en $\R$.
\end{ejem}


Si combinamos esto con el teorema Núcleo-Imagen, vemos que en algunos casos algunas propiedades son imposibles.

\begin{cor}
Dada una transformación lineal $T:V\rightarrow W$, podemos afirmar lo siguiente.
\begin{itemize}
\item Si $\dim(V)<\dim(W)$, entonces $T$ no puede ser sobreyectiva.
\item Si $\dim(V)>\dim(W)$, entonces $T$ no puede ser inyectiva.
\item Si $\dim(V)=\dim(W)$, entonces, $T$ es inyectiva si y solo si $T$ es sobreyectiva.
\end{itemize}
\end{cor}
{\bf Demostración.}
\begin{itemize}
\item Si $\dim(V)<\dim(W)$, entonces $r(T)=\dim(V)-\eta(T)\le \dim(V)<\dim(W)$, y para ser sobreyectiva su rango debe ser igual a $\dim(W)$.
\item Si $\dim(V)>\dim(W)$, entonces $\eta(T)=\dim(V)-r(T)\ge\dim(V)-\dim(W)>0$, y para ser inyectiva su nulidad debe ser 0.
\item Supongamos que $\dim(V)=\dim(W)$.
\begin{itemize}
\item[$\Rightarrow$] Si $T$ es inyectiva, entonces $\eta(T)=0$, entonces $r(T)=\dim(V)=\dim(W)$ por lo tanto $T$ es sobreyectiva.
\item[$\Leftarrow$] Si $T$ es sobreyectiva, entonces $r(T)=\dim(W)$, entonces $\eta(T)=\dim(V)-r(T)=\dim(V)-\dim(W)=0$ por lo tanto $T$ es inyectiva.
\end{itemize}
\end{itemize}
\hfill $\Box$


\begin{ejem} Consideremos la función $E:\K^5\rightarrow\K^6$ definida por 
$$E(x,y,z,s,t)=(x-z, 2x+y-s-2t, y-s, x-t, x-2z+t, z-t),$$
  Sin necesidad de calcular nada, ya sabemos que esta función no puede ser sobreyectiva.
\end{ejem}


\begin{cor}
Si $T:V\rightarrow W$ es lineal e inyectiva, y $B=\{v_1,..,v_k\}$ in conjunto l.i., entonces se tiene que el conjunto $\{T(v_1),..,T(v_k)\}$ es también l.i.
\end{cor}
{\bf Demostración.}
Primero que nada restrinjamos el dominio de $T$ al subespacio $U=\langle B\rangle$ de $V$, así tenemos que $T|_{U}:U\rightarrow W$ también es lineal e inyectiva.

Observamos que $\dim(U)=k$ dado que $B$ sería su base y esta tiene $k$ elementos.

Como $T_U$ es inyectiva, se tiene que $dim (Ker(T|_U))=0$, entonces el Teorema Núcleo-Imagen nos dice que
$$\dim(U)=0+r(T|_U)=r(T|_U)=\dim(Im(T|_U))$$
Dado que $Im(T|_U)=\langle\{T(v_1),..,T(v_k)\}\rangle$ está generado por $k=\dim(U)$ vectores y tiene dimensión $k$, no queda otra opción mas que su generador, $\{T(v_1),..,T(v_k)\}$, sea l.i.
\hfill $\Box$

\begin{ejem}
  Considere la siguiente función $L:\P_2(\R)\rightarrow \R^3$ definida por:
  $$L(p)=(p(-1),p(3),p(10)).$$
  Calculemos su núcleo.
  \begin{eqnarray*}
    Ker(L)&=&\{p\in\P_2(\R)\ :\ (p(-1),p(3),p(10))=(0,0,0)\}\\
    &=&\{p\in\P_2(\R)\ :\ p(-1)=0, p(3)=0, p(10)=0\}
  \end{eqnarray*}
  Es el conjunto de todos los polinomios de grado menor o igual a 2 que se anulan en $x=-1$, 3 y 10, es decir, que tienen 3 raíces y estas son -1, 3 y 10.
  Pero esto es imposible, los polinomios de grado menor o igual a 2 tienen a lo más 2 raíces, solo el polinomio nulo puede tener más de dos raíces... por lo tanto.
  $$Ker(L)=\{0\}\textrm{ y } \eta(L)=0$$
  De aquí vemos que $L$ es inyectiva, y además
  $$r(L)=\dim(\P_2(\R))-0=3,$$
  de donde se deduce también que $Im(L)=\R^3$, y que $L$ es sobreyectiva.

  Este resultado es interesante pues nos dice que para conocer completamente un parábola, basta conocerla en 3 puntos diferentes (-1, 3 y 10 son circunstanciales), y que dados 3 puntos cualesquiera (con abscisas distintas) siempre existirá una parábola que los visite.
\end{ejem}


\section{Representación Matricial}


La clase de las transformaciones lineales es muy redudida en el mundo de las transformaciones.
No cualquiera es una transformación lineal, y por ésto no podemos pedirle cualquier cosa, es más, si especificamos la imagen de una transformación lineal tan solo en unos cuantos puntos ya podemos deducir su valor en el espacio completo.

\begin{ejem}
  Sea una transformación lineal\;\;$T:\K^3\rightarrow\K^3$ tal que $T(1,0,0)=(1,0,1)$, $T(0,1,0)=(2,2,1)$ y $T(0,0,1)=(0,1,1)$. Encuentre $T(x,y,z)$.
  
{\em 
Observar que los vectores $(1,0,0)$, $(0,1,0)$ y $(0,0,1)$ forman la base can\'onica de $\K^3$, luego todo vector $(x,y,z)$ puede ser escrito como una combinaci\'on lineal ellos:
$$(x,y,z)=x(1,0,0)+y(0,1,0)+z(0,0,1)$$
donde $x,y,z\in\K$, y por tanto:
$$T(x,y,z)=T\Big(x(1,0,0)+y(0,1,0)+z(0,0,1)\Big)$$
\hspace{6.3 cm}$=T\Big(x(1,0,0)\Big)+T\Big(y(0,1,0)\Big)+T\Big(z(0,0,1)\Big)$

\hspace{6.3 cm}$=xT(1,0,0)+yT(0,1,0)+zT(0,0,1)$,\\

y puesto que $T(1,0,0)=(1,0,1)$, $T(0,1,0)=(2,2,1)$ y $T(0,0,1)=(0,1,1)$, entonces:\\

\hspace{6.3 cm}$=x(1,0,1)+y(2,2,1)+z(0,1,1)$

\hspace{6.3 cm}$=(x+2y,2y+z,x+y+z)$,\\

es decir, $T(x,y,z)=(x+2y,2y+z,x+y+z)$, o bien:
$$T\left(\begin{array}{c} x\\y \\ z\end{array}\right)=\left(\begin{array}{ccc} 1 & 2 & 0 \\ 0 &2 &1 \\ 1 & 1 & 1\end{array}\right)\left(\begin{array}{c} x\\y \\ z\end{array}\right)$$
}
\end{ejem}
 

 
Cuando los vectores donde conocemos la función conforman una base, el sistema que impongamos tendrá solución única, y por lo tanto podremos conocer la función completamente. 
El ejemplo anterior nos sugiere el siguiente resultado, cuya demostración sigue los mismos pasos que hemos hecho en el ejemplo, veamos. 

\begin{teo}[Fundamental del Álgebra Lineal]\label{teo:tfal}
  Para especificar completamente una transformación lineal de $V$ en $W$ basta conocer su imagen en una base de $V$.
\end{teo}
{\bf Demostración}
Supongamos que tenemos una base $B_V=\{v_1,\dots,v_m\}$ de $V$ y que $\dim(V)=m$.
Supongamos que tenemos una transformación lineal $T$ que conocemos bien en $B_V$, es decir, que sabemos que $T(v_j)=u_j$, con $u_1,\dots,u_m\in W$ vectores conocidos dados.

Queremos calcular $T$ en un vector arbitrario de $V$, sea $v$ un vector cualquiera, y supongamos que $[v]_{B_V}=\left(\begin{array}{c}
\alpha_1\\
\alpha_2\\
\vdots\\
\alpha_m
\end{array}\right)\in\K^m$, es decir que:
$$v=\sum_{j=1}^m \alpha_j v_j.$$
Entonces:
$$
T(v)=T\left(\sum_{j=1}^m \alpha_j v_j\right)=\sum_{j=1}^m \alpha_j T(v_j)=\sum_{j=1}^m \alpha_j u_j.
$$
Hemos demostrado que conocemos $T(v)$, ya que este queda expresado a partir de los datos dados y de las coordenadas de $v$ respecto a la base $B_V$.
\hfill $\Box$

Ahora bien, el ejemplo anterior tiene además la gracia que hemos podido representar la función como una multiplicación de matriz por vector.
Habíamos dicho que esto siempre es posible cuando la función va de $\K^m$ en $\K^n$, con cuerpo $\K$, pero en realidad es posible en todos los espacios, mientras tengan dimensión finita, todo está bien, veamos.

\begin{defi}[Matriz Representante]
Dada $T:V\rightarrow W$ lineal, $B_V=\{v_1,..,v_n\}$ una base de $V$ y $B_W=\{w_1,..,w_m\}$ una base de $W$, se define la matriz representante de $T$ respecto a $B_V$ y $B_W$ como sigue:

\[[T]_{B_V}^{B_W}=\left[\begin{array}{cccc}
| & | & \phantom{[T(v_1)]} & |\\
\left[T(v_1)\right]_{B_W} & \left[T(v_2)\right]_{B_W}& \cdots & \left[T(v_n)\right]_{B_W}\\
| & | & & | \end{array}\right]\in\mathcal{M}_{m\times n}(\K)\]
\end{defi}

\begin{ejem}
Si tomamos la transformación $D:\P_3(\R)\rightarrow\P_2(\R)$, definida por $D(p)=p'$, donde $p'$ es la \emph{derivada} de $p$, y consideramos la base canónica de $\P_3(\R)$ y $\P_2(\R)$, $\Ccal_{\P_3}=\{1,x,x^2,x^3\}$ y $\Ccal_{\P_2}=\{1,x,x^2\}$, respectivamente.

{\em Entonces calculamos la matriz representante de $D$ respecto a $\Ccal_{\P_3}$ como sigue.

\begin{tabular}{c|ccl|l}
$p$&$D(p)$&&descomposición c/r $\Ccal_{\P_2}$&$[D(p)]_{\Ccal_{\P_2}}$\\\hline
$1$ & $0$ &=&$0\cdot1+0x+0x^2$& $(0,0,0)$\\
$x$& $1$&=&$1\cdot1+0x+0x^2$& $(1,0,0)$\\
$x^2$&$2x$&=&$0\cdot1+2x+0x^2$& $(0,2,0)$\\
$x^3$&$3x^2$&=&$0\cdot1+0x+3x^2$& $(0,0,3)$\\
\end{tabular}

Así, la matriz representante de $D$ respecto a la base $\Ccal_{\P_3}$ es:

$$[D]_{\Ccal_{\P_3}}^{\Ccal_{\P_2}}=
\left(\begin{array}{cccc}
0&1&0&0\\
0&0&2&0\\
0&0&0&3
\end{array}\right).$$
}
\end{ejem}

\begin{ejem}
Si tomamos la transformación $T:\M_{2\times 3}(\R)\rightarrow\M_{3\times 2}(\R)$ , definida por $T(A)=A^T$, donde $A^T$ es la traspuesta de $A$, y consideramos las bases canónicas de $\M_{2\times3}(\R)$ y $\M_{3\times2}(\R)$, $\Ccal_{\M_{2\times3}}$ y $\Ccal_{\M_{3\times2}}$, respectívamente.

{\em Entonces calculamos la matriz representante de $T$ respecto a $\Ccal_{\M_{2\times3}}$ y $\Ccal_{\M_{3\times2}}$ como sigue.

{\small
\begin{tabular}{c|ll}
$A$&\hspace{.3cm}$T(A)$\hspace{1.3cm}descomposición c/r $\Ccal_{\M_{3\times2}}$\\\hline
$\left(\begin{array}{ccc} 1&0&0\\0&0&0\end{array}\right)$ & $\left(\begin{array}{cc} 1&0\\0&0\\0&0\end{array}\right)=\color{red}1
\left(\begin{array}{cc} 1&0\\0&0\\0&0\end{array}\right)+0
\left(\begin{array}{cc} 0&1\\0&0\\0&0\end{array}\right)+0
\left(\begin{array}{cc} 0&0\\1&0\\0&0\end{array}\right)+0
\left(\begin{array}{cc} 0&0\\0&1\\0&0\end{array}\right)+0
\left(\begin{array}{cc} 0&0\\0&0\\1&0\end{array}\right)+0
\left(\begin{array}{cc} 0&0\\0&0\\0&1\end{array}\right)$\\\hline
$\left(\begin{array}{ccc} 0&1&0\\0&0&0\end{array}\right)$& $\left(\begin{array}{cc} 0&0\\1&0\\0&0\end{array}\right)=\color{orange}0
\left(\begin{array}{cc} 1&0\\0&0\\0&0\end{array}\right)+0
\left(\begin{array}{cc} 0&1\\0&0\\0&0\end{array}\right)+1
\left(\begin{array}{cc} 0&0\\1&0\\0&0\end{array}\right)+0
\left(\begin{array}{cc} 0&0\\0&1\\0&0\end{array}\right)+0
\left(\begin{array}{cc} 0&0\\0&0\\1&0\end{array}\right)+0
\left(\begin{array}{cc} 0&0\\0&0\\0&1\end{array}\right)$\\\hline
$\left(\begin{array}{ccc} 0&0&1\\0&0&0\end{array}\right)$&$\left(\begin{array}{cc} 0&0\\0&0\\1&0\end{array}\right)=\color{green}0
\left(\begin{array}{cc} 1&0\\0&0\\0&0\end{array}\right)+0
\left(\begin{array}{cc} 0&1\\0&0\\0&0\end{array}\right)+0
\left(\begin{array}{cc} 0&0\\1&0\\0&0\end{array}\right)+0
\left(\begin{array}{cc} 0&0\\0&1\\0&0\end{array}\right)+1
\left(\begin{array}{cc} 0&0\\0&0\\1&0\end{array}\right)+0
\left(\begin{array}{cc} 0&0\\0&0\\0&1\end{array}\right)$\\\hline
$\left(\begin{array}{ccc} 0&0&0\\1&0&0\end{array}\right)$&$\left(\begin{array}{cc} 0&1\\0&0\\0&0\end{array}\right)=\color{cyan}0
\left(\begin{array}{cc} 1&0\\0&0\\0&0\end{array}\right)+1
\left(\begin{array}{cc} 0&1\\0&0\\0&0\end{array}\right)+0
\left(\begin{array}{cc} 0&0\\1&0\\0&0\end{array}\right)+0
\left(\begin{array}{cc} 0&0\\0&1\\0&0\end{array}\right)+0
\left(\begin{array}{cc} 0&0\\0&0\\1&0\end{array}\right)+0
\left(\begin{array}{cc} 0&0\\0&0\\0&1\end{array}\right)$\\\hline
$\left(\begin{array}{ccc} 0&0&0\\0&1&0\end{array}\right)$&$\left(\begin{array}{cc} 0&0\\0&1\\0&0\end{array}\right)=\color{blue}0
\left(\begin{array}{cc} 1&0\\0&0\\0&0\end{array}\right)+0
\left(\begin{array}{cc} 0&1\\0&0\\0&0\end{array}\right)+0
\left(\begin{array}{cc} 0&0\\1&0\\0&0\end{array}\right)+1
\left(\begin{array}{cc} 0&0\\0&1\\0&0\end{array}\right)+0
\left(\begin{array}{cc} 0&0\\0&0\\1&0\end{array}\right)+0
\left(\begin{array}{cc} 0&0\\0&0\\0&1\end{array}\right)$\\\hline
$\left(\begin{array}{ccc} 0&0&0\\0&0&1\end{array}\right)$&$\left(\begin{array}{cc} 0&0\\0&0\\0&1\end{array}\right)=\color{violet}0
\left(\begin{array}{cc} 1&0\\0&0\\0&0\end{array}\right)+0
\left(\begin{array}{cc} 0&1\\0&0\\0&0\end{array}\right)+0
\left(\begin{array}{cc} 0&0\\1&0\\0&0\end{array}\right)+0
\left(\begin{array}{cc} 0&0\\0&1\\0&0\end{array}\right)+0
\left(\begin{array}{cc} 0&0\\0&0\\1&0\end{array}\right)+1
\left(\begin{array}{cc} 0&0\\0&0\\0&1\end{array}\right)$
\end{tabular}
}



Así, la matriz representante de $T$ respecto a las bases $\Ccal_{\M_{2\times3}}$ y $\Ccal_{\M_{3\times2}}$ es:

$$[T]_{\Ccal_{\M_{2\times3}}}^{\Ccal_{\M_{3\times2}}}=
\left(\begin{array}{cccccc}
\color{red}1&\color{orange}0&\color{green}0&\color{cyan}0&\color{blue}0&\color{violet}0\\
\color{red}0&\color{orange}0&\color{green}0&\color{cyan}1&\color{blue}0&\color{violet}0\\
\color{red}0&\color{orange}1&\color{green}0&\color{cyan}0&\color{blue}0&\color{violet}0\\
\color{red}0&\color{orange}0&\color{green}0&\color{cyan}0&\color{blue}1&\color{violet}0\\
\color{red}0&\color{orange}0&\color{green}1&\color{cyan}0&\color{blue}0&\color{violet}0\\
\color{red}0&\color{orange}0&\color{green}0&\color{cyan}0&\color{blue}0&\color{violet}1
\end{array}\right).$$
}
\end{ejem}

Ahora, pero ¿para qué sirve esta matriz? ¿cómo se usa?, pues igual como lo hicimos en el Ejemplo 1, pero teniendo el cuidado de usar el vector coordenada del vector donde queremos aplicar la transformación.


\begin{prop}
La matriz representante de $T$ es la matriz que cumple, para todo $v\in V$:

$$ [T(v)]_{B_W}=[T]_{B_V}^{B_W}[v]_{B_V}.$$
\end{prop}
{\bf Demostración.}
Supongamos primero que $B_V=\{v_1,\dots,v_m\}$ y que $B_W=\{w_1,\dots,w_n\}$, y que $m=\dim(V)$ y $n=\dim(W)$.
Supongamos además que $[T]_{B_V}^{B_W}=R\in\M_{n\times m}(\K)$.
Esto nos dice que 
$$[T(v_j)]=j-\textit{ésima columna de }R,$$
y por lo tanto,
$$T(v_j)=\sum_{i=1}^n R_{ij} w_i.$$
Finalmente, supongamos que $[v]_{B_V}=\left(\begin{array}{c}
\alpha_1\\
\alpha_2\\
\vdots\\
\alpha_m
\end{array}\right)\in\K^m$, es decir, se tiene que:
$$v=\sum_{j=1}^m \alpha_j v_j.$$
Entonces:
\begin{eqnarray*}
T(v)&=&T\left(\sum_{j=1}^m \alpha_j v_j\right)\\
&=&\sum_{j=1}^m \alpha_j T(v_j)\\
&=&\sum_{j=1}^m \alpha_j \sum_{i=1}^n R_{ij} w_i\\
&=&\sum_{i=1}^n\left(\sum_{j=1}^m \alpha_j  R_{ij}\right) w_i.
\end{eqnarray*}
Por lo tanto,
$$[T(v)]_{B_W}=\left(\begin{array}{c}\sum_{j=1}^m \alpha_j  R_{1j}\\
\sum_{j=1}^m \alpha_j  R_{2j}\\
\vdots\\
\sum_{j=1}^m \alpha_j  R_{nj}\end{array}\right)=
\left(\begin{array}{cccc}
R_{11} & R_{12} & \cdots & R_{1m}\\
R_{21} & R_{22} & \cdots & R_{2m}\\
\vdots & \vdots & \ddots & \vdots\\
R_{n1} & R_{n2} & \cdots & R_{nm}
\end{array}\right)
\left(\begin{array}{c}
\alpha_1\\
\alpha_2\\
\vdots\\
\alpha_m
\end{array}\right)
=[T]_{B_V}^{B_W}[v]_{B_V}
$$
Para ver que no hay otra matriz que cumpla esto, basta evaluar $T$ en $v_j$, como $[v_j]_{B_V}=\left(\begin{array}{c}
0\\
\vdots\\
1\\
\vdots\\
0
\end{array}\right)$, entonces $[T(v_j)]_{B_W}=\left(\begin{array}{c}
R_{1j}\\
\vdots\\
R_{nj}
\end{array}\right)$, y como las coordenadas de un vector respecto a una base dada son únicas, cualquier otra matriz que cumpla la igualdad de este teorema, deberá tener sus columnas iguales a las de $R$.
\hfill $\Box$

Es decir, la matriz representante de una transformación lineal en efecto sirve para calcular la transformación mediante una multiplicación de matriz por vector, pero actúa sobre el vector de coordenadas del vector donde se quiere aplicar, por ello es que depende de que estén especificadas las bases respecto a las cuales se usa.

\begin{ejem}
En el Ejemplo 1, la matriz que encontramos funciona con los vectores escritos respecto a las bases canónicas:
$$[T]_{\Ccal_3}^{\Ccal_3}=\left(\begin{array}{ccc} 1 & 2 & 0 \\ 0 &2 &1 \\ 1 & 1 & 1\end{array}\right),$$
si $v=(x,y,z)$, se cumple:
$$T(v)=[T(v)]_{\Ccal_3}=\left(\begin{array}{ccc} 1 & 2 & 0 \\ 0 &2 &1 \\ 1 & 1 & 1\end{array}\right)[v]_{\Ccal_3}=\left(\begin{array}{ccc} 1 & 2 & 0 \\ 0 &2 &1 \\ 1 & 1 & 1\end{array}\right)\left(\begin{array}{c} x\\y \\ z\end{array}\right)$$
\end{ejem}

\begin{ejem}
En el ejemplo de la derivada verificamos lo siguiente.
$$[ax^3+bx^2+cx+d]_{\Ccal_{P_3}}=
\left(\begin{array}{c} d\\c \\ b\\ a\end{array}\right)$$
$$[D(ax^3+bx^2+cx+d)]_{\Ccal_{P_3}}=
\left(\begin{array}{cccc}
0&1&0&0\\
0&0&2&0\\
0&0&0&3
\end{array}\right)
\left(\begin{array}{c} d\\c \\ b\\ a\end{array}\right)=
\left(\begin{array}{c} c\\2b \\ 3a\end{array}\right)
$$
Por lo tanto $D(ax^3+bx^2+cx+d)=c+2bx+3ax^2$
\end{ejem}

\begin{ejem}
En el ejemplo de la traspuesta verificamos que.
$$\left[\left(\begin{array}{ccc} a&b&c\\d&e&f\end{array}\right)\right]_{\Ccal_{2\times3}}=
\left(\begin{array}{c}a\\b\\c\\d\\e\\f\end{array}\right)$$
$$\left[T\left(\begin{array}{ccc} a&b&c\\d&e&f\end{array}\right)\right]_{\Ccal_{2\times3}}=
\left(\begin{array}{cccccc}
\color{red}1&\color{orange}0&\color{green}0&\color{cyan}0&\color{blue}0&\color{violet}0\\
\color{red}0&\color{orange}0&\color{green}0&\color{cyan}1&\color{blue}0&\color{violet}0\\
\color{red}0&\color{orange}1&\color{green}0&\color{cyan}0&\color{blue}0&\color{violet}0\\
\color{red}0&\color{orange}0&\color{green}0&\color{cyan}0&\color{blue}1&\color{violet}0\\
\color{red}0&\color{orange}0&\color{green}1&\color{cyan}0&\color{blue}0&\color{violet}0\\
\color{red}0&\color{orange}0&\color{green}0&\color{cyan}0&\color{blue}0&\color{violet}1
\end{array}\right)
\left(\begin{array}{c}\color{red}a\\\color{orange}b\\\color{green}c\\\color{cyan}d\\\color{blue}e\\\color{violet}f\end{array}\right)=
\left(\begin{array}{c}\color{red}a\\\color{cyan}d\\\color{orange}b\\\color{blue}e\\\color{green}c\\\color{violet}f\end{array}\right)
$$
Como $\Ccal_{3\times 2}=\left\{\left(\begin{array}{cc} 1&0\\0&0\\0&0\end{array}\right),
\left(\begin{array}{cc} 0&1\\0&0\\0&0\end{array}\right),
\left(\begin{array}{cc} 0&0\\1&0\\0&0\end{array}\right),
\left(\begin{array}{cc} 0&0\\0&1\\0&0\end{array}\right),
\left(\begin{array}{cc} 0&0\\0&0\\1&0\end{array}\right),
\left(\begin{array}{cc} 0&0\\0&0\\0&1\end{array}\right)\right\}$,\break verificamos que 
$$T\left(\begin{array}{ccc} a&b&c\\d&e&f\end{array}\right)=\left(\begin{array}{cc} a&d\\b&e\\c&f\end{array}\right).$$
\end{ejem}

Vamos a terminar esta sección estableciendo resultados importantes que relacionan la composición de funciones con la multiplicación de matrices.
%
%\begin{prop}
%\begin{itemize}
%\item Dada $T:V\rightarrow W$ lineal e invertible, resulta que $T^{-1}:W\rightarrow V$ es también lineal.
%\item Dadas $L:U\rightarrow V$ y $T:V\rightarrow W$ lineales, resulta que $T\circ L:U\rightarrow W$ es también lineal.
%\end{itemize}
%\end{prop}
\begin{prop}
\begin{enumerate}
\item Dada $B_V$ una base de $V$, se cumple: $$[id]_{B_V}^{B_V}=I_n.$$
\item Dada $T:V\rightarrow W$ lineal e invertible, dadas $B_V$ base de $V$ y $B_W$ base de $W$, resulta que $T^{-1}:W\rightarrow V$ es también lineal y
$$[T^{-1}]_{B_W}^{B_V}=\left([T]_{B_V}^{B_W}\right)^{-1}.$$
\item Dadas $L:U\rightarrow V$ y $T:V\rightarrow W$ lineales, y dadas $B_U$ base de $U$, $B_V$ base de $V$ y $B_W$ base de $W$, resulta que $T\circ L:U\rightarrow W$ es también lineal y
$$[T\circ L]_{B_U}^{B_W}=[T]_{B_V}^{B_W}[L]_{B_U}^{B_V}.$$
%\item $dim(\mathcal{L}(V,W))=dim(V)dim(W)$.
\end{enumerate}
\end{prop}
{\bf Demostración.} {\small
\begin{enumerate}
  \item Es claro que la identidad es la única matriz que cumple:
  $$[id(v)]_{B_V}=[v]_{B_V}=I_n[v]_{B_V}.$$
 \item Supongamos ahora que $T$ es invertible, esto significa que $T$ es inyectiva y sobreyectiva.
 Usando el teorema núcleo-imagen sabemos que si es inyectiva, entonces $\dim(V)\le \dim(W)$, y si es sobreyectiva, entonces $\dim(V)\ge\dim(W)$., por lo tanto $\dim(V)=\dim(W)$.
 Así la matriz representante de $T$ es cuadrada, es además de rango completo, por lo tanto es una matriz invertible.
 
 Sea ahora $w\in W$ y $v\in V$ tales que $T(v)=w$, es decir: $v=T^{-1}(w)$.
 Esto lo podemos expresar en la matriz representante a través de los vectores coordenada de $v$ y $w$:
 $$ [w]_{B_W}=[T(v)]_{B_W}=[T]_{B_V}^{B_W}[v]_{B_V}.$$
 Pero despejando $v$ en términos de $w$, obtenemos:
 $$ ([T]_{B_V}^{B_W})^{-1}[w]_{B_W}=[v]_{B_V}=[T^{-1}(w)]_{B_V}=[T^{-1}]_{B_W}^{B_V}[w]_{B_W},$$
 como esto se cumple para todo $w$, demuestra que $[T^{-1}]_{B_W}^{B_V}=\left([T]_{B_V}^{B_W}\right)^{-1}$.
  \item Sea $u\in U$, se cumple:
    $$[(T\circ L)]_{B_U}^{B_W}[u]_{B_U}
    =[(T\circ L)(u)]_{B_W}
    =[T(L(u))]_{B_W}
    =[T]_{B_V}^{B_W}[L(u)]_{B_V}
    =[T]_{B_V}^{B_W}[L]_{B_U}^{B_V}[u]_{B_U},$$
    lo que prueba que $[T\circ L]_{B_U}^{B_W}=[T]_{B_V}^{B_W}[L]_{B_U}^{B_V}$.\hfill $\Box$   
\end{enumerate}
}

\begin{ejem}
  Sean $H:\R^2\rightarrow \R^4$ definida por $H(x,y)=(x+2y,x,-y,x+y)$, y $T:\R^4\rightarrow \R^3$ definida por $T(a,b,c,d)=(a+b,2a+b,c+d)$.

  {\em La composición de $H$ con $T$ es la función $T\circ H:\R^2\rightarrow \R^3$ definida por:
  \begin{eqnarray*}
    (T\circ H)(x,y)&=&T(H(x,y))\\
    &=&T(\color{red}{x+2y},\color{blue}{x},\color{orange}{-y},\color{magenta}{x+y}\color{black}{)}\\
    &=&(\color{red}{x+2y}\color{black}{+}\color{blue}{x},\color{black}{2(}\color{red}{x+2y}\color{black}{)+}\color{blue}{x},\color{orange}{-y}\color{black}{+}\color{magenta}{x+y})\\
    &=&(2x+2y,3x+4y,x)
  \end{eqnarray*}
  Matricialmente vemos que:
  \[
    M_H=\left(\begin{array}{rr}1&2\\1&0\\0&-1\\1&1\end{array}\right)\qquad
    M_T=\left(\begin{array}{rrrr}1&1&0&0\\2&1&0&0\\0&0&1&1\end{array}\right),
  \]
  por lo tanto:
  \[
  M_{T\circ H}=\left(\begin{array}{rrrr}1&1&0&0\\2&1&0&0\\0&0&1&1\end{array}\right)
    \left(\begin{array}{rr}1&2\\1&0\\0&-1\\1&1\end{array}\right)
      =\left(\begin{array}{rr}2&2\\3&4\\1&0\end{array}\right),
        \]
  que coincide con lo que habríamos obtenido directamente de la fórmula de $T\circ H$.}
\end{ejem}

\begin{ejem}
  Consideremos ahora la función $G:\R^2\rightarrow\R^2$ definida por $G(a,b)=(a+2b,2a+4b)$.
  Su matriz asociada es claramente:
  $$M_G=\left(\begin{array}{rr}1&2\\2&4\end{array}\right).$$
  La cual es una matriz no invertible, de donde obtenemos que $G$ tampoco lo es.
\end{ejem}
  
  
\section{Matriz de Paso}

\begin{obs}
Si consideramos dos bases distintas de $V$, $B_1$ y $B_2$, entonces la matriz representante de la función identidad de $V$ en $V$ ya no será la matriz identidad, será una matriz interesante que llamamos \emph{matriz de paso de la base $B_1$ a la base $B_2$}, y la denotamos por:

$$ P_{B_1}^{B_2}=[id]_{B_1}^{B_2}$$

Cumple la interesante propiedad que para todo $v\in V$:

$$ P_{B_1}^{B_2}[v]_{B_1}=[v]_{B_2},$$

además

$$ P_{B_1}^{B_2}=\left(P_{B_2}^{B_1}\right)^{-1}.$$
\end{obs}

\begin{ejem}
Considere el espacio de los polinomios de grado menor o igual que 3, $\mathcal{P}_3(\R)$, y su base canónica, $\Ccal_{\P_3}=\{1,x,x^2,x^3\}$.
Considere también la base $\B=\{ x^3-1, x^2+x-1, x+1,1\}$.

{\em La matriz de paso de $\B$ a $\Ccal_{\P_3}$ corresponde a la matriz representante de la función identidad respecto a estas bases, veamos.

\begin{tabular}{r|rcr|r}
$p$&$id(p)$&&descomposición c/r $\Ccal_{\P_2}$&$[P]_{\Ccal_{\P_2}}$\\\hline
$x^3-1$ & $x^3-1$ &=&$-1\cdot1+0\cdot x+0\cdot x^2+1\cdot x^3$& $(-1,0,0,1)$\\
$x^2+x-1$& $x^2+x-1$&=&$-1\cdot1+1\cdot x+1\cdot x^2+0\cdot x^3$& $(-1,1,1,0)$\\
$x+1$&$x+1$&=&$1\cdot1+1\cdot x+0\cdot x^2+0\cdot x^3$& $(1,1,0,0)$\\
$1$&$1$&=&$1\cdot1+0\cdot x+0\cdot x^2+0\cdot x^3$& $(1,0,0,0)$\\
\end{tabular}

Por lo tanto, la matriz es:
$$P_{\B}^{\Ccal_{\P_3}} = \left(\begin{array}{rrrr}
-1&-1&1&1\\
0&1&1&0\\
0&1&0&0\\
1&0&0&0
\end{array}\right).
$$
Ahora bien, la matriz  de paso de $\Ccal_{\P_3}$ a $\B$ es la inversa de esta matriz.
$$P_{\Ccal_{\P_3}}^{\B} = (P_{\B}^{\Ccal_{\P_3}})^{-1}
$$
Calculémosla.

{\small
$
\left(\begin{array}{rrrr|rrrr}
-1&-1&1&1&1&0&0&0\\
0&1&1&0&0&1&0&0\\
0&1&0&0&0&0&1&0\\
1&0&0&0&0&0&0&1
\end{array}\right)
\sim
\left(\begin{array}{rrrr|rrrr}
-1&-1&1&1&1&0&0&0\\
0&1&1&0&0&1&0&0\\
0&1&0&0&0&0&1&0\\
0&-1&1&1&1&0&0&1
\end{array}\right)
\sim
\left(\begin{array}{rrrr|rrrr}
-1&-1&1&1&1&0&0&0\\
0&1&1&0&0&1&0&0\\
0&0&-1&0&0&-1&1&0\\
0&0&2&1&1&0&1&1
\end{array}\right)
$\\

$
\sim
\left(\begin{array}{rrrr|rrrr}
-1&-1&1&1&1&0&0&0\\
0&1&1&0&0&1&0&0\\
0&0&-1&0&0&-1&1&0\\
0&0&0&1&1&-2&3&1
\end{array}\right)
\sim
\left(\begin{array}{rrrr|rrrr}
-1&-1&1&0&0&2&-3&-1\\
0&1&1&0&0&1&0&0\\
0&0&-1&0&0&-1&1&0\\
0&0&0&1&1&-2&3&1
\end{array}\right)
$\\

$
\sim
\left(\begin{array}{rrrr|rrrr}
-1&-1&0&0&0&1&-2&-1\\
0&1&0&0&0&0&1&0\\
0&0&-1&0&0&-1&1&0\\
0&0&0&1&1&-2&3&1
\end{array}\right)
\sim
\left(\begin{array}{rrrr|rrrr}
-1&0&0&0&0&1&-1&-1\\
0&1&0&0&0&0&1&0\\
0&0&-1&0&0&-1&1&0\\
0&0&0&1&1&-2&3&1
\end{array}\right)
$\\

$
\sim
\left(\begin{array}{rrrr|rrrr}
1&0&0&0&0&-1&1&1\\
0&1&0&0&0&0&1&0\\
0&0&1&0&0&1&-1&0\\
0&0&0&1&1&-2&3&1
\end{array}\right)
$
$$
P_{\Ccal_{\P_3}}^{\B} = 
\left(\begin{array}{rrrr}
0&-1&1&1\\
0&0&1&0\\
0&1&-1&0\\
1&-2&3&1
\end{array}\right)
$$
}

Entonces ahora podemos usar esta matriz para encontrar el vector de coordenadas de un polinomio respecto a la base $\B$.

\begin{eqnarray*}
[ax^3+bx^2+cx+d]_{\B}&=&P_{\Ccal_{\P_3}}^{\B} [ax^3+bx^2+cx+d]_{\Ccal_{\P_3}}\\
&=&\left(\begin{array}{rrrr}
0&-1&1&1\\
0&0&1&0\\
0&1&-1&0\\
1&-2&3&1
\end{array}\right)
\left(\begin{array}{c} d\\c\\b\\a\end{array} \right)\\
&=& \left(\begin{array}{c} a+b-c\\c\\c-b\\a+3b-3c+d\end{array} \right)
\end{eqnarray*}
Por ejemplo para $x^3-2$, su vector de coordenadas serían:
$$[x^3-2]_{\B}=\left(\begin{array}{c} 1\\0\\0\\-1\end{array} \right),$$
lo cual implica que:
$$x^3-2= 1\cdot(x^3-1)+0\cdot( x^2+x-1)+0\cdot( x+1)+(-1)\cdot1=x^3-1-1$$
}
\end{ejem}

Esta matriz sirve para obtener fácilmente el vector de coordenadas.
Pero también sirve para ayudar en la búsqueda y cálculo de la matriz representante de una transformación lineal gracias a la siguiente observación.

\begin{obs}
Si tenemos dos bases de $W$, $B_{1W}$ y $B_{2W}$, y dos bases de $V$, $B_{1V}$ y $B_{2V}$, entonces:
$$ [T]_{B_{1V}}^{B_{1W}}=P_{B_{2W}}^{B_{1W}} [T]_{B_{2V}}^{B_{2W}} P_{B_{1V}}^{B_{2V}}.$$
Es decir, la matriz de paso sirve para encontrar una matriz relativa a unas bases a partir de la matriz relativa a otras bases, y esto también es útil para encontrar la fórmula de la transformación a partir de su valor en vectores dados.
\end{obs}

\begin{ejem}
 {\em 
    Supongamos que tenemos una transformación lineal $F:\R^2\rightarrow\R^3$ tal que\break $F(1,1)=(1,1,1)$, $F(1,2)=(1,2,1)$, entonces, para poder conocer a $F$ necesitamos que $\D=\{(1,1),(1,2)\}$ sea base de $\R^2$, afortunadamente lo es.
    Luego, la matriz representante de $F$ respecto a la base $\D$ de $\R^2$ y la base canónica $\Ccal_3$ de $\R^3$ es 
    $$ [F]_{\D}^{\Ccal_3}=\left(\begin{array}{cc} 1& 1\\ 1 & 2\\1&1\end{array}\right).$$
    Si buscamos su matriz representante respecto a las bases canónicas de ambos espacios respectivamente, necesitamos la matriz de paso de la base canónica $\Ccal_2$ de $\R^2$ a la base $\D$.
    
    Como antes, es más sencillo calcular la matriz de paso de $\D$ a $\Ccal_2$, pues basta escribir los vectores de $\D$ en términos de los vectores canónicos, y eso son los mismos vectores tal cual, luego ponerlos como columna, entonces:
    $$P_\D^{\Ccal_2}=\left(\begin{array}{rr} 1& 1\\ 1 & 2\end{array}\right),$$
   luego ocupar la fórmula de la inversa para obtener la matriz de paso inverso:
   $$P_{\Ccal_2}^\D=(P_\D^{\Ccal_2})^{-1}=\left(\begin{array}{rr} 1& 1\\ 1 & 2\end{array}\right)^{-1}=\left(\begin{array}{rr} 2& -1\\ -1 & 1\end{array}\right).$$
Entonces, aplicando la fórmula de la observación anterior tenemos:
$$[F]_{\Ccal_2}^{\Ccal_3}=[F]_{\D}^{\Ccal_3}P_{\Ccal_2}^\D=
  =\left(\begin{array}{cc} 1& 1\\ 1 & 2\\1&1\end{array}\right)\left(\begin{array}{rr} 2& -1\\ -1 & 1\end{array}\right)
  =\left(\begin{array}{cc} 1&0\\0&1\\1&0\end{array}\right).$$
Con esto podemos obtener la fórmula de $F$ fácilmente:
$$F\left(\begin{array}{c}x\\y\end{array}\right)=\left(\begin{array}{cc} 1&0\\0&1\\1&0\end{array}\right)\left(\begin{array}{c}x\\y\end{array}\right)=\left(\begin{array}{c}x\\y\\x\end{array}\right)$$
Que en efecto cumple:
$$F\left(\begin{array}{c}1\\1\end{array}\right)=\left(\begin{array}{c}1\\1\\1\end{array}\right)\qquad F\left(\begin{array}{c}1\\2\end{array}\right)=\left(\begin{array}{c}1\\2\\1\end{array}\right)$$
tal como se pedía al comienzo.
}
\end{ejem}


\begin{defi}
Dos matrices $A$, $B$ cuadradas en $\mathcal{M}_{n\times n}(\K)$ se dicen \emph{semejantes} si existe una matriz $M\in \mathcal{M}_{n\times n}(\K)$ tal que $A=M^{-1} B M$.
\end{defi}


\begin{prop}
\begin{itemize}
\item Toda matriz  cuadrada $A$ es semejante a sí misma.
\item Si $A$ es semejante a $B$, entonces $B$ es semejante a $A$.
\item Si $A$ es semejante a $B$ y $B$ es semejante a $C$, entonces $A$ es semejante a $C$.
\end{itemize}
\end{prop}

\begin{obs}
Si $V=W$, y $B_{1}$, $B_{2}$ son dos bases de $V$, vemos que 
$$ [T]_{B_{1}}^{B_{1}}=\left(P_{B_{1}}^{B_{2}}\right)^{-1} [T]_{B_{2}}^{B_{2}} P_{B_{1}}^{B_{2}}.$$
Es decir, $[T]_{B_{1}}^{B_{1}}$ y $[T]_{B_{2}}^{B_{2}}$ son semejantes.
\end{obs}

\begin{prop}
$A$ y $B$ son semejantes si y solo si representan la misma transformación lineal.
\end{prop}
{\bf Demostración}
Si $A$ y $B$ representan la misma aplicación lineal, ya sabemos, de la observación anterior, que son semejantes.

Supongamos ahora que $A$ y $B$ son semejantes, y sea $M$ la matriz que cumple $A=M^{-1}BM$.
Definimos $T:\K^n\rightarrow \K^n$ por $T(x)=Bx$, es claro que $B$ es la matriz representante de $T$ respecto a la base canónica $C$. Tomamos ahora la base $B_2$ consistente en los vectores columna de $M$.

Se tendrá que $P_{B_2}^C=M$, y que $P_C^{B_2}=M^{-1}$, por lo tanto 

$$A=M^{-1}BM=\left(P_{B_{2}}^{C}\right)^{-1} [T]_{C}^{C} P_{B_{2}}^{C}=[T]_{B_{2}}^{B_{2}}$$

Es decir, $A$ y $B$ representan a $T$.
\hfill $\Box$


\begin{defi}
Dada una matriz $A\in\mathcal{M}_{m\times n}(\K)$, se define:
\begin{itemize}
\item $n(A)=dim(\{x\in\K^n\ |\ Ax=0\})$, la nulidad de $A$.
\item $R(A)=dim(\{Ax\ |\ x\in\K^n\})$, el rango de $A$.
\end{itemize}
\end{defi}

\begin{prop}
Dada $T:V\rightarrow W$ lineal, $B_V$ base de $V$ y $B_W$ base de $W$, se cumple:
\begin{itemize}
\item $n([T]_{B_V}^{B_W})=n(T)$, y
\item $R([T]_{B_V}^{B_W})=r(T)$.
\end{itemize}
Lo cual implica que matrices que representan la misma transformación lineal tienen igual nulidad y rango.
\end{prop}

\begin{prop}
Si $A$ y $B$ son semejantes, entonces:
\begin{itemize}
\item $tr(A)=tr(B)$,
\item $det(A)=det(B)$,
\item $R(A)=R(B)$,
\item $\eta(A)=\eta(B)$.
\end{itemize}
\end{prop}



\section{Vectores y Valores Propios}

Una función lineal $f:\mathbb{R}\rightarrow\mathbb{R}$ se puede escribir $f(x)=mx$, con $m$ su pendiente. Esta pendiente nos dice si la función es ``rápida'' o ``lenta'', cuánto ``estira'' o ``estrecha'' a sus preimágenes, etc.
El concepto de valor propio generaliza la idea de pendiente a una transformación lineal $T:V\rightarrow V$ o a una matriz cuadrada, a través de una ecuación similar.

En este capítulo trabajamos con transformaciones lineales de un espacio en sí mismo.

\begin{defi}\label{def:vvp}
Dado $V$ un espacio vectorial sobre un cuerpo $\K$ y una aplicación lineal $T:V\rightarrow V$, decimos que $\lambda\in\K$ es un \emph{valor propio} de $T$ si existe $v\not =\Theta$ tal que

$$ T(v)=\lambda v,$$

en tal caso, decimos que $v$ es un \emph{vector propio} de $T$ asociado a $\lambda$.

El conjunto de valores propios de $T$ se llama \emph{espectro} de $T$ y se denota por $\sigma(T)$.

Análogamente,  dada una matriz $A\in\mathcal{M}_{n\times n}(\K)$, decimos que $\lambda\in\K$ es un \emph{valor propio} de $A$ si existe $v\in\K^n$ tal que
$$ Av=\lambda v\quad \wedge\quad v\not =\Theta$$
en tal caso, decimos que $v$ es un \emph{vector propio} de $A$ asociado a $\lambda$. El conjunto de valores propios de $A$ se llama \emph{espectro} de $A$ y se denota por $\sigma(A)$.

\end{defi}

\begin{ejem} {\em Si $A=\left(\begin{matrix}1 & 2\\ 3 & 6 \end{matrix}\right)$, entonces: $\lambda_1=7$ es un valor propio de $A$ con vector propio asociado $u_1=(1,3)$:
$$\left(\begin{matrix}1 & 2\\ 3 & 6 \end{matrix}\right)\left(\begin{matrix}1 \\ 3  \end{matrix}\right)=\left(\begin{matrix}7 \\ 21  \end{matrix}\right)=7\left(\begin{matrix}1 \\ 3  \end{matrix}\right)$$

 y $\lambda_2=0$ es un valor propio de $A$ con el vector propio asociado $u_2=(2,-1)$:
 
$$\left(\begin{matrix}1 & 2\\ 3 & 6 \end{matrix}\right)\left(\begin{matrix}2 \\ -1  \end{matrix}\right)=\left(\begin{matrix}0 \\ 0  \end{matrix}\right)=0\left(\begin{matrix}2 \\ -1  \end{matrix}\right)$$ 
 
 así $0 \in \sigma(A)$ y $7 \in \sigma(A)$}.\end{ejem}
 
 \vspace{0.2 cm}

En otras palabras, un vector propio es un vector al cual la transformación trata con una simple ponderación por escalar.
Por supuesto no todos los vectores serán tratados así en general, por eso cuando ocurre se destaca particularmente.
Como veremos la clase siguiente, estos vectores donde la función es \emph{simple} ayudarán a entenderla mejor y a simplificar algunos cálculos.
Puede visualizar este concepto en el módulo interactivo que se propone en el siguiente enlace: \url{https://cfrd.udec.cl/proyectos/AlgebraLineal/imagen_tr.html}.

\begin{prop}
Dada $T:V\rightarrow V$ lineal, $V$ espacio vectorial sobre el cuerpo $\K$ y $\B$ una base de $V$, las siguientes afirmaciones son equivalentes para todo $\lambda \in\K$:
\begin{enumerate}
\item $\lambda$ es valor propio de $T$
\item $Ker(T-\lambda id)\not= \{\Theta_V\}$
\item $n(T-\lambda id)\not =0$
\item $n([T]_B^B-\lambda Id)\not=0$
\item $det([T]_B^B-\lambda Id)=0$
\end{enumerate}
\end{prop}

{\bf Demoestración} Sea $T:V\rightarrow V$ una transformaci\'on lineal, y $\B=\{v_1,\dots,v_n\}$ una base de $V$.
\begin{itemize}
\item[$1\Rightarrow 2$] Si $\lambda$ es valor propio de $T$, entonces existe $v\in V$ no nulo tal que $T(v)=\lambda v$.
Esto implica que $T(v)-\lambda id(v)=(T-\lambda id)(v)=\Theta_V$, por lo tanto $v\in\Ker(T-\lambda id)$, y como $v\not= \Theta_V$, entonces $\Ker(T-\lambda id)\not=\{\Theta_V\}$.
\item[$2\Rightarrow 3$] Si $\Ker(T-\lambda id)\not=\{\Theta_V\}$, significa que la dimensión de $\Ker(T-\lambda id)$ es 1 o más, por lo tanto $n(T-\lambda id)\not =0$.
\item[$3\Rightarrow 4$] Si $n(T-\lambda id)\not =0$, entonces hay algún vector no nulo $v\in \Ker(T-\lambda id)$, esto implica que $T(v)-\lambda id(v)=\Theta_V$. 
Pasando esta igualdad a la matriz representante de estos operadores lineales obtenemos: $[T]_\B^\B[v]_\B-\lambda Id[v]_\B=\Theta_{\K^n}$.
Por lo tanto $[v]_\B\in \Ker([T]_B^B-\lambda Id)$, y como $[v]_\B\not=\Theta_{\K^n}$, esto significa que $\Ker([T]_B^B-\lambda Id)$ es no trivial, y por lo tanto su dimensión es mayor que 0, es decir, $n([T]_B^B-\lambda Id)\not=0$.
\item[$4\Rightarrow 5$] Si $n([T]_B^B-\lambda Id)\not=0$, tenemos que la matriz $[T]_B^B-\lambda Id$ es singular, no es invertible, por lo tanto $det([T]_B^B-\lambda Id)=0$.
\item[$5\Rightarrow 1$] Si $det([T]_B^B-\lambda Id)=0$, entonces la matriz $[T]_B^B-\lambda Id$ es singular, por lo tanto existe $x=(x_1,\dots,x_n)\K^n\setminus\{\Theta_{\K^n}\}$ tal que $([T]_B^B-\lambda Id)x=\Theta_{\K^n}$.
Tomando $v=\sum_{i=1}^n x_iv_i \in V$, tenemos que $[v]_\B=x$, así: 
$$[T(v)-\lambda v]_\B=[T(v)]_\B-\lambda [id(v)]_\B=[T]_B^B[v]_\B-\lambda [v]_\B=([T]_B^B-\lambda Id)[v]_\B =\Theta_{\K^n}.$$
Por lo tanto, $T(v)-\lambda v=\Theta_V$, entonces $v$ es vector propio de $T$ asociado a $\lambda$ y $\lambda$ es valor propio de $T$.
\end{itemize}
 \hfill $\Box$

\begin{defi}Dada una matriz $A\in\mathcal{M}_{n\times n}(\K)$, se denomina {\em \textbf{polinomio caracter\'istico}} de $A$ a:
$$p_A(\lambda)=det(A-\lambda Id),$$
el cual es un polinomio de grado $n$. 
Adem\'as a $p_A(\lambda)=0$ se le llama {\em \textbf{ecuaci\'on caracter\'istica de $A$.}}

Análogamente, dada una transformación lineal $T:V\rightarrow V$, su polinomio característico es:
$$p_T(\lambda)=det([T]_\B^\B-\lambda Id),$$
para cualquier base $\B$ de $V$, ya que en realidad no depende de ésta, {\bf siempre que se tome la misma a la partida y a la llegada.}
\end{defi}

\vspace{0.3 cm}

\begin{prop}
\begin{itemize}
\item Dada una matriz $A\in\mathcal{M}_{n\times n}(\K)$, $\lambda\in\K$ es valor propio de $A$ si  y s\'olo si $p_A(\lambda)=0$.
\item Dada una transformación lineal $T:V\rightarrow V$, $\lambda\in\K$ es valor propio de $T$ si  y s\'olo si $p_T(\lambda)=0$.
\end{itemize}
\end{prop}

\vspace{0.3 cm}

\begin{ejem}
Sea $A=\left(\begin{array}{ccc} 3 & 0 & 2 \\ 0 & 2 & 1 \\ 1 & 0 &2  \end{array}\right)$.
\begin{enumerate}[a)]
\item Calcule el polinomio caracter\'istico de $A$.
\item Encuentre el espectro de $A$.
\end{enumerate}

{\em 
\textbf{Soluci\'on.}

\begin{enumerate}[a)]
\item El polinomio caracter\'istico de $A$ se define como $p_A(\lambda)=det(A-\lambda \text{Id}_3)$, es decir:
\begin{eqnarray*}
p_A(\lambda)=det(A-\lambda \text{Id}_3)&=& det\left(\left(\begin{array}{ccc} 3 & 0 & 2 \\ 0 & 2 & 1 \\ 1 & 0 &2  \end{array}\right)- \lambda\left(\begin{array}{ccc} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{array}\right)\right)\\
\\
&=&det\left(\left(\begin{array}{ccc} 3-\lambda & 0 & 2  \\ 0 & 2-\lambda & 1 \\ 1 & 0 & 2-\lambda \end{array}\right)\right)\\ 
\\
&=&(3-\lambda)(2-\lambda)(2-\lambda)+(-2(2-\lambda))\\
\\
&=&(2-\lambda)\Big((3-\lambda)(2-\lambda)-2\Big)\\
\\
&=&(2-\lambda)\Big(4-5\lambda+\lambda^2\Big)\\
\\
p_A(\lambda)&=&(2-\lambda)(\lambda-1)(\lambda-4)
\end{eqnarray*}

\item El espectro de $A$ es el conjunto de todos los valores propios de $A$ y se denota por $\sigma(A)$. Por su parte, los valores propios de $A$ son las ra\'ices del polinomio caracter\'istico, esto es, son los $\lambda\in\R$ tales que $p_A(\lambda)=0$, por tanto los valores propios de $A$ son: $\lambda_1=1$, $\lambda_2=2$ y $\lambda_3=4$, as\'i:
$$\sigma(A)=\{1,2,4\}$$
\end{enumerate}}
\end{ejem}

\vspace{0.5 cm}

\begin{defi}
Si $\lambda$ es un valor propio de $T$, se define el \emph{espacio propio asociado a $\lambda$} por
\begin{itemize}
\item $S_\lambda(T)=\{ v\in V\ |\ T(v)=\lambda v\}=Ker(T-\lambda id)$.
\end{itemize}
Lo análogo se define para matrices cuadradas.
\end{defi}

\begin{prop}
Si $\lambda$ es un valor propio de un operador lineal $T:V\rightarrow V$, entonces su espacio propio es un s. e. v de $V$.
\end{prop}

%\begin{defi}
%Dada una matriz $A\in\mathcal{M}_{n\times n}(\K)$. Sea $\lambda\in\K$ un valor propio de $A$, el subespacio vectorial el cual se llama {\em \textbf{subespacio propio de}} $A$ {\em \textbf{asociado a}} $\lambda$ y se define como:
%$$S_{\lambda}(A)=Ker(A-\lambda\text{Id})$$
%\end{defi}

\begin{ejem}
Para $A=\left(\begin{array}{ccc} 3 & 0 & 2 \\ 0 & 2 & 1 \\ 1 & 0 &2  \end{array}\right)$ vista en el ejemplo anterior, calcule el subespacio propio asociado al valor propio $\lambda=1$.\\

\textbf{Soluci\'on.}\\
{\em
\begin{eqnarray*}
S_{\lambda=1}(A)&=&Ker(A-\lambda\text{Id}_3)\\
&=&\left\{ v\in\R^3: (A- \lambda\text{Id}_3)v=\theta_{\R^3}\right\}\\
\\
&=&\left\{ \left(\begin{array}{c} x  \\ y \\ z  \end{array}\right)\in\R^3: \left(\left(\begin{array}{ccc} 3 & 0 & 2 \\ 0 & 2 & 1 \\ 1 & 0 &2  \end{array}\right) - \lambda\left(\begin{array}{ccc} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{array}\right)\right)\left(\begin{array}{c} x  \\ y \\ z  \end{array}\right)=\left(\begin{array}{c} 0 \\ 0 \\ 0  \end{array}\right)\right\}\\
\\
&=&\left\{ \left(\begin{array}{c} x  \\ y \\ z  \end{array}\right)\in\R^3: \left(\left(\begin{array}{ccc} 3 & 0 & 2 \\ 0 & 2 & 1 \\ 1 & 0 &2  \end{array}\right) - 1\left(\begin{array}{ccc} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{array}\right)\right)\left(\begin{array}{c} x  \\ y \\ z  \end{array}\right)=\left(\begin{array}{c} 0 \\ 0 \\ 0  \end{array}\right)\right\}\\
\\
&=&\left\{ \left(\begin{array}{c} x  \\ y \\ z  \end{array}\right)\in\R^3: \left(\begin{array}{ccc} 3-1 & 0 & 2 \\ 0 & 2-1 & 1 \\ 1 & 0 &2-1  \end{array}\right)\left(\begin{array}{c} x  \\ y \\ z  \end{array}\right)=\left(\begin{array}{c} 0 \\ 0 \\ 0  \end{array}\right)\right\}\\
\\
&=&\left\{ \left(\begin{array}{c} x  \\ y \\ z  \end{array}\right)\in\R^3: \left(\begin{array}{ccc} 2 & 0 & 2 \\ 0 & 1 & 1 \\ 1 & 0 & 1  \end{array}\right)\left(\begin{array}{c} x  \\ y \\ z  \end{array}\right)=\left(\begin{array}{c} 0 \\ 0 \\ 0  \end{array}\right)\right\}\\
\end{eqnarray*}
Escalonando la matriz y resolviendo el sistema, se llega a:
\begin{eqnarray*}
S_{\lambda=1}(A)&=&\left\{ \left(\begin{array}{c} x  \\ y \\ z  \end{array}\right)\in\R^3:  y=-z \wedge x=-z\right\}\\
%\\
%&=&\left\{ \left(\begin{array}{c} x  \\ y \\ z \end{array}\right)\in\R^3: z\in\R, y=x=-z\right\}\\
\\
&=&\left\{ \left(\begin{array}{r} -z \\ -z \\ z \end{array}\right)\in\R^3: z\in\R\right\}\\
\\
&=&\left\{ z\left(\begin{array}{r} -1  \\ -1 \\ 1  \end{array}\right)\in\R^3: z\in\R\right\}\\
\\
&=&\left\langle\left\{\left(\begin{array}{r} -1  \\ -1 \\ 1 \end{array}\right)\right\}\right\rangle.
\end{eqnarray*}
}
\end{ejem}

\begin{defi}
Si el polinomio característico de $T$ se factoriza como sigue:

$$p_T(x)=(x-\lambda_1)^{r_1}(x-\lambda_2)^{r_2}..(x-\lambda_k)^{r_k}q(x),$$

donde $q(x)$ es un polinomio sin raíces en $\K$, entonces $r_i$ se llama \emph{multiplicidad algebraica del valor propio $\lambda_i$ de $T$}.

Por otra parte, la multiplicidad geométrica de $\lambda_i$ se define como $g_i=dim(S_{\lambda_i})$.

Conceptos análogos se definen en el caso de matrices cuadradas.
\end{defi}

\begin{ejem}{\em 
Para la matriz $A$, vista en el ejemplo anterior, el polinomio característico era $p_A(\lambda)=(2-\lambda)(\lambda-1)(\lambda-4)$, por lo tanto los 3 valores propios tienen multiplicidad algebraica igual a 1.
Además, dado que $S_1(A)$ tiene  dimensión 1, podemos afirmar que el valor propio 1 tiene multiplicidad geométrica igual a 1.}
\end{ejem}

\begin{ejem} Considere la transformación lineal $F:\P_2(\R)\rightarrow\P_2(\R)$, definida por $F(ax^2+bx+c)=(b+2a)x^2-ax-c$.
Determine espectro, polinomio característico y multiplicidades.
{\em 

Para calcular el polinomio característico, primero que nada debemos tomar una base de $ \P_2(\R)$ y calcular la matriz representante de $F$ respecto a esta.
Tomamos la base canónica para no complicarnos: $\Ccal_{\P_2}=\{1,x,x^2\}$.

\begin{tabular}{c|ccl|l}
$p$&$F(p)$&&descomposición c/r $\Ccal_{\P_2}$&$[F(p)]_{\Ccal_{\P_2}}$\\\hline
$1$ & $-1$ &=&$-1\cdot1+0\cdot x+0\cdot x^2$& $(-1,0,0)$\\
$x$& $x^2$&=&$0\cdot1+0\cdot x+1\cdot x^2$& $(0,0,1)$\\
$x^2$&$2x^2-x$&=&$0\cdot1+(-1)\cdot x+2\cdot x^2$& $(0,-1,2)$\\
\end{tabular}

Así,
$$[F]_{\Ccal_{\P_2}}^{\Ccal_{\P_2}}=\left(\begin{matrix} -1 & 0 & 0 \\ 0 & 0 & -1 \\ 0 & 1 & 2 \end{matrix}\right),$$
por lo tanto su polinomio característico es:
$$p_F(\lambda)=det\left(\left(\begin{matrix} -1 & 0 & 0 \\ 0 & 0 & -1 \\ 0 & 1 & 2 \end{matrix}\right)-\lambda\left(\begin{matrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{matrix}\right)\right)=-(\lambda+1)(\lambda-1)^2$$
luego sus valores propios son $\lambda_1=-1$ y $\lambda_2=1$, con multiplicidad algebraica 1 y 2 respectivamente.

Para calcular las multiplicidades geométricas, calculamos los subespacios propios.

\begin{eqnarray*}
S_{-1}(F)&=&\left\{ ax^2+bx+c\in\P_2(\R) : 
\left(\left(\begin{array}{ccc} -1 & 0 & 0 \\ 0 & 0 & -1 \\ 0 & 1 &2  \end{array}\right) - (-1)\left(\begin{array}{ccc} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{array}\right)\right)\left(\begin{array}{c} c  \\ b \\ a  \end{array}\right)=
\left(\begin{array}{c} 0 \\ 0 \\ 0  \end{array}\right)\right\}\\
&=&\left\{ ax^2+bx+c\in\P_2(\R) : 
\left(\begin{array}{ccc} 0 & 0 & 0 \\ 0 & 1 & -1 \\ 0 & 1 &3  \end{array}\right)\left(\begin{array}{c} c  \\ b \\ a  \end{array}\right)=
\left(\begin{array}{c} 0 \\ 0 \\ 0  \end{array}\right)\right\}\\
&=&\left\{ ax^2+bx+c\in\P_2(\R) : a=b=0\right\}\\
&=&\left\langle\left\{1\right\}\right\rangle
\end{eqnarray*}
\begin{eqnarray*}
S_{1}(F)&=&\left\{ ax^2+bx+c\in\P_2(\R) : 
\left(\left(\begin{array}{ccc} -1 & 0 & 0 \\ 0 & 0 & -1 \\ 0 & 1 &2  \end{array}\right) - (1)\left(\begin{array}{ccc} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{array}\right)\right)\left(\begin{array}{c} c  \\ b \\ a  \end{array}\right)=
\left(\begin{array}{c} 0 \\ 0 \\ 0  \end{array}\right)\right\}\\
&=&\left\{ ax^2+bx+c\in\P_2(\R) : 
\left(\begin{array}{ccc} -2 & 0 & 0 \\ 0 & -1 & -1 \\ 0 & 1 &1  \end{array}\right)\left(\begin{array}{c} c  \\ b \\ a  \end{array}\right)=
\left(\begin{array}{c} 0 \\ 0 \\ 0  \end{array}\right)\right\}\\
&=&\left\{ ax^2+bx+c\in\P_2(\R) : c=0\wedge a=-b\right\}\\
&=&\left\langle\left\{x-x^2\right\}\right\rangle
\end{eqnarray*}
por tanto la multiplicidad geom\'etrica de ambos valores propios es 1.
}
\end{ejem}
En este caso vemos que la multiplicidad geométrica de cada valor propio es menor o igual a su multiplicidad algebraica.
Esto es cierto para cualquier matriz, y lo vamos a estudiar en la clase siguiente.

\begin{ejem}
Considere el operador derivada $D$ en el subespacio de funciones generadas por el seno y el coseno:

$$S=\langle\{\sen,\cos\}\rangle, \textrm{ s.e.v. del espacio de las funciones de $\R$ en $\R$ sobre el cuerpo $\R$.}$$ 

Es claro que $B=\{\sen,\cos\}$ es l.i y por lo tanto es base de $S$ ya que $\sen$ no es ponderación de $\cos$ por escalar.

Calculamos la matriz representante del operador derivada en este espacio.

\begin{tabular}{c|ccl|l}
$f$&$D(f)$&&descomposición c/r $B$&$[D(f)]_{B}$\\\hline
$\sen$ & $\cos$ &=&$0\cdot\sen+1\cdot \cos$& $(0,1)$\\
$\cos$& $-\sen$&=&$-1\cdot\sen+0\cdot \cos$& $(-1,0)$\\
\end{tabular}

Así,
$$[D]_{B}^{B}=\left(\begin{matrix} 0&-1 \\ 1&0 \end{matrix}\right),$$
por lo tanto su polinomio característico es:
$$p_D(\lambda)=det\left(\left(\begin{matrix} 0&-1 \\ 1&0 \end{matrix}\right)-\lambda\left(\begin{matrix} 1 & 0 \\ 0 & 1   \end{matrix}\right)\right)=\lambda^2+1$$
luego sus valores propios son $\lambda_1=i$ y $\lambda_2=-i$, cada uno con multiplicidad algebraica 1.

Sin embargo, como el cuerpo del operador es $\R$, estos valores solo son valores propios de la matriz representante de $D$, pero no son valores propios de $D$, y éste no tiene ningún valor propio, ni vector propio: $\sigma(D)=\phi$ y $\sigma([D]_B^B)=\{i,-i\}$.

$D$ tampoco tendrá vectores propios, pero la matriz sí, veamos.

\begin{eqnarray*}
S_{i}([D]_B^B)&=&\left\{ \left(\begin{array}{c} a\\b  \end{array}\right)\in\R^2 : 
\left(\left(\begin{matrix} 0&-1 \\ 1&0 \end{matrix}\right)-i\left(\begin{matrix} 1 & 0 \\ 0 & 1   \end{matrix}\right)\right) \left(\begin{array}{c} a  \\ b \end{array}\right)=
\left(\begin{array}{c} 0 \\ 0   \end{array}\right)\right\}\\
&=&\left\{ \left(\begin{array}{c} a\\b  \end{array}\right)\in\R^2 : 
\left(\begin{matrix} -i&-1 \\ 1&-i \end{matrix}\right) \left(\begin{array}{c} a  \\ b \end{array}\right)=
\left(\begin{array}{c} 0 \\ 0   \end{array}\right)\right\}\\
&=&\left\{ \left(\begin{array}{c} a\\b  \end{array}\right)\in\R^2 : -ai-b=0 \wedge a-ib=0\right\}\\
&=&\left\{ \left(\begin{array}{c} a\\b  \end{array}\right)\in\R^2 : b=-ai \wedge a-i(-ai)=0\right\}\\
&=&\left\{ \left(\begin{array}{c} a\\b  \end{array}\right)\in\R^2 : b=-ai \wedge 0=0\right\}\\
&=&\left\{ \left(\begin{array}{c} a\\-ai  \end{array}\right)\in\R^2 : a \textrm{ libre  en } \R\right\}\\
&=&\left\langle\left\{\left(\begin{array}{r} 1\\-i  \end{array}\right)\right\}\right\rangle
\end{eqnarray*}

Es un subespacio de $\C^2$.

\begin{eqnarray*}
S_{-i}([D]_B^B)&=&\left\{ \left(\begin{array}{c} a\\b  \end{array}\right)\in\R^2 : 
\left(\left(\begin{matrix} 0&-1 \\ 1&0 \end{matrix}\right)+i\left(\begin{matrix} 1 & 0 \\ 0 & 1   \end{matrix}\right)\right) \left(\begin{array}{c} a  \\ b \end{array}\right)=
\left(\begin{array}{c} 0 \\ 0   \end{array}\right)\right\}\\
&=&\left\{ \left(\begin{array}{c} a\\b  \end{array}\right)\in\R^2 : 
\left(\begin{matrix} i&-1 \\ 1&i \end{matrix}\right) \left(\begin{array}{c} a  \\ b \end{array}\right)=
\left(\begin{array}{c} 0 \\ 0   \end{array}\right)\right\}\\
&=&\left\{ \left(\begin{array}{c} a\\b  \end{array}\right)\in\R^2 : ai-b=0 \wedge a+ib=0\right\}\\
&=&\left\{ \left(\begin{array}{c} a\\b  \end{array}\right)\in\R^2 : b=ai \wedge a+i(ai)=0\right\}\\
&=&\left\{ \left(\begin{array}{c} a\\b  \end{array}\right)\in\R^2 : b=ai \wedge 0=0\right\}\\
&=&\left\{ \left(\begin{array}{c} a\\ai  \end{array}\right)\in\R^2 : a \textrm{ libre  en } \R\right\}\\
&=&\left\langle\left\{\left(\begin{array}{r} 1\\i  \end{array}\right)\right\}\right\rangle
\end{eqnarray*}


\end{ejem}

Dijimos que los valores propios servían para conocer propiedades claves de las transformaciones lineales. 
Un primer ejemplo de ello es la siguiente observación:

\begin{obs}\label{teo:invertible}
Dada $T:V\rightarrow V$ lineal, se cumple que
$$ T\textrm{  es invertible }\Leftrightarrow 0\not\in\sigma(T) $$
En efecto, dado que $T$ va del espacio en sí mismo, resulta que es inyectiva si y solo si es sobre yectova, lo cual implica que resulta que $T$ es invertible si y solo si es inyectiva.
Pero habíamos visto que la inyectividad equivale a que $\Ker(T)=\{\Theta\}$, como $T=T-0 id$, esto equivale a que $0$ NO sea valor propio de $T$.
\end{obs}



En los ejemplos que vimos la clase pasada, vimos que la multiplicidad geométrica resultó menor o igual a la multiplicidad algebraica. 
Esto no es una casualidad, de hecho hay un teorema que lo establece para cualquier transformación lineal.

\begin{teo}\label{teo:1gr}
Si $\lambda$ es un valor propio de $T$, y $r$ y $g$ son su multiplicidad algebraica y geométrica, respectivamente, entonces:

$$1\le g\le r.$$
\end{teo}

La demostración lamentablemente excede la dificultad de este curso, pero el resultado es de suma importancia y utilidad en los ejercicios.

\begin{ejem}
Sea $A=\left(\begin{array}{ccc} 3 & 0 & 2 \\ 0 & 2 & 1 \\ 1 & 0 &2  \end{array}\right)$.
Calcule las multiplicidades algebraicas y geométricas de cada uno de los valores propios de $A$.

{\em 
\textbf{Soluci\'on.}
En el apunte anterior calculamos el polinomio característico de $A$:
$$p_A(\lambda)=(2-\lambda)(\lambda-1)(\lambda-4),$$
de donde obtuvimos que el espectro de $A$ es:
$\sigma(A)=\{1,2,4\}$.

Dado que el exponente de cada factor lineal en la factorización de $p_A$ es 1, concluimos que la multiplicidad algebraica de cada uno de sus valores propios es 1.

Gracias al Teorema~\ref{teo:1gr}, podemos concluir que entonces la multiplicidad geométrica de cada uno de los valores propios es también 1.
}
\end{ejem}

No siempre se tiene tanta suerte, si una multiplicidad algebraica resultara igual a 2 o más, entonces la única manera de conocer la multiplicidad geométrica es calcular el espacio propio asociado para obtener si dimensión, tal como hicimos en el último ejemplo de la última clase.

Otra propiedad que es útil para verificar si andamos bien o mal con nuestros cálculos es la siguiente.

\begin{prop}\label{teo:grp}
Sea $T:V\rightarrow V$, donde $V$ es un espacio vectorial sobre $\K$ de dimensión $n$. 
Si $\lambda_1,\lambda_2,\dots,\lambda_k$ son valores propios distintos de $T$, entonces 
$$\sum_{i=1}^k r_i \le n,$$
con igualdad en el caso $\K=\C$.
\end{prop}
\underline{Demostración}: {\small
Suponiendo, en el caso general que
$$p_T(x)=(x-\lambda_1)^{r_1}(x-\lambda_2)^{r_2}..(x-\lambda_k)^{r_k}q(x),$$
tenemos que 
$$n=gr(p_T(x))=\sum_{i=1}^k r_i+gr(q(x)).$$
De donde es claro el resultado ya que $gr(q(x))\ge 0$.
 \hfill $\Box$
}

Verificamos que en los ejemplos vistos se cumple con igualdad, aunque no siempre es así.

\begin{ejem}
Calcule el polinomio característico, valores propios y multiplicidades geométricas y algebraicas del siguiente operador.
$$ L:\M_{2\times 2}(\R)\rightarrow\M_{2\times 2}(\R)$$
$$L\left(\begin{array}{cc} a&b\\c&d\end{array}\right)=\left(\begin{array}{cc} a&d\\b&c\end{array}\right)$$

{\bf Solución.} {\em
Primero calculamos su matriz representante respecto a la base canónica.

\begin{tabular}{c|c|l}
$M$&$L(M)$&$[L(M))]_{\Ccal_{\M_{2\times 2}}}$\\\hline
$\left(\begin{array}{cc} 1&0\\0&0\end{array}\right)$& $\left(\begin{array}{cc} 1&0\\0&0\end{array}\right)$& $(1,0,0,0)$\\
$\left(\begin{array}{cc} 0&1\\0&0\end{array}\right)$& $\left(\begin{array}{cc} 0&0\\1&0\end{array}\right)$& $(0,0,1,0)$\\
$\left(\begin{array}{cc} 0&0\\1&0\end{array}\right)$& $\left(\begin{array}{cc} 0&0\\0&1\end{array}\right)$& $(0,0,0,1)$\\
$\left(\begin{array}{cc} 0&0\\0&1\end{array}\right)$& $\left(\begin{array}{cc} 0&1\\0&0\end{array}\right)$& $(0,1,0,0)$
\end{tabular}
\quad
$[L]_{\Ccal_{\M_{2\times 2}}}^{\Ccal_{\M_{2\times 2}}}=\left(\begin{array}{cccc} 1&0&0&0\\0&0&0&1\\0&1&0&0\\0&0&1&0\end{array}\right)$

Ahora podemos calcular el polinomio característico.

\begin{eqnarray*}
p_L(x)&=&det\left(\begin{array}{cccc} 1-x&0&0&0\\0&-x&0&1\\0&1&-x&0\\0&0&1&-x\end{array}\right)\\
&=&(-1)^{1+1}(1-x) det\left(\begin{array}{ccc} -x&0&1\\1&-x&0\\0&1&-x\end{array}\right)\\
&=&(1-x)\left[(-1)^{1+1}(-x) det\left(\begin{array}{ccc} -x&0\\1&-x\end{array}\right)+(-1)^{2+1}1det\left(\begin{array}{ccc} 0&1\\1&-x\end{array}\right)\right]\\
&=&(1-x)[-x(x^2-0)-(0-1)]\\
&=&(1-x)(1-x^3)\
\end{eqnarray*}
el polinomio aun no está totalmente factorizado ya que sabemos que 1 es raíz de $1-x^3$, puesto que al evaluar en $x=1$ da 0, pero ¿cuáles serán los otros factores? Para encontrarlos realizamos división de polinomios.
$$
\begin{array}{rrrrrrrrrrr}
-x^3&+&&&&&1&:&-x+1&=&x^2+x+1\\
-(-x^3&+&x^2)&&&&&\\\cline{1-3}
&&-x^2&+&&&1\\
&&-(-x^2&+&x)&&\\\cline{3-5}
&&&&-x&+&1\\
&&&&-(-x&+&1)\\\cline{5-7}
&&&&&&0
\end{array}
$$
El polinomio $x^2+x+1$ no tiene raíces reales, de hecho su discriminante es: $b^2-4ac=1-4=-3<0$.
Por lo tanto la factorización total de $p_L$ es:
$$p_L(x)=(x-1)^2(x^2+x+1).$$
Así, el espectro de $L$ es $\sigma(L)=\{1\}$, con su único valor propio $\lambda=1$ de multiplicidad 2.
Entonces, la suma de las multiplicidades algebraicas de sus valores propios es $2\le4$.
}
\end{ejem}



La siguiente propiedad servirá de cimiento para el tema de la clase de hoy.
Dijimos que la teoría desarrollada permitía entender mejor la transformación ya que la transformación se comporta de manera simple sobre sus subespacios propios.
Conviene entonces mirar la transformación en el subespacio generado por todos sus vectores propios.
Esto no siempre se puede hacer, pero al menos algunas cosas podemos hacer.
Lo primero es observar que la unión de las bases de los subespacios propios es l.i., tal como dice el siguiente teorema.

\begin{prop}\label{teo:vpli}
Sea $T:V\rightarrow V$.
Si $v_1,..,v_k$ son vectores propios asociados a valores propios distintos $\lambda_1,\lambda_2,\dots,\lambda_k$, entonces $\{v_1,\dots,v_k\}$ es l. i.
\end{prop}

\underline{Demostración}: {\small
Haremos una demostración por contradicción.
Es una demostración complicada del punto de vista lógico, usted podrá seguirla sólo si toma una actitud muy crítica hacia ella.
Es una demostración interesante, aunque no es esencial para la comprensión de esta unidad.
Espero que la disfrute. 

Primero suponemos que existe un operador $T:V\rightarrow V$ que no cumple la proposición, es decir, que posee un conjunto de valores propios $\lambda_1,\lambda_2,\dots,\lambda_k$, y un conjunto de vectores propios asociados a éstos: $\{v_1,..,v_k\}$, que no es l. i.
Suponemos sin pérdida de generalidad que $\lambda_1$ es no nulo, pues si $k\ge 2$ ha de haber un valor no nulo, y si $k=1$, el resultado es cierto pues todo conjunto de un solo vector no nulo es l. i., y los vectores propios son siempre no nulos.

Vamos a suponer también que este conjunto es el más pequeño posible que cumple tal propiedad, es decir, vamos a suponer que cualquier subconjunto de éste sí es l. i., siempre podemos suponer esto pues si tuviera un subconjunto l.d., podríamos tomar aquel en ves de \'este.
\footnote{Este tipo de razonamiento es una forma escondida de inducción matemática.}

Como $\{v_1,..,v_k\}$ no es l. i., existen escalares $\alpha_1,\dots,\alpha_k$ tales que 

\begin{equation}\label{ec:li}
\sum_{i=1}^k \alpha_i v_i=\Theta.
\end{equation}

Ninguno de estos escalares es nulo, ya que si hubiera uno nulo, el conjunto de los vectores restantes seguiría siendo l. d. y sería un conjunto más pequeño, lo cual contradice nuestro supuesto.

Aplicamos ahora $T$ a ambos lados en la ecuación~\ref{ec:li}, y obtenemos:
\begin{eqnarray*}
T\left(\sum_{i=1}^k \alpha_iv_i\right)&=&\Theta\\
\sum_{i=1}^k \alpha_iT(v_i)&=&\Theta\\
\sum_{i=1}^k \alpha_i\lambda_iv_i&=&\Theta\quad /\frac{1}{\lambda_1}
\end{eqnarray*}
\begin{eqnarray}
\alpha_1v_1+\sum_{i=2}^k \alpha_i\frac{\lambda_i}{\lambda_1}v_i&=&\Theta.\label{ec:vp}
\end{eqnarray}
Ahora miramos (\ref{ec:li})$-$(\ref{ec:vp}):
$$\sum_{i=2}^k \alpha_i(1-\frac{\lambda_i}{\lambda_1})v_i=\Theta$$

Como $\lambda_i\not=\lambda_1$ para todo $i\ge2$, tenemos $\alpha_i(1-\frac{\lambda_i}{\lambda_1})\not = 0$, para todo $i\ge 2$, y por lo tanto el conjunto $\{v_2,\dots,v_k\}$ es l. d., lo cual contradice nuestro supuesto, y evidencia una contradicción.
\hfill $\Box$
}

vspace{0.3 cm}

\begin{ejem} {\em Se vi\'o que la matriz
$$C=\left(\begin{matrix} -1 & 0 & 0 \\ 0 & 0 & -1 \\ 0 & 1 & 2 \end{matrix}\right)$$

tiene espectro $\sigma(C)=\{-1,1\}$ y subespacios:\\

\hspace{4 cm}$S_{-1}=\left\langle\left\{\left(\begin{matrix} 1 \\ 0  \\ 0 \end{matrix}\right)\right\}\right\rangle$\hspace{1 cm}y\hspace{1 cm}$S_1=\left\langle\left\{\left(\begin{matrix} 0 \\ 1  \\ -1 \end{matrix}\right)\right\}\right\rangle$\\

La observación~\ref{teo:invertible} dice que $C$ es invertible y la propiedad~\ref{teo:vpli} nos asegura que el conjunto $\{(1,0,0),(0,1,-1)\}$ es l.i.
}
\end{ejem}

En general la unión de las bases de los distintos subespacios de un operador será siempre l.i., propiedad que vamos a explotar fuertemente en lo que sigue.



\subsection{Diagonalización}
    



El caso más interesante ocurre cuando la unión de las bases de los subespacios propios es una base del espacio completo, pues entonces podremos reescribir la transformación de manera mucho más simple, gracias al concepto de ``diagonalización".

\begin{defi}
Una transformación lineal $T:V\rightarrow V$ se dice diagonalizable si existe una base $B$ de $V$ tal que $[T]_B^B$ es diagonal.
\end{defi}

Si $[T]_B^B$ es diagonal, será mucho más fácil operar con ella, elevarla a potencia e invertible cuando se pueda, será muy simple, bastará con elevar a potencia los valores de la diagonal o invertir los valores de la diagonal. 
De aquí el interés en encontrar la base que diagonaliza a una transformación lineal.
Pero no siempre existe, y el siguiente teorema esclarece este asunto.


\begin{teo}
$T$ es diagonalizable si y solo si existe una base de $V$ formada solo por vectores propios de $T$.
\end{teo}
\underline{Demostraci\'on}: {
\begin{itemize}
\item[($\Leftarrow$)] Sea $\B=\{v_1,\dots,v_n\}$ una base de vectores propios de $T$ asociados a valores propios $\lambda_1,\dots,\lambda_n$, respectivamente (no necesariamente distintos).
Esto es, $T(v_i)=\lambda_iv_i$ para todo $i$.
Demostremos que entonces $[T]_\B^\B$ es diagonal.

En efecto,  $[T(v_i)]_\B=(0,\dots,0,\lambda_i,0,\dots,0)$, con el $\lambda_i$ en la $i$-esima coordenada, ya que solo el vector $v_i$ se usa para describir $v_i$ en térmnos de la base $\B$.
Pero esto significa que su $i$-ésima columna tendrá solo la $i$-esima fcoordenada no nula, justo en la diagonal.
Por lo tanto la matriz $[T]_\B^\B$ será diagonal.
\item[($\Rightarrow$)] En el sentido inverso, supongamos que existe una base $\Ccal=\{u_1,\dots,u_n\}$ tal que $[T]_\Ccal^\Ccal=D$, con $D$ una matriz diagonal.
Llamemos $\{d_{ii}\}_{i=1}^n$ a los coeficientes de la matriz diagonal.

Mostremos que $\{u_1,\dots,u_n\}$ son vectores propios de $T$.
Para ello observamos primero que $[u_i]_\Ccal=e_i$, el $i$-esimo vector de la base canónica de $\K^n$, ya que $u_i$ es el $i$-ésimo vector de la base $\Ccal$, y $u_i=1u_i$.

Ahora usaremos el TFAL:
$$[T(u_i)]_\Ccal=[T]_\Ccal^\Ccal[u_i]=De_i=\textrm{ columna $i$-ésima de }D.$$
Como $D$ es diagonal, solo el $i$-ésimo valor de la columna $i$-ésima de $D$ es no nulo, y como dijimos vale $d_{ii}$, por lo tanto $T(u_i)=\sum_{j=1}^n d_{ij}u_j=d_{ii}u_i$.
Lo cual prueba que $u_i$ es un vector propio asociado a $d_{ii}$, por lo tanto $\Ccal=\{u_1,\dots,u_n\}$ es una base de $V$ formada solo por vectores propios de $T$ y demuestra el teorema.\hfill $\Box$
\end{itemize}
\begin{ejem} {\em 
La matriz $A=\left(\begin{matrix}1 & 2\\ 3 & 6 \end{matrix}\right)$ tiene espectro $\sigma(A)=\{0,7\}$, con subespacios propios:\\

\hspace{6 cm}$S_{0}=\langle\{(2,-1)\}\rangle$ y $S_{7}=\langle\{(1,3)\}\rangle$\\

El conjunto $\{(1,3),(2,-1)\}$ es l. i. y, al tener cardinalidad 2, resulta ser una base de $\R^2$.\\

Luego el teorema 2, dice que $A$ es diagonalizable con $P=\left(\begin{matrix}2 & 1\\ -1 & 3 \end{matrix}\right)$ y $D=\left(\begin{matrix}0 & 0\\ 0 & 7 \end{matrix}\right)$, en efecto:\\

\hspace{4 cm}$A=PDP^{-1}$\\

\hspace{4 cm}$A=\left(\begin{matrix}2 & 1\\ -1 & 3 \end{matrix}\right)\left(\begin{matrix}0 & 0\\ 0 & 7 
\end{matrix}\right)\left(\begin{matrix}3/7 & -1/7\\ 1/7 & 2/7 \end{matrix}\right)$\\

\hspace{4 cm}$A=\left(\begin{matrix}0 & 7\\ 0 & 21 \end{matrix}\right)\left(\begin{matrix}3/7 & -1/7\\ 1/7 & 2/7 \end{matrix}\right)$\\

\hspace{4 cm}$A=\left(\begin{matrix}1 & 2\\ 3 & 6 \end{matrix}\right)$\\

Entonces calcular las potencias de $A$ es simple:

\hspace{4 cm}$A^n=\left(\begin{matrix}2 & 1\\ -1 & 3 \end{matrix}\right)\left(\begin{matrix}0 & 0\\ 0 & 7^n 
\end{matrix}\right)\left(\begin{matrix}3/7 & -1/7\\ 1/7 & 2/7 \end{matrix}\right)$\\

\hspace{4 cm}$A^n=\left(\begin{matrix}0 & 7^n\\ 0 & 3\cdot7^n \end{matrix}\right)\left(\begin{matrix}3/7 & -1/7\\ 1/7 & 2/7 \end{matrix}\right)$\\

\hspace{4 cm}$A^n=\left(\begin{matrix}7^{n-1} & 2\cdot7^{n-1}\\ 3\cdot7^{n-1} & 6\cdot7^{n-1} \end{matrix}\right)=7^{n-1}A$\\

Fórmula que de otra manera sería imposible obtener.
}
\end{ejem}

\vspace{0.3 cm}

La demostración del teorema anterior es útil cuando se quiere diagonalizar una matriz, es decir cuando queremos conocer explícitamente la matriz $P$ que la diagonaliza. 
Ahora bien, si solo se desea saber si es o no diagonalizable, basta con calcular las multiplicidades geométricas de cada valor propio, como muestra el siguiente resultado.

\begin{teo}\label{teo:sumg}
Si $g_1,..,g_k$ son las multiplicidades geométricas de los valores propios de $T$, entonces
$T$ es diagonalizable si y solo si $\displaystyle{ \sum_{i=1}^k g_i}=dim(V)$.
\end{teo}
 \underline{Demostración}: {
De la propiedad~\ref{teo:vpli} sabemos que la unión de las bases de los subespacios propios es l.i.
La base del subespacio propio asociado al valor propio $\lambda_i$ tiene $g_i$ vectores, pues la dimensión de este espacio es $g_i$, por lo tanto la unión de las bases de todos los subespacios propios tiene ${\displaystyle \sum_{i=1}^k g_i}$, por lo tanto esta es base de $V$ si y solo si su dimensión es igual a la dimensión de $V$.
 \hfill $\Box$
}

\begin{teo}\label{teo:g=r}
$T$ es diagonalizable si y solo si $p_T(x)$ se factoriza en factores lineales y para cada valor propio $\lambda$ de $T$ se cumple $g_\lambda=r_\lambda$.
\end{teo}
 \underline{Demostración}: {
 De la demostración de la propiedad~\ref{teo:grp}, vemos que si $p_T(x)$ se factoriza en factores lineales, entonces el polinomio $q(x)$ de la demostración no existe (es 1), y por lo tanto se cumple:
  $\sum_{i=1}^k r_i=n.$
Así, si para todo valor propio $\lambda_i$ se cumple $g_i=r_i$, entonces 
  $\sum_{i=1}^k g_i=n,$
  de donde el teorema~\ref{teo:sumg} nos permite concluir que $T$ es diagonalizable.
  
  Recíprocamente, si $T$ es diagonalizable, entonces $\sum_{i=1}^k g_i=n,$ y como $g_i\le r_i$, la única manera que esto ocurra es que $\sum_{i=1}^k r_i=n.$
   \hfill $\Box$
}

\begin{ejem} {\em Determine si la siguiente matriz es diagonalizable o no:

$$M=\left(\begin{matrix}1 & 0 & 0 & 0 & 0 \\ 2 & -1 & 2 & 0 & 0 \\ -1 & 0 & 1 & 0 & 0 \\ 0 & 0 & 0 & 3 & 2 \\ 0 & 0 & 0 & -4 & -3 \end{matrix}\right).$$

\underline{Soluci\'on}:
Su polinomio característico es:
$$p_M(\lambda)=det(M-\lambda \text{Id})=(1-\lambda)(-1-\lambda)(1-\lambda)[(3-\lambda)(-3-\lambda)+8]$$
$$=-(1-\lambda)^2(1+\lambda)(\lambda^2-9+8)=-(\lambda-1)^3(\lambda+1)^2$$
Los valores propios serán $\lambda_1=-1$ y $\lambda_2=1$, con multiplicidades algebraicas iguales a 2 y 3 respectivamente. Habrá dos subespacios propios:
\begin{align*}
 S_{\lambda_1=-1 }  &  =\text{Ker }(M-(-1)\text{Id})\\
         &  =\left\{\left(\begin{matrix}x\\ y \\ z \\ s \\ t \end{matrix}\right)\in\R^5:
         \left(\left(\begin{matrix}
         1 & 0 & 0 & 0 & 0 \\ 
         2 & -1 & 2 & 0 & 0 \\ 
         -1 & 0 & 1 & 0 & 0 \\ 
         0 & 0 & 0 & 3 & 2 \\ 
         0 & 0 & 0 & -4 & -3 
         \end{matrix}\right)-(-1)
         \left(\begin{matrix}
         1 & 0 & 0 & 0 & 0 \\ 
         0 & 1 & 0 & 0 & 0 \\ 
         0 & 0 & 1 & 0 & 0 \\ 
         0 & 0 & 0 & 1 & 0 \\ 
         0 & 0 & 0 & 0 & 1 
         \end{matrix}\right)\right)\left(\begin{matrix}x\\ y \\ z \\ s \\ t \end{matrix}\right)=\left(\begin{matrix}0\\ 0 \\ 0 \\ 0 \\ 0 \end{matrix}\right)\right\} \\
   &  =\left\{\left(\begin{matrix}x\\ y \\ z \\ s \\ t \end{matrix}\right)\in\R^5:
   \left(\begin{matrix}
   2 & 0 & 0 & 0 & 0 \\ 
   2 & 0 & 2 & 0 & 0 \\ 
   -1 & 0 & 2 & 0 & 0 \\ 
   0 & 0 & 0 & 4 & 2 \\ 
   0 & 0 & 0 & -4 & -2 
   \end{matrix}\right)\left(\begin{matrix}x\\ y \\ z \\ s \\ t \end{matrix}\right)=\left(\begin{matrix}0\\ 0 \\ 0 \\ 0 \\ 0 \end{matrix}\right)\right\} \\
   & =\left\{\left(\begin{matrix}x\\ y \\ z \\ s \\ t \end{matrix}\right)\in\R^5: 2x=0\wedge 2x+2z=0\wedge -x+2z=0\wedge y\in\R \wedge4s+2t=0\right\} \\
  & =\left\{\left(\begin{matrix}x\\ y \\ z \\ s \\ t \end{matrix}\right)\in\R^5: x=z=0, y\in\R, t=-2s\right\} \\
  & =\left\langle\left\{\left(\begin{matrix}0\\ 1 \\ 0 \\ 0 \\ 0 \end{matrix}\right),\left(\begin{matrix}0\\ 0 \\ 0 \\ 1 \\ -2 \end{matrix}\right)\right\}\right\rangle.\\
\end{align*}
el cual es de dimensión 2, por tanto $g_1=2$. Por otro lado:

\begin{align*}
 S_{\lambda_2=1 }  &  =\text{Ker }(M-(1)\text{Id})\\
         &  =\left\{\left(\begin{matrix}x\\ y \\ z \\ s \\ t \end{matrix}\right)\in\R^5:\left(
         \left(\begin{matrix}
         1 & 0 & 0 & 0 & 0 \\ 
         2 & -1 & 2 & 0 & 0 \\ 
         -1 & 0 & 1 & 0 & 0 \\ 
         0 & 0 & 0 & 3 & 2 \\ 
         0 & 0 & 0 & -4 & -3 
         \end{matrix}\right)-(1)\left(\begin{matrix}1 & 0 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 & 0 \\ 0 & 0 & 1 & 0 & 0 \\ 0 & 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 0 & 1 \end{matrix}\right)\right)\left(\begin{matrix}x\\ y \\ z \\ s \\ t \end{matrix}\right)=\left(\begin{matrix}0\\ 0 \\ 0 \\ 0 \\ 0 \end{matrix}\right)\right\} \\
        &  =\left\{\left(\begin{matrix}x\\ y \\ z \\ s \\ t \end{matrix}\right)\in\R^5:
         \left(\begin{matrix}
         0 & 0 & 0 & 0 & 0 \\ 
         2 & -2 & 2 & 0 & 0 \\ 
         -1 & 0 & 0 & 0 & 0 \\ 
         0 & 0 & 0 & 2 & 2 \\ 
         0 & 0 & 0 & -4 & -4 
         \end{matrix}\right)\left(\begin{matrix}x\\ y \\ z \\ s \\ t \end{matrix}\right)=\left(\begin{matrix}0\\ 0 \\ 0 \\ 0 \\ 0 \end{matrix}\right)\right\} \\
   & =\left\{\left(\begin{matrix}x\\ y \\ z \\ s \\ t \end{matrix}\right)\in\R^5: 2x-2y+2z=0\wedge -x=0\wedge 2s+2t=0\right\} \\
  & =\left\{\left(\begin{matrix}x\\ y \\ z \\ s \\ t \end{matrix}\right)\in\R^5: x=0\wedge y=z\wedge t=-s\right\} \\
  & =\left\langle\left\{\left(\begin{matrix}0\\ 1 \\ 1 \\ 0 \\ 0 \end{matrix}\right),\left(\begin{matrix}0\\ 0 \\ 0 \\ 1 \\ -1 \end{matrix}\right)\right\}\right\rangle.\\
\end{align*}
y por tanto $g_2=2$. Luego por teorema~\ref{teo:sumg}, la matriz no es diagonalizable pues $g_1+g_2=4$ y $n=5$.
También el teorema~\ref{teo:g=r} nos lleva a la misma conclusión ya que $g_2=2$ mientras $r_2=3$.
}
\end{ejem}

\chapter{Espacios Vectoriales con Producto Interior}


En el presente capítulo vamos a generalizar una aspecto de la \emph{geometría euclidiana} a espacios vectoriales abstractos.

La geometría que ustedes conocen hasta ahora trabaja con rectas, distancias y ángulos en el plano. Acá vamos a exportar esos conceptos a dimensiones superiores. Pero primero recordemos algo de trigonometría.

\begin{center}
\begin{tikzpicture}[scale=1]
 	\draw[-] (-1,0) -- (4,0) node[below] {x};
      	\draw[-] (0,-1) -- (0,3) node[left] {y};
	\coordinate[label=below:$u_x$] (UX) at (1,0);
	\coordinate[label=below:$v_x$] (VX) at (1.5,0);
	\coordinate[label=left:$u_y$] (UY) at (0,2.5);
	\coordinate[label=left:$v_y$] (VY) at (0,.5);
	\coordinate[label=right:$u$] (U) at (1,2.5);
	\coordinate[label=right:$v$] (V) at (1.5,.5);
	\draw[fill=black] (U) circle(0.03);
	\draw[fill=black] (V) circle(0.03);
	\draw[-] (0,0) -- (U);
	\draw[dashed] (UX) -- (U);
	\draw[dashed] (UY) -- (U);
	\draw[dashed] (VX) -- (V);
	\draw[dashed] (VY) -- (V);
	\draw[-] (0,0) -- (V);
	\coordinate (L) at (4.5,1.5);
	\coordinate (M) at (-.9,-.3);
	\draw[dashed] (L) -- (M);
\end{tikzpicture} 
\end{center}
Del teorema de Pitágoras, se tiene que la magnitud de un vector $u=(u_x,u_y)$ está dada por $||u||=\sqrt{u_x^2+u_y^2}$.
Entonces, podemos valernos del Teorema del Coseno para calcular el ángulo $\angle^u_v$:
$$||u-v||^2=||u||^2+||v||^2-2||u|| ||v|| cos(\angle^u_v)$$

De éste obtenemos que:

\begin{eqnarray*}
  cos(\angle^u_v)&=&\frac{u_x^2+u_y^2+v_x^2+v_y^2-(u_x-v_x)^2-(u_y-v_y)^2}{2||u|| ||v|| }\\
  &=&\frac{2u_xv_x+2u_yv_y}{2||u|| ||v|| }\\
  &=&\frac{u_xv_x+u_yv_y}{||u|| ||v|| }
\end{eqnarray*}


Entonces el ángulo se puede calcular directamente a partir de las coordenadas de los vectores $u$ y $v$.
Además, observamos que en el numerador aparece una expresión interesante que puede escribirse como multiplicación de matrices:

  $$u_xv_x+u_yv_y=\left(\begin{array}{rr}u_x&u_y\end{array}\right)\left(\begin{array}{r}v_x\\ v_y\end{array}\right)=\left(\begin{array}{r}u_x\\u_y\end{array}\right)^\top\left(\begin{array}{r}v_x\\ v_y\end{array}\right)$$
  
  \vspace{0.2 cm}

Este producto es el tema del presente capítulo. Buscamos definir el producto interior en espacios vectoriales abstractos, y de dimensiones arbitrarias. Como buscamos total generalidad y total libertad, y como lo que nos gusta del producto interno son sus ``cualidades'', no su forma precisa, entonces lo definiremos a través de las propiedades que nos interesa que cumpla.\\

Las propiedades que escogemos para definirlo son el resultado de décadas de investigación matemática, que logra sintetizar lo deseado de manera óptima. Al igual que en el caso de los espacios vectoriales, la definición que presentaremos asegura que todo lo demás se pueda obtener ``gratis''. Este enfoque abstracto tiene la ventaja de otorgarnos la libertad de definir varios productos internos para un mismo espacio según nuestra conveniencia, lo cual puede ser pertinente en algunas aplicaciones \emph{no} geométricas.\\

Este capítulo se restringe siempre a los cuerpos $\R$ y $\C$, dado que el concepto de producto interno pierde sentido en otros cuerpos.


\begin{defi}\textbf{{\em (Producto Interior)}}:
Dado un espacio vectorial sobre un cuerpo $\K$, con $\K=\R$ o $\C$, decimos que la operación $\langle \phantom{x};\phantom{x} \rangle:V\times V\rightarrow \K$ es un \emph{producto interno (p. i.)} si satisface las siguientes propiedades.
\begin{enumerate}
\item $(\forall u,v,w\in V)(\forall \alpha\in \K)$
\begin{enumerate}
\item $\langle \alpha u ; v\rangle=\alpha \langle u; v\rangle$
\item $\langle u+v ; w\rangle= \langle u; w\rangle+\langle v; w\rangle$
\end{enumerate}
\item\label{sim} $(\forall u,v\in V)\ \langle u ;v \rangle=\overline{\langle v ;u \rangle}$
\item\label{ge} $(\forall u\in V)\ \langle u;u\rangle \ge 0$
\item $(\forall u\in V)\ [\langle u;u\rangle = 0 \Leftrightarrow u=\Theta]$
\end{enumerate}
\end{defi}

Observamos que la propiedad \ref{sim} asegura que $\langle u; u\rangle\in \R$, lo que da sentido a la propiedad \ref{ge}.
También observamos que si $\K=\R$, entonces el \emph{conjugado} queda sin efecto en la propiedad \ref{sim}, y en ese caso la operación es simétrica.

\begin{ejem}
Considere $\R^3$, la aplicaci\'on $\langle \cdot; \cdot \rangle:\R^3\times \R^3\rightarrow\;\R$ definida como:
   $$\langle x;y\rangle =x_1y_1+x_2y_2+x_3y_3$$
donde $x=(x_1,x_2,x_3)$ e $y=(y_1,y_2,y_3)$, es producto interior sobre $\R^3$.

\underline{Demostraci\'on:}\\
{\em 
Sea $\alpha\in\R$ y sean $v=(v_1,v_2,v_3)$, $u=(u_1,u_2,u_3)$ y $w=(w_1,w_2,w_3)$ $\in$ $\R^3$.\\

1.a) $\langle \alpha u; w \rangle=\langle (\alpha u_1,\alpha u_2,\alpha u_3);(w_1,w_2,w_3)\rangle=(\alpha u_1)w_1+(\alpha u_2)w_2+(\alpha u_3)w_3$\\

\hspace{8.1 cm}$=\alpha\Big( u_1w_1+ u_2w_2+ u_3w_3\Big)$\\

\hspace{8.1 cm}$=\alpha\langle u; w \rangle$\\

1.b) $\langle u+v; w \rangle=\langle (u_1+v_1,u_2+v_2,u_3+v_3);(w_1,w_2,w_3)\rangle$\\

\hspace{2.7 cm}$=(u_1+v_1)w_1+(u_2+v_2)w_2+(u_3+v_3)w_3$\\

\hspace{2.7 cm}$=(u_1w_1+u_2w_2+u_3w_3)+(v_1w_1+v_2w_2+v_3w_3)$\\

\hspace{2.7 cm}$=\langle u; w \rangle+\langle v; w \rangle$\\

2)  $\langle u; v \rangle=\langle (u_1,u_2,u_3);(v_1,v_2,v_3)\rangle$\\

\hspace{1.6 cm}$=u_1v_1+u_2v_2+u_3v_3$\\

\hspace{1.6 cm}$=v_1u_1+v_2u_2+v_3u_3$\\

\hspace{1.6 cm}$=\langle v; u \rangle$\\

3) $\langle u; u \rangle=\langle (u_1,u_2,u_3);(u_1,u_2,u_3)\rangle$\\

\hspace{1.6 cm}$=u_1u_1+u_2u_2+u_3u_3$\\

\hspace{1.6 cm}$=u_1^2+u_2^2+u_3^2\geq 0$\\

4) $\langle u; u \rangle=0\;\Rightarrow\;u_1^2+u_2^2+u_3^2=0\;\Rightarrow\;u_1=u_2=u_3=0\;\Rightarrow\;u=(0,0,0)$\\

Por otro lado, $u=(0,0,0)\;\Rightarrow\;\langle u;u\rangle =u_1^2+u_2^2+u_3^2=0$.\\

Por lo tanto, la aplicaci\'on definida es un producto interior en $\R^3$.


}
\end{ejem}

\begin{obs}
Recordemos algunas propiedades de los n\'umeros complejos que necesitaremos en este capítulo.

Para todo $x,y\in\C$, y $a,b\in \R$ se cumple lo siguiente.\\

\begin{multicols}{3}
\begin{enumerate}
\item $\overline{a+bi}=a-bi$
\item $\overline{xy}=\overline{x}\overline{y}$
\item $\overline{x+y}=\overline{x}+\overline{y}$
\item $\overline{\overline{x}}=x$
\item $|x|=\sqrt{\overline{x}x}$
\item $x^{-1}=\frac{\overline{x}}{|x|^2}$
\item $x=\overline{x}\Leftrightarrow x\in \R$
\item $x+\overline{x}=2Re(x)$
\item $x-\overline{x}=2i\,Im(x)$
\end{enumerate}
\end{multicols}
\end{obs}


\begin{ejem}
Considere $\R^3$, la aplicaci\'on $\langle \cdot; \cdot \rangle:\C^3\times \C^3\rightarrow\;\C$ definida como:
 
   $$\langle x;y\rangle =x_1y_1+x_2y_2+x_3y_3$$
   
donde $x=(x_1,x_2,x_3)$ e $y=(y_1,y_2,y_3)\in\C^3$.

{\em
Lo único que aquí cambia es que ahora estamos en el cuerpo de los números complejos. 

Por lo tanto la propiedad 1.a) y 1.b) se cumplen igualmente, ya que en su demostración no usamos ninguna propiedad particular de los reales.

La propiedad 2, NO se cumple, ya que $\overline{z}\not=z$ si es que $z\not\in\R$, y es claro que en algunos casos se puede tener que $\langle x;y\rangle\not \in\R$, por ejemplo si $x=(i,i,i)$, a $y=(0,1-i,1+i)$, entonces:
$$\langle x;y\rangle=0+i+1+i-1=2i=\langle y;x\rangle\not = \overline{\langle y;x\rangle}=-2i.$$

Pero más grave aún, la propiedad 3 no se cumple:
$$\langle x;x\rangle=-1-1-1=-3\not\ge0$$

Tampoco la propiedad 4:
$$\langle y;y\rangle=(1-i)^2+(1+i)^2=1-i-i-1+1+i+i-1=0$$

Por lo tanto en $\C^3$ tendremos que considerar otra definición.
}
\end{ejem}

\vspace{0.3 cm}

\begin{defi}
\begin{itemize}
\item En $\R^n$ sobre $\R$, el producto interno usual es $\langle \cdot; \cdot \rangle:\R^n\times \R^n\rightarrow\;\R$ definido como:
   
   $$\langle x;y\rangle =\displaystyle\sum_{j=1}^{n}x_jy_j$$
   
   donde $x=(x_1,x_2,...,x_n)$ e $y=(y_1,y_2,...,y_n)$.
   
\item En $\C^n$ sobre $\C$, el producto interno usual es $\langle \cdot; \cdot \rangle:\C^n\times \C^n\rightarrow\;\R$ definido como:
   
   $$\langle x;y\rangle =\displaystyle\sum_{j=1}^{n}x_j\overline{y}_j$$
   
   donde $x=(x_1,x_2,...,x_n)$ e $y=(y_1,y_2,...,y_n)$.
\end{itemize}	
\end{defi}


\vspace{0.3 cm}

\begin{prop} Si $\langle \phantom{x};\phantom{x} \rangle:V\times V\rightarrow \K$ es un producto interno, entonces se cumple lo siguiente:

$(\forall u,v,w\in V)(\forall\;\alpha\in \K)$
\begin{enumerate}
\item $\langle  u ; \alpha v\rangle=\overline{\alpha} \langle u; v\rangle$
\item $\langle w;u+v\rangle= \langle w;u\rangle+\langle w;v\rangle$
\end{enumerate}
\end{prop}

\underline{Demostraci\'on:}\\

$\langle  u ; \alpha v\rangle=\overline{\langle \alpha v; u\rangle}=\overline{\alpha\langle v; u\rangle}=\overline{\alpha}\ \overline{\langle v; u\rangle}=\overline{\alpha}\langle u; v\rangle$\\

Por otro lado:\\

$\langle  u ; v+w\rangle=\overline{\langle v+w; u\rangle}=\overline{\langle v;u\rangle+\langle w; u\rangle}=\overline{\langle v;u\rangle}+ \overline{\langle w; u\rangle}=\langle u;v\rangle+\langle u; w\rangle$
\hfill$\Box$
 


\begin{prop}[Desigualdad de Cauchy-Schwarz]
Si $\langle \phantom{x};\phantom{x} \rangle:V\times V\rightarrow \K$ es un producto interno, entonces se cumple la siguiente desigualdad.
$$ (\forall u,v\in V)\ |\langle u; v\rangle |\le \sqrt{\langle u; u\rangle} \sqrt{\langle v; v\rangle} $$
\end{prop}

\underline{Demostraci\'on:}

 Partiremos haciendo un tremendo truco: tomaremos $\alpha\in\C$ arbitrario. De la propiedad 3 de p.i. tenemos la siguiente desigualdad, que podemos desarrollar.
 
  \begin{eqnarray*}
    0&\le&\langle u-\alpha v;u-\alpha v\rangle\\
    &=&\langle u;u\rangle-\overline{\alpha}\langle u;v\rangle-\alpha\langle v;u\rangle+|\alpha|^2\langle v;v\rangle\\
    &=&\langle u;u\rangle-\overline{\alpha}\langle u;v\rangle-\overline{\overline{\alpha\langle v;u\rangle}}+|\alpha|^2\langle v;v\rangle\\
    &=&\langle u;u\rangle-\overline{\alpha}\langle u;v\rangle-\overline{\overline{\alpha}\langle u;v\rangle}+|\alpha|^2\langle v;v\rangle\\
    &=&\langle u;u\rangle-2Re(\overline{\alpha}\langle u;v\rangle)+|\alpha|^2\langle v;v\rangle
  \end{eqnarray*}
  
  Acá procede el truco maestro que consistente en tomar $\alpha={\displaystyle \frac{\langle u;v\rangle}{\langle v;v\rangle}}$, claro que esto solo es posible si $v\not =\Theta$, de manera que el caso $v=\Theta$ se debe analizar por separado, por el momento sigamos.
  
    \begin{eqnarray*}
      0&\le&\langle u;u\rangle-2Re\left(\overline{\frac{\langle u;v\rangle}{\langle v;v\rangle}}\langle u;v\rangle\right)+\frac{|\langle u;v\rangle|^2}{\langle v;v\rangle^2}\langle v;v\rangle\\
      &=&\langle u;u\rangle-2\frac{|\langle u;v\rangle|^2}{\langle v;v\rangle}+\frac{|\langle u;v\rangle|^2}{\langle v;v\rangle}\\
      &=&\langle u;u\rangle-\frac{|\langle u;v\rangle|^2}{\langle v;v\rangle},
  \end{eqnarray*}
    lo cual nos lleva a concluir que,
    
    $$|\langle u;v\rangle|^2\le\langle v;v\rangle\langle u;u\rangle.$$
    
    Como todos son números positivos, podemos concluir la desigualdad buscada tomando raíz cuadrada, ya que al ser esta una función creciente, respeta la desigualdad.\hfill $\Box$\\
    

Se dijo que el producto interno servía para calcular ángulos, vemos que la desigualdad de Cauchy-Schwarz permite definir correctamente un coseno.\\

\begin{defi}
Dado un e.v. con p.i. $V$ se define el coseno entre dos vectores $u,v\in V$ como sigue.
$$ \cos(\angle^u_v)=\frac{\langle u; v\rangle}{\sqrt{\langle u; u\rangle} \sqrt{\langle v; v\rangle}}\in[-1,1]$$
\end{defi}

Cumple la buena característica que si $u=\beta v$, entonces $cos(\angle^u_v)=sign(\beta)=\pm1$.\\

Por supuesto, en espacios abstractos, este ángulo no tiene un sentido geométrico.
Tampoco tendrá un sentido genométrico si no tomamos el producto \emph{usual} o si estamos en $\C^n$.
Pero como vimos al comienzo, en $\R^n$ sí corresponderá al ángulo definido propiamente tal.
Le siguiente enlace contiene un módulo interactivo que ilustra esta propiedad: \url{https://cfrd.udec.cl/proyectos/AlgebraLineal/angulo-entre-vectores.html}.

\vspace{0.6 cm}

\section{Norma y Norma Inducida}

Otra noción importante en geometría es el ``tamaño'', que en matemática llamamos ``norma''.
En la sección anterior vimos una fuerte relación entre la magnitud de un vector y el producto interior de este consigo mismo.\\

Sin embargo, es posible definir tamaños con otros criterios, por ejemplo, en una ciudad la distancia entre dos puntos estará dada por el número de cuadras necesarias para llegar de un punto al otro, lo cual difiere sustancialmente de la longitud del trazo recto que une esos dos puntos. Por lo tanto, necesitaremos abrirnos a criterios más flexibles para definir el concepto de norma.\\

Tal como hemos hecho hasta ahora, optamos por definirlo en términos de sus propiedades (no de su fórmula).\\

\vspace{0.3 cm}

 \begin{defi}
Dado un espacio vectorial sobre $\K$, con $\K=\R$ o $\C$, se dice que la función\break $\|\phantom{x}\|:V\rightarrow \R_+\cup\{0\}$ es una \emph{norma} si satisface las siguientes propiedades.
\begin{enumerate}
\item $(\forall u\in V)\ \|u\| \ge 0$
\item $(\forall u\in V)\ [\|u\|=0\Leftrightarrow u=\Theta]$
\item $(\forall \alpha \in \K)(\forall u\in V)\ \|\alpha u\|=|\alpha | \ \|u\|$
\item $(\forall u,v\in V)\ \|u+v\|\le \|u\| + \|v\|$
\end{enumerate}
\end{defi}

Parece razonable pedir que una norma sea no negativa, y que solo el vector nulo tenga norma 0.
También es razonable que si ponderamos por un escalar, su norma crezca proporcionalmente.\\

Finalmente la cuarta propiedad se conoce como \emph{desigualdad triangular}, ya que expresa que siempre en un triángulo, la longitud de cualquier lado es menor o igual que la suma de las longitudes de los otros dos lados.

\vspace{0.3 cm}

\begin{ejem}
{\em En $V=\R^n$, la función $\|u\|=\displaystyle{\sum_{i=1}^n |u_i|}$ es una norma.
  Justamente esta norma corresponde a la distancia medida en una ciudad cuadriculada como la nuestra, es conocida también como la \emph{distancia de Manhatan}.
  Verificamos sus propiedades.
  \begin{enumerate}
  \item $\|u\|=\sum_{i=1}^n |u_i|\ge0$ 
  \item Si $\|u\|=\sum_{i=1}^n |u_i|=0$, entonces para cada $i$ se debe tener $|u_i|=0$, por lo tanto $u=\Theta$.
  \item $\|\lambda u\|=\sum_{i=1}^n |\lambda u_i|=|\lambda|\sum_{i=1}^n |u_i|=|\lambda|\|u\|$.
  \item Observamos primero que si $a,b\in \R$, entonces $|a+b|\le |a|+|b|$, ya que si $a$ y $b$ tienen el mismo signo se tendrá la igualdad, y en caso contrario $|a+b|$ será estrictamente más pequeño que $|a|+|b|$.
    Así:
    
    $$\|u+v\|=\sum_{i=1}^n |u_i+v_i|\le\sum_{i=1}^n |u_i|+|v_i|=\|u\|+\|v\|.$$
  \end{enumerate}
}


\end{ejem}

\vspace{0.3 cm}

\begin{prop}
Dado un espacio vectorial $V$ con producto interior $\langle\phantom{x};\phantom{x}\rangle$, se cumple que la funci\'on definida por 
$$ (\forall u\in V)\;\|u\|=\sqrt{\langle u;u\rangle},$$

es una norma. Tal norma se llamar\'a \emph{norma inducida} por $\langle\phantom{x};\phantom{x}\rangle$.
\end{prop}


\underline{Demostraci\'on:} 

 \begin{enumerate}
  \item $\|u\|=\sqrt{\langle u;u\rangle}\ge0$
  \item Si $\|u\|=0$, entonces $\sqrt{\langle u;u\rangle}=0$, y por propiedad del producto interior concluimos que $u=\Theta$.
  \item $\|\lambda u\|=\sqrt{\langle \lambda u;\lambda u\rangle}=\sqrt{\lambda\overline{\lambda}\langle  u; u\rangle}=\sqrt{|\lambda|^2\langle u;u\rangle}=|\lambda|\| u\|$
  \item La desigualdad triangular no es tan simple, nuevamente requiere un truco, y además usa la desigualdad de Cauchy-Schwarz.
    Lo primero es observar que es equivalente probar que la desigualdad se cumple al cuadrado, demostraremos entonces que:
    $$\|u+v\|^2\le (\|u\|+\|v\|)^2$$
    En efecto,
    \begin{eqnarray*}
      \|u+v\|^2&=&\langle u+v;u+v\rangle\\
      &=&\langle u;u\rangle+\langle u;v\rangle+\langle v;u\rangle+\langle v;v\rangle\\
      &=&\|u\|^2+\langle u;v\rangle+\overline{\langle u;v\rangle}+\|v\|^2\\
      &=&\|u\|^2+2Re(\langle u;v\rangle)+\|v\|^2\hspace{0.5 cm},\hspace{0.2 cm}z+\bar{z}=2Re(z)\\
      &\le&\|u\|^2+2|\langle u;v\rangle|+\|v\|^2\hspace{1.1 cm},\hspace{0.2 cm}Re(z)\leq |z|\\
      &\le&\|u\|^2+2\|u\|\|v\|+\|v\|^2\\
      &=&(\|u\|+\|v\|)^2
    \end{eqnarray*}
  \end{enumerate}
  La primera desigualdad se justifica ya que la parte real de un complejo es siempre menor que su módulo.

  La segunda desigualdad se vale de la desigualdad de Cauchy-Schwarz.
  \hfill $\Box$

\vspace{0.3 cm}

\begin{prop}
Dado un espacio vectorial $V$ con producto interior $\langle\phantom{x};\phantom{x}\rangle$ y norma inducida $\|\phantom{x}\|$, se cumplen los siguientes:

\begin{enumerate}
\item {\em (Pitágoras)} $(\forall u,v\in V)\ \| u+v\|^2=\|u\|^2+\|v\|^2 $ si y solo si $Re\langle u,v\rangle=0$.
\item {\em (Ley del Paralelógramo)} $(\forall u,v\in V)\ \| u+v\|^2+\|u-v\|^2=2\|u\|^2+2\|v\|^2$.
\item $||u||=||v||\Leftrightarrow \langle u+v; u-v\rangle=0$.
\end{enumerate}
\end{prop}

\vspace{0.3 cm}

\begin{ejem}{\em
Considere $\R^2$ y los productos interiores:
    $$\langle x;y\rangle_2=x_1y_1+x_2y_2\;\;\wedge\;\;\langle x;y\rangle_*=x_1y_1+3x_2y_2$$
    
    y considere las siguientes normas:
    $$||x||_{\infty}=max\{|x_i|:\;i=1,2\}\;\;,\;||x||_2=\sqrt{x_1^2+x_2^2}\;\;,\;||x||_*=\sqrt{x_1^2+3x_2^2}$$
   
    a) Calcule las normas anteriores para $(2,-1)$.
      
    b) Pruebe que la norma $||x||_{\infty}$ no cumple la propiedad del paralelogramo pero que $||x||_2$ y $||x||_*$ s\'i lo hacen.
        
    c) Grafique las bolas de centro en $(0,0)$ y radio $1$ para las $3$ normas.
    
    d) Calcule el producto interior entre $(1,2)$ y $(2,-1)$ con ambos productos.\\
    
    \underline{Soluci\'on:}\\
    
    a)
    \begin{itemize}
    \item $\| (2,-1)\|_{\infty}=max\{|2|,|-1|\}=2$
    \item $\| (2,-1)\|_{2}=\sqrt{(2)^2+(-1)^2}=\sqrt{5}$
    \item $\| (2,-1)\|_{*}=\sqrt{(2)^2+3(-1)^2}=\sqrt{7}$
    \end{itemize}
    
    \vspace{0.5 cm}
    
    b) La propiedad del paralel\'ogramo es $(\forall\;u,v\in V)\ \| u+v\|^2+\|u-v\|^2=2\|u\|^2+2\|v\|^2$.\\
    
    Considere $u=(1,0)$ y $v=(0,1)$, entonces: 
    \begin{itemize}
    \item $\| u+v\|_{\infty}=\| (1,1)\|_{\infty}=max\{|1|,|1|\}=1$
    \item $\| u-v\|_{\infty}=\| (1,-1)\|_{\infty}=max\{|1|,|-1|\}=1$
    \item $\| u\|_{\infty}=\| (1,0)\|_{\infty}=max\{|1|,|0|\}=1$
    \item $\| v\|_{\infty}=\| (0,1)\|_{\infty}=max\{|0|,|1|\}=1$
    \end{itemize}
    
    luego $\| u+v\|_{\infty}^2+\|u-v\|_{\infty}^2\not=2\|u\|_{\infty}^2+2\|v\|_{\infty}^2$ y por tanto $\|\cdot\|_{\infty}$ no es una norma inducida (es norma pero no es una norma inducida por alg\'un producto interior).\\
    
    Por otra parte, sea $u=(u_1,u_2)$ y $v=(v_1,v_2)$:
    
    \begin{itemize}
    \item $||u+v||_{*}^2=(u_1+v_1)^2+3(u_2+v_2)^2$
    \item $||u-v||_{*}^2=(u_1-v_1)^2+3(u_2-v_2)^2$
    \item $||u||_{*}^2=u_1^2+3u_2^2$
    \item $||v||_{*}^2=v_1^2+3v_2^2$
    \end{itemize}
    
    luego:\\
    
    $||u+v||_{*}^2+||u-v||_{*}^2=(u_1+v_1)^2+3(u_2+v_2)^2+(u_1-v_1)^2+3(u_2-v_2)^2$\\
    
    \hspace{3.8 cm}$=u_1^2+2u_1v_1+v_1^2+3u_2^2+6u_2v_2+3v_2^2+u_1^2-2u_1v_1+v_1^2+3u_2^2-6u_2v_2+3v_2^2$\\
    
    \hspace{3.8 cm}$=2u_1^2+2v_1^2+6u_2^2+6v_2^2$\\
    
    \hspace{3.8 cm}$=2(u_1^2+3u_2^2)+2(v_1^2+3v_2^2)$\\
    
    \hspace{3.8 cm}$=2||u||_{*}^2+2||v||_{*}^2$
    
    \vspace{0.5 cm}
    
    c) Ver video adjunto.
    
    \vspace{0.5 cm}
    
    d) 
    \begin{itemize}
    \item $\langle (1,2);(2,-1)\rangle_2=(1)(2)+(2)(-1)=0$
    \item $\langle (1,2);(2,-1)\rangle_*=(1)(2)+3(2)(-1)=-4$
    \end{itemize}
    
    
    
    
    }
\end{ejem}

\vspace{0.3 cm}
\begin{ejem} {\em
En el espacio vectorial $\P_n(\R)$ sobre $\R$ hay dos productos internos clásicos, o más bien dicho, dos familias de productos internos clásicos.
\begin{item}
\item Dados $a<b\in\R$, fijos, se define el producto:
$$\langle p;q\rangle_{[a,b]}=\int_a^bp(x)q(x)dx.$$
Resulta que es un producto interno, ya que claramente es lineal respecto a $p(x)$, es simétrico, además, dado que $p(x)^2\ge 0$, resulta que la integral de $p(x)^2$ también es mayor o igual a 0.

La última propiedad de producto interno es más delicada, para verla, supongamos que $\int_a^b p(x)^2 dx=0$, como $p(x)^2\ge 0$, y dado que $p$ es una función continua, un teorema de cálculo nos dice que la única manera que esta integral sea nula es que $\forall x\in[a,b],\ p(x)=0$ (no podemos decir nada de lo que pasa con $p$ fuera de $[a,b]$).
Pero como $p$ es un polinomio, solo puede anularse en una cantidad finita de puntos, salvo que sea el polinomio nulo, $n$ a lo más.
El intervalo $[a,b]$ tiene infinitos puntos, por lo tanto deducimos que $p$ debe ser el polinomio nulo en este caso.
\item Dados $n+1$ puntos diferentes en $\R$: $x_0,x_1,\dots,x_n$, se define el producto:
$$\langle p;q\rangle_{\{x_0,x_1,\dots,x_n\}}=\sum_{i=0}^np(x_i)q(x_i).$$
Resulta que es un producto interno en $\P_n(\R)$, por argumentos muy similares a los anteriores, notar que es importante tomar $n+1$, y no menos, para que así el último argumento funcione.

Un caso particular de este producto en $\P_2(\R)$ sería por ejemplo:
$$\langle p;q\rangle_{\{-1,0,1\}}=p(-1)q(-1)+p(0)q(0)+p(1)q(1).$$
\end{item}
}
\end{ejem}

\begin{ejem} {\em
Por su parte,  en el suebspacio $\M_{m\times n}(\C)$ consideramos un producto análogo al producto usual de $\C^n$.
$$\langle A;B\rangle=\sum_{i=1}^m\sum_{j=1}^n A_{ij}\overline{B_{ij}}$$
Es un producto interno por las mismas razones que lo es el producto usual de $\C^n$, ya que funciona como si miráramos a la matriz como una gran columna.

Resulta que este producto también se puede escribir de una manera más sucinta, usando la función \emph{traza}.
$$tr:\M_{m\times m}(\C)\rightarrow \C, tr(M)=\sum_{i=1}^n M_{ii}$$
La traza es una función lineal de $\M_{m\times m}(\R)$ en $\R$, que cumple con las siguientes propiedades.
\begin{enumerate}
\item Si $\alpha\in\C$, $tr(\alpha M)=\alpha tr(M)$.
\item Si $M, H\in\M_{m\times m}(\C)$, $tr(M+H)=tr(M)+tr(H)$.
\item Si $M\in\M_{m\times m}(\C)$, $tr(\overline{M})=\overline{tr(M)}$.
\item Si $M\in\M_{m\times m}(\C)$, $tr(M^\top)=tr(M)$.
\item Si $L\in\M_{k\times l}(\C)$, $P\in\M_{l\times k}(\C)$ $tr(LP)=tr(PL)=\sum_{i=1}^k\sum_{j=1}^k L_{ij}P_{ij}$.
\end{enumerate}
Gracias a estas propiedades tenemos la siguiente igualdad, que puede ser útil para hacer resultados teóricos respecto al producto interno usual de las matrices.
$$\langle A;B\rangle=tr(A^\top \overline{B}).$$
Por ejemplo, en $\M_{2\times 3}(\C)$, tenemos.
\begin{eqnarray*}
\left\langle \left(\begin{array}{ccc} 1&0&1-i\\1&-1&1+i\end{array}\right); \left(\begin{array}{ccc} 0&1+2i&-3\\0&i&-i\end{array}\right)\right\rangle&=& 0+0+-3(1-i)+0-1(-i)+(1+i)i\\
&=& -3+3i+i+i-1\\
&=& -4+5i\\
tr\left(\left(\begin{array}{cc} 1&1\\0&-1\\1-i&1+i\end{array}\right) \left(\begin{array}{ccc} \overline{0}&\overline{1+2i}&\overline{-3}\\\overline{0}&\overline{i}&\overline{-i}\end{array}\right)\right)
&=&tr\left(\left(\begin{array}{cc} 1&1\\0&-1\\1-i&1+i\end{array}\right) \left(\begin{array}{ccc} 0& 1-2i&-3\\ 0&-i&i\end{array}\right)\right)\\
&=&tr\left(\begin{array}{ccc} 0&?&?\\ ?& i & ?\\? & ? & -3(1-i)+(1+i)i \end{array}\right)\\
&=&0+i-3 +3i+i-1\\
&=&-4+5i
\end{eqnarray*}

}
\end{ejem}

\section{Ortogonalidad y distancia}


Como dijimos antes, gracias a la desigualdad de Cauchy-Schwarz podemos considerar el siguiente número y llamarle ``coseno'', y deducir un ángulo entre vectores a partir de éste:

$$cos(\angle^u_v)=\frac{\langle u; v\rangle}{\|u\| \|v\|}.$$

Cumple con las buenas propiedades deseado y en particular permite concebir los ángulos de $\frac{\pi}{2}$ y $-\frac{\pi}{2}$ como aquellos cuyo coseno es 0, definiendo así la importante noción de \emph{perpendicularidad} u \emph{ortogonalidad}.

 

\begin{defi}
Dado un espacio vectorial $V$ con producto interior $\langle\phantom{x};\phantom{x}\rangle$ y norma inducida $\|\phantom{x}\|$, decimos que:

\begin{enumerate}
\item \emph{$u$ y $v$ son ortogonales} si $\langle u;v \rangle=0$ y entonces denotamos $u\bot v$,
\item \emph{el conjunto $\{u_1,..,u_k\}$ es ortogonal} si $(\forall i\not = j)\ u_i\bot u_j$ y $(\forall i\in\{1,..,k\})\ u_i\not= \Theta$,
\item \emph{el conjunto $\{u_1,..,u_k\}$ es ortonormal} si es ortogonal y $(\forall i\in\{1,..,k\})\ \|u\|=1$.
\end{enumerate}
\end{defi}

\begin{ejem} {\em
    Considerando los productos que vimos la clase pasada:
    $$\langle x;y\rangle_2=x_1y_1+x_2y_2\;\;\wedge\;\;\langle x;y\rangle_*=x_1y_1+3x_2y_2$$
    y tomamos dos vectores cuales quiera; $(1,2)$ y $(6,-1)$ con ambos productos.
    \begin{itemize}
    \item $\langle (1,2);(6,-1)\rangle_2=(1)(6)+(2)(-1)=4$
    \item $\langle (1,2);(6,-1)\rangle_*=(1)(6)+3(2)(-1)=0$
    \end{itemize}
    Para saber si dos vectores son ortogonales basta calcular su producto interno, si da 0 son ortogonales, si no, no.
    
    Vemos que la propiedad de ortogonalidad depende del producto que una toma como base.

    $(1,2)$ y $(6,-1)$ son ortogonales respecto a $\langle x;y\rangle_*$, pero no respecto a $\langle x;y\rangle_2$.

    La noción de conjunto ortogonal exige dos cosas: que todos los productos entre vectores distintos sean nulos y que $\Theta$ no esté en el conjunto.

    Con el producto usual de $\C^3$ tenemos que el conjunto $\{(1,i,1),(i,1,0), (1,i,-2)\}$ es ortogonal:

    \begin{itemize}
    \item $\langle (1,i,1);(i,1,0)\rangle=(1)(\overline{i})+(i)(\overline{1})+(1)(\overline{0})=-i+i+0=0$
    \item $\langle (1,i,1);(1,i,-2)\rangle=(1)(\overline{1})+(i)(\overline{i})+(1)(\overline{-2})=1+1-2=0$
    \item $\langle (1,i,-2);(i,1,0)\rangle=(1)(\overline{i})+(i)(\overline{1})+(-2)(\overline{0})=-i+i+0=0$
    \item $\langle (i,1,0);(1,i,1)\rangle=\overline{\langle(1,i,1);(i,1,0)\rangle}=\overline{0}=0$, etc.
    \end{itemize}

   Pero no es ortonormal ya que $\langle (1,i,1);(1,i,1)\rangle=1\overline{1}+i\overline{i}+1\overline{1}=1+1+1=3$, entonces $\|(1,i,1)\|=\sqrt{3}\not =1$.
    
  Sin embargo $\{\frac{1}{\sqrt{3}}(1,i,1),\frac{1}{\sqrt{2}}(i,1,0), \frac{1}{\sqrt{6}}(1,i,-2)\}$ sí es ortonormal. Un conjunto ortonormal se obtiene de un conjunto ortogonal dividiendo cada vector por su norma, acción que se denomina ``normalizar''.
}
\end{ejem}

\begin{prop}
Si $\{u_1,..,u_k\}$ es ortogonal, entonces es l.i.
\end{prop}
    {\bf Demostración.} {\small
      Supongamos que no es así, y que existen escalares $\lambda_1,\lambda_2,\dots,\lambda_k\in\K$ no todos nulos tales que
      $$\sum_{i=1}^k \lambda_i u_i=\Theta$$
      Si aplicamos producto interno a ambos lados por uno de los vectores $u_j$ tal que $\lambda_j\not=0$, obtenemos.
      \begin{eqnarray*}
        \left\langle\sum_{i=1}^k \lambda_i u_i;u_j\right\rangle&=&\langle\Theta;u_j\rangle\\
        \sum_{i=1}^k \lambda_i \langle u_i;u_j\rangle&=&0\\
        \lambda_j \langle u_j;u_j\rangle&=&0
      \end{eqnarray*}
      Como $u_j$ es ortogonal a todo $u_i$ con $i\not = j$, todos los productos de la suma valen 0, salvo $\langle u_j;u_j\rangle$.
      Sin embargo, como la suma completa vale 0, podemos concluir que $\lambda_j\|u_j\|=0$, y como $u_j\not=\Theta$, concluimos que $\lambda_j=0$, lo cual es una contradicción.\hfill $\Box$
    }

\begin{ejem} {\em
Por ejemplo, el siguiente conjunto es ortogonal respecto al producto interno usual de matrices: $\left\{\left(\begin{array}{rr} 1&-1\\-1&1\end{array}\right), \left(\begin{array}{rr} 1&1\\1&1\end{array}\right),\left(\begin{array}{rr} 1&-1\\1&-1\end{array}\right)\right\}$.
Por lo tanto de inmediato sabemos que es l.i.

Por otra parte, observamos que esta base no es ortonormal, ya que la norma inducida por el producto interno usual de matrices, que consiste en tomar la raíz de la suma de los cuadrados de cada coeficiente, es raíz de 4 = 2.
}
\end{ejem}

\begin{prop}
Si $\{u_1,..,u_k\}$ es ortogonal, entonces es base de $U=\langle\{u_1,..,u_k\}\rangle$ y para todo $v,w\in U$ se cumple que la i-ésima coordenada de $v$ en la base $\{u_1,..,u_k\}$ es $\frac{\langle v;u_i\rangle}{\|u_i\|^2}$ y además:
\begin{eqnarray*}
\langle v;w\rangle &=& \sum_{i=1}^k \frac{ \langle v;u_i\rangle\overline{\langle w;u_i\rangle}}{\|u_i\|^2},\quad \textrm{ y }\quad
\|v\|^2=\sum_{i=1}^k \frac{|\langle v;u_i\rangle|^2}{\|u_i\|^2}.
\end{eqnarray*}
\end{prop}
    {\bf Demostración.} {\small
      Supongamos que las coordenadas de $v$ respecto a $\{u_1,..,u_k\}$ son $(\alpha_1,\alpha_2,\dots,\alpha_k)$, es decir:
      $\sum_{i=1}^k\alpha_i u_i=v$,
      si, como antes, hacemos producto con un vector $u_j$ cualquiera de la base y obtenemos:
      $\langle v;u_j\rangle = \left\langle\sum_{i=1}^k\alpha_iu_i;u_j\right\rangle=\sum_{i=1}^k\alpha_i\left\langle u_i;u_j\right\rangle=\alpha_j\left\langle u_j;u_j\right\rangle$.
      De donde es claro que $$\alpha_j=\frac{\langle v;u_j\rangle}{\langle u_j;u_j\rangle}=\frac{\langle v;u_j\rangle}{\|u_j\|^2}.$$
      Usamos esto para el siguiente cálculo:\\
      $\displaystyle{
        \langle v;w\rangle =\left\langle\sum_{i=1}^k \frac{\langle v;u_i\rangle}{\|u_i\|^2}u_i;\sum_{j=1}^k \frac{\langle w;u_j\rangle}{\|u_j\|^2}u_j\right\rangle
        = \sum_{i=1}^k\sum_{j=1}^k  \frac{\langle v;u_i\rangle}{\|u_i\|^2}\frac{\overline{\langle w;u_j\rangle}}{\|u_j\|^2} \left\langle u_i;u_j\right\rangle
        = \sum_{i=1}^k\frac{\langle v;u_i\rangle\overline{\langle w;u_i\rangle}}{\|u_i\|^4}\|u_i\|^2}$.
     % =\sum_{i=1}^k \frac{ \langle v;u_i\rangle\overline{\langle w;u_i\rangle}}{\|u_i\|^2}.} $
        
     El último resultado se obtiene directamente de aplicar éste a $w=v$ y que  $\|v\|^2=\langle v;v\rangle$.\hfill $\Box$
    }
     

    Este resultado nos dice que solo con la ortogonalidad obtenemos la independencia lineal de manera gratuita, por lo tanto obtenemos una base des espacio que el conjunto genera.

    Además, nos da una manera directa de obtener las coordenadas, sin necesidad de resolver ningún sistema lineal, sólo calcular dos productos y ya está.
    Ese es la mayor ventaja de tener una \emph{base ortogonal}.

    Por otra parte, también nos dice que el producto entre dos vectores cualesquiera, respecto a un producto cualquiera, tiene la forma del producto interno usual entre las coordenadas de los vectores respecto a la base ortogonal. De alguna manera, todos los productos son como el producto usual si uno hace el cambio de coordenadas apropiado.

\begin{ejem} {\em
Consideremos $\B=\left\{\left(\begin{array}{rr} 1&-1\\-1&1\end{array}\right), \left(\begin{array}{rr} 1&1\\1&1\end{array}\right),\left(\begin{array}{rr} 1&-1\\1&-1\end{array}\right)\right\}$ y $S=\langle\B\rangle$.
Ya sabemos que $\B$ es base de $S$.
Si tomamos $A=\left(\begin{array}{rr} 1&2\\3&0\end{array}\right)\in S$, ¿cómo obtenemos sus coordenadas respecto a $\B$?
Tiene 3 coordenadas $(\alpha_1,\alpha_2,\alpha_3)$, gracias a que $\B$ es ortogonal, las obtenemos como sigue:
$\alpha_1=\frac{1}{4}\left\langle \left(\begin{array}{rr} 1&2\\3&0\end{array}\right);\left(\begin{array}{rr} 1&-1\\-1&1\end{array}\right)\right\rangle=\frac{1}{4}(1-2-3)=-1$\\
$\alpha_2=\frac{1}{4}\left\langle \left(\begin{array}{rr} 1&2\\3&0\end{array}\right);\left(\begin{array}{rr} 1&1\\1&1\end{array}\right)\right\rangle=\frac{1}{4}(1+2+3)=\frac{3}{2}$\\
$\alpha_3=\frac{1}{4}\left\langle \left(\begin{array}{rr} 1&2\\3&0\end{array}\right);\left(\begin{array}{rr} 1&-1\\1&-1\end{array}\right)\right\rangle=\frac{1}{4}(1-2+3)=\frac{1}{2}$\\
Así:
$$A=\left(\begin{array}{rr} 1&2\\3&0\end{array}\right)=
-1\left(\begin{array}{rr} 1&-1\\-1&1\end{array}\right)
+\frac{3}{2} \left(\begin{array}{rr} 1&1\\1&1\end{array}\right)
+\frac{1}{2}\left(\begin{array}{rr} 1&-1\\1&-1\end{array}\right).$$
}
\end{ejem}


\section{Proyección}


    
\begin{defi}
Dado un s.e.v. $U$, se define la \emph{proyección ortogonal} de $v$ en $U$ como el vector $u\in U$ que está más cerca de $v$, es decir el vector $u\in U$ que cumple:
$$\forall w\in U,\ ||v-u||\le ||v-w||,$$
tal vector lo denotamos por $u=Proy_U(v)$.
\end{defi}

En general no es fácil encontrar $u$, es un proceso de búsqueda, además, a priori, no tendría por qué haber un único $u$, la definición podría no tener sentido entonces... sin embargo sí lo tiene gracias al siguiente teorema, al menos en aquellos subespacios que posean una base ortogonal.
Un video que ilustra este concepto es el siguiente: \url{https://vimeo.com/604043973/d9e9c6eaf5}.

\begin{teo}
Si $\|\cdot\|$ es una norma inducida por un p.i. $\langle\cdot;\cdot\rangle$, $U$ es un s.e.v. generado por el conjunto ortogonal $\{u_1,.., u_k\}$, y $v\in V$, entonces se cumplen las siguientes propiedades.
\begin{enumerate}
\item $\forall y\in U,\ y\bot \left(v-Proy_U(v)\right)$ respecto a $\langle\cdot;\cdot\rangle$
\item $Proy_U(v)=\displaystyle{\sum_{i=1}^k \frac{\langle v,u_i\rangle}{\|u_i\|^2}u_i}$
\end{enumerate}
\end{teo}
    {\bf Demostración} {\small
      Primero definamos $x=\sum_{i=1}^k \frac{\langle v,u_i\rangle}{\|u_i\|^2}u_i$ y observemos que está en $U$, es claro ya que se trata de una c.l. de elementos de $U$.

      Esta definición nos dice también que las coordenadas de $x$ respecto a la base de $U$ son justamente los $\frac{\langle v;u_i\rangle}{\|u_i\|^2}$ pero estas también son iguales a  $\frac{\langle x;u_i\rangle}{\|u_i\|^2}$ por la propiedad anterior. Entonces $\langle v;u_i\rangle=\langle x;u_i\rangle$, para todo $i\in\{1,\dots ,k\}$.

      Esta igualdad se hereda a todos los vectores de $U$, en efecto si $y=\sum_{i=1}^k\alpha_i u_i$, entonces\\
      $\langle v;y\rangle
      =\langle v; \sum_{i=1}^k \alpha_i u_i\rangle
      =\sum_{i=1}^k\overline{\alpha_i} \langle v;u_j\rangle
      =\sum_{i=1}^k\overline{\alpha_i} \langle x;u_j\rangle
      =\langle x;\sum_{i=1}^k\alpha_i u_j\rangle
      =\langle x; y\rangle$.
      
Esto implica directamente que $\langle v-x;y\rangle=0$, es decir que $(v-x)\bot y$, para cualquier $y\in U$ (Parte 1).
      
Tomemos ahora un vector $w\in U$, vamos a demostrar que $x$ cumple lo necesario para ser la proyección de $v$ en $U$, es decir, que $x$ está más cerca de $v$ que $w$.
Primero observemos que
      $v-w=(v-x)+(x-w)$, pero $x-w\in U$, entonces por lo demostrado antes resulta que $(x-w)\bot (v-x)$, así podemos aplicar pitágoras a esta suma de vectores:

      $\|v-w\|^2=\|v-x\|^2+\|x-w\|^2\ge \|v-x\|^2$, por lo tanto $\|v-w\|\ge \|v-x\|$, para todo $w\in U$.\hfill $\Box$
    }

En otras palabras, el teorema anterior nos da dos cosas: 1) un método para calcular la proyección, y 2) nos da una propiedad geométrica muy bonita e intuitiva: la proyección se logra allí donde se forma un ángulo recto con $U$.

\hspace{-.5cm}
\begin{minipage}{.5\textwidth}
\begin{center}
  \begin{tikzpicture}[scale=1.2]
    \draw[-] (-1,0) -- (4,0) node[below] {$x$};
    \draw[-] (0,-1) -- (0,3) node[left] {$y$};
    \coordinate[label=below:$1$] (UX) at (1,0);
    \coordinate[label=below:$2$] (VX) at (2,0);
    \coordinate[label=left:$2$] (UY) at (0,2);
    \coordinate[label=left:$1$] (VY) at (0,1);
    \coordinate[label=right:$u$] (U) at (1,2.5);
    \coordinate[label=right:$v$] (V) at (1.5,.5);
    \draw[fill=black] (U) circle(0.03);
    \draw[fill=black] (V) circle(0.03);
    \draw[-] (0,0) -- (U);
    %\draw[dashed] (UX) -- (U);
    %\draw[dashed] (UY) -- (U);
    %\draw[dashed] (VX) -- (V);
    %\draw[dashed] (VY) -- (V);
    \draw[-] (0,0) -- (V);
    \coordinate[label=right:$U$] (L) at (1.4,3.5);
    \coordinate (M) at (-.4,-1);
    \draw[dashed] (L) -- (M);
    \coordinate[label=left:$w$] (Pv) at  (11/29,55/58);
    \draw[fill=black] (Pv) circle(0.03);
    \draw[dashed] (Pv) -- (V);
    \draw (V) circle(1.2070196);
  \end{tikzpicture} 
\end{center}
\end{minipage}
\begin{minipage}{.5\textwidth}
\begin{eqnarray*}
U&=&\langle\{u\}\rangle \textrm{ con } u=\left(1;\frac{5}{2}\right)\textrm{ y } v=\left(\frac{3}{2}; \frac{1}{2}\right)\\
\\
w&=&Proy_U(v)\ =\ \frac{\langle v;u\rangle}{\langle u;u\rangle}u\ =\ \frac{\frac{3}{2}\cdot1+\frac{1}{2}\cdot\frac{5}{2}}{1^2+\left(\frac{5}{2}\right)^2}u\\
&=&\frac{11}{29}u\ =\ \left(\frac{11}{29},\frac{55}{58}\right)
\end{eqnarray*}
\end{minipage}

 

{\bf Ejemplo.} {\em 
$U=\langle\{(1,1,1), (1,1,0)\}\rangle$, tomemos el p.i. usual.

¿Son ortogonales? $\langle(1,1,1);(1,1,0)\rangle=1+1+0=1$ NO, no podemos aplicar la teoría.

Tomo $(0,0,1)\in U$ ya que $(0,0,1)=(1,1,1)-(1,1,0)$ entonces 

$U=\langle\{(1,1,1), (1,1,0),(0,0,1)\}\rangle=\langle\{(1,1,0),(0,0,1)\}\rangle$


¿Son ortogonales? $\langle(1,1,0);(0,0,1)\rangle=0+0+0=0$ SI!!

Calculemos pa proyección de $v=(1,2,4)$ sobre $U$.

$Proy_U (1,2,4)=\frac{\langle (1,2,4);(1,1,0)\rangle}{||(1,1,0)||^2}(1,1,0)+\frac{\langle (1,2,4);(0,0,1)\rangle}{||(0,0,1)||^2}(0,0,1)$

$= \frac{3}{2}(1,1,0)+\frac{4}{1}(0,0,1)=(\frac{3}{2},\frac{3}{2},4)$

$v-Proy_U(v) =(-\frac{1}{2},\frac{1}{2},0)\bot \{(1,1,1), (1,1,0)\}$

$\langle(-\frac{1}{2},\frac{1}{2},0),(1,1,1)\rangle=0$; \quad $\langle(-\frac{1}{2},\frac{1}{2},0),(1,1,0)\rangle=0$

}


 
Observaciones finales:

\begin{itemize}
\item Veremos que cualquier espacio de dimensión finita tiene una base ortogonal respecto a su producto interior.
\item En cualquier espacio $U$ hay a lo más $dim(U)$ vectores ortogonales entre sí.
\item Tener una base ortogonal de $U$ sirve para calcular más fácilmente las coordenadas de sus vectores respecto a esa base.
\item Tener una base ortogonal de $U$ sirve para obtener la proyección de cualquier vector del espacio en el subespacio $U$.
\item La proyección de un $v$ es la mejor aproximación que hay en $U$ de $v$ respecto a la norma inducida por el producto interno.
  \item Esto solo es verdad cuando tenemos una norma inducida por un producto interno, para las otras normas, la proyección puede no ser única, y es más complicado calcularla.
\end{itemize}

En los ejemplos previos hemos trabajado con tuplas de $\R^3$, pero esta teoría puede aplicarse a cualquier espacio vectorial, por ejemplo a los polinomios, caso en que se busca un polinomio que esté lo más cerca posible a un subespacio dado, en el sentido de la norma que se haya definido.
Un video que nos ilustra una aplicación al espacio de los polinomios se encuentra en el siguiente video: \url{https://vimeo.com/604044659/6cecb43b49}.

\subsection{Método de Gram-Schmidt}

     La clase pasada introdujimos el concepto de proyección de un vector sobre un subespacio vectorial.

Tal proyección tiene utilidad y significado para la ingeniería.
Además la teoría provee una manera de calcularla cuando se dispone de una base ortogonal del subespacio donde se desea proyectar.
Recordamos las dos principales propiedades que nos servirán en esta clase.

\begin{enumerate}
\item $Proy_U(v)=\sum_{i=1}^k \frac{\langle v,u_i\rangle}{\|u_i\|^2}u_i$
\item $\forall i\in\{1,\dots,k\},\ v-Proy_U(v)\bot u_i$
\end{enumerate}

 

El objetivo de esta clase será establecer un método que permita obtener un conjunto ortogonal a partir de un conjunto l.i, ya que eso es necesario para poder calcular la proyección.

\begin{prop}[Método de Gram-Schmidt]
Dado un conjunto l.i. $\{v_1,..,v_k\}$ en $V$, definimos el conjunto $\{u_1,..,u_k\}$, por recurrencia, como sigue.
\begin{eqnarray*}
u_1&=&v_1\\
u_{i+1} & =& v_{i+1} - Proy_{\langle \{u_1,..,u_i\}\rangle}(v_{i+1})\\
&=& v_{i+1}-\sum_{j=1}^i \frac{\langle v_{i+1},u_j\rangle}{\|u_j\|^2}u_j
\end{eqnarray*}
Resulta que para cada $i\in\{1,..,k\}$, $\{u_1,..,u_i\}$ es ortogonal y genera el mismo espacio que $\{v_1,..,v_i\}$.
\end{prop}
{\bf Demostración.} {\small
  La haremos por inducción, en $i$.

  Para $i=1$ está claro que $\{u_1\}$ es ortogonal, pues $u_1\not=\Theta$ y es solo un vector.
  Además también es claro que $\langle\{u_1\}\rangle=\langle\{v_1\}\rangle$.

  Supongamos ahora que la propiedad es cierta para un $i<k$ dado, y demostrémosla para $i+1$ (Hipótesis de Inducción).
  
  Primero que nada, observamos que $u_{i+1}$ se puede calcular gracias a que el espacio $\{u_1,\dots,u_i\}$ es ortogonal. Además, $\langle\{u_1,\dots,u_i\}\rangle=\langle\{v_1,\dots,v_i\}\rangle$, entonces estamos proyectando en realidad sobre este último espacio.

  Por la forma en que se define $u_{i+1}$, es claro que $u_{i+1}\bot u_j$, para todo $j\le i$.
  Además $u_{i+1}\in\langle\{v_1,\dots, v_i,v_{i+1}\}\rangle=\langle\{u_1,\dots, u_i,v_{i+1}\}\rangle$, por lo tanto, podemos reemplazar $v_{i+1}$ por $u_{i+1}$ generando el mismo espacio, es decir $\langle\{v_1,\dots, v_i,v_{i+1}\}\rangle=\langle\{u_1,\dots, u_i,u_{i+1}\}\rangle$.\hfill$\Box$
}

En otras palabras, para obtener la base ortogonal, se parte con uno de los vectores de la base y sucesivamente vamos proyectando los siguientes vectores, para quedarnos con el vector ortogonal que esta proyección genera, obtenemos así una base ortogonal que es equivalente a la otra en cada paso.

\begin{cor}
Todo espacio de dimensión finita tiene alguna base ortonormal.
\end{cor}

Este método asegura la existencia y la manera para encontrar la base ortogonal que necesitamos, por lo tanto asegura la posibilidad de calcular la proyección siempre.

\begin{ejem}
Calculemos una base $\mathcal{B}$ ortogonal respecto al producto interno usual para \break $S=\langle\{(1,0,-1),(0,i,2i),(i,3i,-i)\}\rangle$.

{\em
El primer vector es el primer vector del sistema generador de $S$: $u_1=(1,0,-1)$. Aplicamos la fórmula para obtener el segundo vector:
\begin{eqnarray*}
u_2&=&(0,i,2i)-\frac{\langle (0,i,2i);(1,0,-1)\rangle}{\langle (1,0,-1);(1,0,-1)\rangle}(1,0,-1)\\
u_2&=&(0,i,2i)-\frac{0\cdot 1+i\cdot 0+2i\cdot(-1)}{1\cdot 1+(-1)\cdot(-1)}(1,0,-1)\\
u_2&=&(0,i,2i)-\frac{-2i}{2}(1,0,-1)\\
u_2&=&(0,i,2i)+(i,0,-i)\\
u_2&=&(i,i,i)\\
\end{eqnarray*}
La fórmula para el tercero ya usa estos dos primeros vectores:
\begin{eqnarray*}
u_3&=&v_3-\frac{\langle v_3; u_1\rangle}{\langle u_1;u_1\rangle}u_1-\frac{\langle v_3; u_2\rangle}{\langle u_2;u_2\rangle}u_2\\
u_3&=&(i,3i,-i)-\frac{\langle (i,3i,-i);(1,0,-1)\rangle}{\langle (1,0,-1);(1,0,-1)\rangle}(1,0,-1)-\frac{\langle (i,3i,-i);(i,i,i)\rangle}{\langle (i,i,i);(i,i,i)\rangle}(i,i,i)\\
u_3&=&(i,3i,-i)-\frac{i\cdot1+3i\cdot0+(-i)\cdot(-1))}{2}(1,0,-1)-\frac{i\cdot(-i)+3i\cdot(-i)+(-i)\cdot(-i)}{i\cdot(-i)+i\cdot(-i)+i\cdot(-i)}(i,i,i)\\
u_3&=&(i,3i,-i)-\frac{i+i}{2}(1,0,-1)-\frac{1+3-1}{1+1+1}(i,i,i)\\
u_3&=&(i,3i,-i)-(i,0,-i)-(i,i,i)\\
u_3&=&(-i,2i,-i)
\end{eqnarray*}

Así la base ortogonal pedida es: $\{ (1,0,-1),(i,i,i),(-i,2i,-i)\}$.
 
Nos conviene verificar su ortogonalidad:

$\langle (1,0,-1);(i,i,i)\rangle=1\overline{i}+0-1\overline{i}=0$;

$\langle (1,0,-1);(-i,2i,-i)\rangle=1\overline{-i}+0-1\overline{-i}=0$;

$\langle (i,i,i);(-i,2i,-i)\rangle=i\overline{-i}+i\overline{2i}+i\overline{-i}=-1+2-1=0$.
}
\end{ejem}

\begin{ejem}
Calculemos una base $\mathcal{O}$ ortogonal de $\P_2(\R)$ respecto al producto siguiente
$$ \langle p;q\rangle_{[0,1]}=\int_0^1 p(x)q(x)dx.$$

{\em
Tomaremos como base de partida a la base canónica de $\P_2(\R)$: $\Ccal_{\P_3}=\{1,x,,x^2\}$.

\begin{eqnarray*}
u_1(x)&=& 1
\end{eqnarray*}

\begin{eqnarray*}
u_2(x)&=&x-\frac{\langle x;1\rangle}{\langle 1;1\rangle}1\\
u_2(x)&=&x-\frac{\int_0^1 x dx}{\int_0^1 1dx}1\\
u_2(x)&=&x-\frac{1/2}{1}1\\
u_2(x)&=&x+\frac{1}{2}\\
\end{eqnarray*}
La fórmula para el tercero ya usa estos dos primeros vectores:
\begin{eqnarray*}
u_3(x)&=&x^2-\frac{\langle x^2; 1\rangle}{\langle 1;1\rangle}1-\frac{\langle x^2; x-1/2\rangle}{\langle x-1/2;x-1/2\rangle}(x-1/2)\\
u_3(x)&=&x^2-\frac{\int_0^1 x^2dx}{1}1-\frac{\int_0^1x^3-x^2/2dx}{\int_0^1(x-1/2)^2}(x-1/2)dx\\
u_3(x)&=&x^2-\frac{1/3}{1}1-\frac{1/4-1/6}{\int_0^1 x^2-x+1/4dx}(x-1/2)\\
u_3(x)&=&x^2-\frac{1}{3}-\frac{1/12}{1/3-1/2+1/4}(x-1/2)\\
u_3(x)&=&x^2-\frac{1}{3}-(x-1/2)\\
u_3(x)&=&x^2-x+\frac{1}{6}
\end{eqnarray*}

Así la base ortogonal pedida es: $\{ 1,x-\frac{1}{2},x^2-x+\frac{1}{6}\}$.
 
Nos conviene verificar su ortogonalidad:

\begin{eqnarray*}
\langle 1;x-\frac{1}{2}\rangle&=&\int_0^1x-\frac{1}{2} dx\\
&=&\frac{1}{2}-\frac{1}{2}\\
\langle 1;x^2-x+\frac{1}{6}\rangle&=&\int_0^1 x^2-x+\frac{1}{6}dx\\
&=&\frac{1}{3}-\frac{1}{2}+\frac{1}{6}\\
&=&0\\
\langle x-\frac{1}{2};x^2-x+\frac{1}{6}\rangle&=&\int_0^1(x-\frac{1}{2})(x^2-x+\frac{1}{6})dx\\
&=&\int_0^1 x^3-\frac{x^2}{2}-x^2+\frac{x}{2}+\frac{x}{6}-\frac{1}{12}dx\\
&=&\int_0^1 x^3-\frac{3}{2}x^2+\frac{2}{3}x-\frac{1}{12}dx\\
&=&\frac{1}{4}-\frac{1}{2}+\frac{1}{3}-\frac{1}{12}\\
&=&0
\end{eqnarray*}
}
\end{ejem}

Puede revisar Wikipedia, donde encontrará un video muy claro sobre la ejecución de este algoritmo al caso de $\R^3$: \url{https://es.wikipedia.org/wiki/Proceso_de_ortogonalizaci}.%C3%B3n_de_Gram-Schmidt



\end{document}
